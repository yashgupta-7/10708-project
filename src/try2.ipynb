{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "from utils import train, test, edgeindex2adj\n",
    "# citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "# pubmed = Planetoid(root='.', name='Pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "from models import ALP, GCN\n",
    "dataset = cora\n",
    "data = dataset[0]\n",
    "\n",
    "# take k random training nodes for each class\n",
    "k = 2\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "print(data.train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yshape = dataset[0].y.shape[0]\n",
    "model = ALP(num_layers=12, alpha=0.99, yshape=yshape)\n",
    "outs = model.lpa(dataset)\n",
    "\n",
    "gcn = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "optimizer_gcn = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "optimizer_alp = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.468, 0.487]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14) tensor(500) tensor(1000) tensor(1194)\n"
     ]
    }
   ],
   "source": [
    "train_mask = data.train_mask.clone()\n",
    "val_mask = data.val_mask.clone()\n",
    "test_mask = data.test_mask.clone()\n",
    "\n",
    "unlab_mask = torch.ones_like(data.train_mask)\n",
    "unlab_mask[(data.train_mask == True) | (data.val_mask == True) | (data.test_mask == True)] = False\n",
    "\n",
    "print(train_mask.sum(), val_mask.sum(), test_mask.sum(), unlab_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9654680490493774, Train: 0.5714, Val: 0.2300, Test: 0.2120, Loss2: 1.8164259195327759\n",
      "Epoch: 002, Loss: 1.8978215456008911, Train: 0.5714, Val: 0.2620, Test: 0.2420, Loss2: 1.8163565397262573\n",
      "Epoch: 003, Loss: 1.8485069274902344, Train: 0.7143, Val: 0.3220, Test: 0.3020, Loss2: 1.8164395093917847\n",
      "Epoch: 004, Loss: 1.770589828491211, Train: 0.7143, Val: 0.3400, Test: 0.3360, Loss2: 1.816402792930603\n",
      "Epoch: 005, Loss: 1.696747899055481, Train: 0.8571, Val: 0.3820, Test: 0.3910, Loss2: 1.8162449598312378\n",
      "Epoch: 006, Loss: 1.5720155239105225, Train: 0.8571, Val: 0.4080, Test: 0.4070, Loss2: 1.8161628246307373\n",
      "Epoch: 007, Loss: 1.5604878664016724, Train: 0.8571, Val: 0.3980, Test: 0.4070, Loss2: 1.8162505626678467\n",
      "Epoch: 008, Loss: 1.520721673965454, Train: 0.8571, Val: 0.4040, Test: 0.4070, Loss2: 1.8163783550262451\n",
      "Epoch: 009, Loss: 1.3960915803909302, Train: 0.8571, Val: 0.4280, Test: 0.4210, Loss2: 1.8164405822753906\n",
      "Epoch: 010, Loss: 1.2945082187652588, Train: 0.8571, Val: 0.4300, Test: 0.4240, Loss2: 1.8164271116256714\n",
      "Epoch: 011, Loss: 1.2193655967712402, Train: 0.8571, Val: 0.4200, Test: 0.4240, Loss2: 1.8163745403289795\n",
      "Epoch: 012, Loss: 1.2144533395767212, Train: 0.8571, Val: 0.4120, Test: 0.4240, Loss2: 1.8163299560546875\n",
      "Epoch: 013, Loss: 1.0948885679244995, Train: 0.8571, Val: 0.4100, Test: 0.4240, Loss2: 1.8163297176361084\n",
      "Epoch: 014, Loss: 1.045485019683838, Train: 0.8571, Val: 0.4100, Test: 0.4240, Loss2: 1.8163596391677856\n",
      "Epoch: 015, Loss: 1.0263864994049072, Train: 0.9286, Val: 0.4140, Test: 0.4240, Loss2: 1.8163877725601196\n",
      "Epoch: 016, Loss: 1.268263339996338, Train: 0.9286, Val: 0.4140, Test: 0.4240, Loss2: 1.8163954019546509\n",
      "Epoch: 017, Loss: 0.9786266088485718, Train: 0.9286, Val: 0.4140, Test: 0.4240, Loss2: 1.816393494606018\n",
      "Epoch: 018, Loss: 1.0255972146987915, Train: 0.9286, Val: 0.4360, Test: 0.4190, Loss2: 1.816390037536621\n",
      "Epoch: 019, Loss: 0.9847766160964966, Train: 1.0000, Val: 0.4520, Test: 0.4400, Loss2: 1.816388726234436\n",
      "Epoch: 020, Loss: 0.9531036019325256, Train: 1.0000, Val: 0.4780, Test: 0.4490, Loss2: 1.8163871765136719\n",
      "Epoch: 021, Loss: 0.9302098751068115, Train: 1.0000, Val: 0.4940, Test: 0.4620, Loss2: 1.8163877725601196\n",
      "Epoch: 022, Loss: 0.95317542552948, Train: 1.0000, Val: 0.5000, Test: 0.4750, Loss2: 1.8163928985595703\n",
      "Epoch: 023, Loss: 0.9118430614471436, Train: 1.0000, Val: 0.5000, Test: 0.4750, Loss2: 1.8164002895355225\n",
      "Epoch: 024, Loss: 0.8140639662742615, Train: 1.0000, Val: 0.5020, Test: 0.4990, Loss2: 1.8164079189300537\n",
      "Epoch: 025, Loss: 0.7339834570884705, Train: 1.0000, Val: 0.5080, Test: 0.5120, Loss2: 1.816414475440979\n",
      "Epoch: 026, Loss: 0.7622038722038269, Train: 1.0000, Val: 0.5200, Test: 0.5230, Loss2: 1.81641685962677\n",
      "Epoch: 027, Loss: 0.6437448263168335, Train: 1.0000, Val: 0.5260, Test: 0.5330, Loss2: 1.816415786743164\n",
      "Epoch: 028, Loss: 0.8033430576324463, Train: 1.0000, Val: 0.5440, Test: 0.5530, Loss2: 1.8164129257202148\n",
      "Epoch: 029, Loss: 0.8134193420410156, Train: 1.0000, Val: 0.5500, Test: 0.5580, Loss2: 1.8164117336273193\n",
      "Epoch: 030, Loss: 0.5749416947364807, Train: 1.0000, Val: 0.5500, Test: 0.5580, Loss2: 1.8164145946502686\n",
      "Epoch: 031, Loss: 0.8102988004684448, Train: 1.0000, Val: 0.5500, Test: 0.5580, Loss2: 1.8164194822311401\n",
      "Epoch: 032, Loss: 0.5569902062416077, Train: 1.0000, Val: 0.5440, Test: 0.5580, Loss2: 1.8164221048355103\n",
      "Epoch: 033, Loss: 0.6562177538871765, Train: 1.0000, Val: 0.5420, Test: 0.5580, Loss2: 1.8164207935333252\n",
      "Epoch: 034, Loss: 0.6809122562408447, Train: 1.0000, Val: 0.5380, Test: 0.5580, Loss2: 1.8164187669754028\n",
      "Epoch: 035, Loss: 0.6518678069114685, Train: 1.0000, Val: 0.5420, Test: 0.5580, Loss2: 1.8164184093475342\n",
      "Epoch: 036, Loss: 0.5083659291267395, Train: 1.0000, Val: 0.5360, Test: 0.5580, Loss2: 1.8164196014404297\n",
      "Epoch: 037, Loss: 0.6767719388008118, Train: 1.0000, Val: 0.5340, Test: 0.5580, Loss2: 1.816421389579773\n",
      "Epoch: 038, Loss: 0.3369981348514557, Train: 1.0000, Val: 0.5320, Test: 0.5580, Loss2: 1.8164231777191162\n",
      "Epoch: 039, Loss: 0.38482987880706787, Train: 1.0000, Val: 0.5380, Test: 0.5580, Loss2: 1.816424012184143\n",
      "Epoch: 040, Loss: 0.43540892004966736, Train: 1.0000, Val: 0.5340, Test: 0.5580, Loss2: 1.8164242506027222\n",
      "Epoch: 041, Loss: 0.5373267531394958, Train: 1.0000, Val: 0.5280, Test: 0.5580, Loss2: 1.8164242506027222\n",
      "Epoch: 042, Loss: 0.36861148476600647, Train: 1.0000, Val: 0.5200, Test: 0.5580, Loss2: 1.8164242506027222\n",
      "Epoch: 043, Loss: 0.4511643648147583, Train: 1.0000, Val: 0.5200, Test: 0.5580, Loss2: 1.8164244890213013\n",
      "Epoch: 044, Loss: 0.5302124619483948, Train: 1.0000, Val: 0.5100, Test: 0.5580, Loss2: 1.8164249658584595\n",
      "Epoch: 045, Loss: 0.4009478688240051, Train: 1.0000, Val: 0.5140, Test: 0.5580, Loss2: 1.8164253234863281\n",
      "Epoch: 046, Loss: 0.6260121464729309, Train: 1.0000, Val: 0.5180, Test: 0.5580, Loss2: 1.8164260387420654\n",
      "Epoch: 047, Loss: 0.2696639895439148, Train: 1.0000, Val: 0.5160, Test: 0.5580, Loss2: 1.816426157951355\n",
      "Epoch: 048, Loss: 0.5660464763641357, Train: 1.0000, Val: 0.5160, Test: 0.5580, Loss2: 1.8164262771606445\n",
      "Epoch: 049, Loss: 0.6356236338615417, Train: 1.0000, Val: 0.5200, Test: 0.5580, Loss2: 1.8164262771606445\n",
      "Epoch: 050, Loss: 0.5468799471855164, Train: 1.0000, Val: 0.5220, Test: 0.5580, Loss2: 1.8164265155792236\n",
      "Epoch: 051, Loss: 0.4370204210281372, Train: 1.0000, Val: 0.5240, Test: 0.5580, Loss2: 1.8164266347885132\n",
      "Epoch: 052, Loss: 0.31346675753593445, Train: 1.0000, Val: 0.5280, Test: 0.5580, Loss2: 1.8164271116256714\n",
      "Epoch: 053, Loss: 0.4509592056274414, Train: 1.0000, Val: 0.5260, Test: 0.5580, Loss2: 1.816427230834961\n",
      "Epoch: 054, Loss: 0.5064296722412109, Train: 1.0000, Val: 0.5320, Test: 0.5580, Loss2: 1.816427230834961\n",
      "Epoch: 055, Loss: 0.5462570190429688, Train: 1.0000, Val: 0.5360, Test: 0.5580, Loss2: 1.816427230834961\n",
      "Epoch: 056, Loss: 0.3589331805706024, Train: 1.0000, Val: 0.5440, Test: 0.5580, Loss2: 1.816427230834961\n",
      "Epoch: 057, Loss: 0.3757669925689697, Train: 1.0000, Val: 0.5460, Test: 0.5580, Loss2: 1.8164273500442505\n",
      "Epoch: 058, Loss: 0.3139285743236542, Train: 1.0000, Val: 0.5420, Test: 0.5580, Loss2: 1.81642746925354\n",
      "Epoch: 059, Loss: 0.3197643458843231, Train: 1.0000, Val: 0.5440, Test: 0.5580, Loss2: 1.8164275884628296\n",
      "Epoch: 060, Loss: 0.3278059661388397, Train: 1.0000, Val: 0.5500, Test: 0.5580, Loss2: 1.8164275884628296\n",
      "Epoch: 061, Loss: 0.3321196436882019, Train: 1.0000, Val: 0.5560, Test: 0.5470, Loss2: 1.8164278268814087\n",
      "Epoch: 062, Loss: 0.34140047430992126, Train: 1.0000, Val: 0.5580, Test: 0.5510, Loss2: 1.8164278268814087\n",
      "Epoch: 063, Loss: 0.31201255321502686, Train: 1.0000, Val: 0.5620, Test: 0.5500, Loss2: 1.8164278268814087\n",
      "Epoch: 064, Loss: 0.3492323160171509, Train: 1.0000, Val: 0.5620, Test: 0.5500, Loss2: 1.8164278268814087\n",
      "Epoch: 065, Loss: 0.31270113587379456, Train: 1.0000, Val: 0.5640, Test: 0.5500, Loss2: 1.8164279460906982\n",
      "Epoch: 066, Loss: 0.3361581265926361, Train: 1.0000, Val: 0.5640, Test: 0.5500, Loss2: 1.8164279460906982\n",
      "Epoch: 067, Loss: 0.3663831353187561, Train: 1.0000, Val: 0.5560, Test: 0.5500, Loss2: 1.8164280652999878\n",
      "Epoch: 068, Loss: 0.3270410895347595, Train: 1.0000, Val: 0.5540, Test: 0.5500, Loss2: 1.8164280652999878\n",
      "Epoch: 069, Loss: 0.4114855229854584, Train: 1.0000, Val: 0.5520, Test: 0.5500, Loss2: 1.8164280652999878\n",
      "Epoch: 070, Loss: 0.40023815631866455, Train: 1.0000, Val: 0.5520, Test: 0.5500, Loss2: 1.8164280652999878\n",
      "Epoch: 071, Loss: 0.3142834007740021, Train: 1.0000, Val: 0.5520, Test: 0.5500, Loss2: 1.8164280652999878\n",
      "Epoch: 072, Loss: 0.3187583386898041, Train: 1.0000, Val: 0.5520, Test: 0.5500, Loss2: 1.8164281845092773\n",
      "Epoch: 073, Loss: 0.28982096910476685, Train: 1.0000, Val: 0.5500, Test: 0.5500, Loss2: 1.8164281845092773\n",
      "Epoch: 074, Loss: 0.5728418827056885, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.8164281845092773\n",
      "Epoch: 075, Loss: 0.23704314231872559, Train: 1.0000, Val: 0.5420, Test: 0.5500, Loss2: 1.8164281845092773\n",
      "Epoch: 076, Loss: 0.3501134514808655, Train: 1.0000, Val: 0.5400, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 077, Loss: 0.30284377932548523, Train: 1.0000, Val: 0.5440, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 078, Loss: 0.3967500627040863, Train: 1.0000, Val: 0.5400, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 079, Loss: 0.2984478175640106, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 080, Loss: 0.4197361171245575, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 081, Loss: 0.4261360466480255, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 082, Loss: 0.2568286061286926, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 083, Loss: 0.31727150082588196, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 084, Loss: 0.41573265194892883, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 085, Loss: 0.26580601930618286, Train: 1.0000, Val: 0.5260, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 086, Loss: 0.17744183540344238, Train: 1.0000, Val: 0.5220, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 087, Loss: 0.34377747774124146, Train: 1.0000, Val: 0.5200, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 088, Loss: 0.18631260097026825, Train: 1.0000, Val: 0.5220, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 089, Loss: 0.28489968180656433, Train: 1.0000, Val: 0.5240, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 090, Loss: 0.31997114419937134, Train: 1.0000, Val: 0.5280, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 091, Loss: 0.3084479868412018, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 092, Loss: 0.21694359183311462, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 093, Loss: 0.29804253578186035, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 094, Loss: 0.20843227207660675, Train: 1.0000, Val: 0.5400, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 095, Loss: 0.2618292570114136, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 096, Loss: 0.1793801635503769, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 097, Loss: 0.35969290137290955, Train: 1.0000, Val: 0.5440, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 098, Loss: 0.23678238689899445, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 099, Loss: 0.17840202152729034, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 100, Loss: 0.26214200258255005, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 101, Loss: 0.4101206064224243, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 102, Loss: 0.1515018343925476, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 103, Loss: 0.22989636659622192, Train: 1.0000, Val: 0.5260, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 104, Loss: 0.2722237706184387, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 105, Loss: 0.17167699337005615, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 106, Loss: 0.20793867111206055, Train: 1.0000, Val: 0.5280, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 107, Loss: 0.20464873313903809, Train: 1.0000, Val: 0.5240, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 108, Loss: 0.2588822841644287, Train: 1.0000, Val: 0.5240, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 109, Loss: 0.24661226570606232, Train: 1.0000, Val: 0.5220, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 110, Loss: 0.17829640209674835, Train: 1.0000, Val: 0.5240, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 111, Loss: 0.3257848918437958, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 112, Loss: 0.40307340025901794, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 113, Loss: 0.20187027752399445, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 114, Loss: 0.2384330779314041, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 115, Loss: 0.1845412254333496, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 116, Loss: 0.2771916687488556, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 117, Loss: 0.22202831506729126, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 118, Loss: 0.18091097474098206, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 119, Loss: 0.25726914405822754, Train: 1.0000, Val: 0.5400, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 120, Loss: 0.1625305712223053, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 121, Loss: 0.2904834747314453, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 122, Loss: 0.22838108241558075, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 123, Loss: 0.2762073576450348, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 124, Loss: 0.31549522280693054, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 125, Loss: 0.21191911399364471, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 126, Loss: 0.23393462598323822, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 127, Loss: 0.1443008929491043, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 128, Loss: 0.14326691627502441, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 129, Loss: 0.2716309130191803, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 130, Loss: 0.10604356229305267, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 131, Loss: 0.1582108736038208, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 132, Loss: 0.1310618668794632, Train: 1.0000, Val: 0.5420, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 133, Loss: 0.30286088585853577, Train: 1.0000, Val: 0.5440, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 134, Loss: 0.3522552251815796, Train: 1.0000, Val: 0.5420, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 135, Loss: 0.18626105785369873, Train: 1.0000, Val: 0.5440, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 136, Loss: 0.13947023451328278, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 137, Loss: 0.16939067840576172, Train: 1.0000, Val: 0.5500, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 138, Loss: 0.25525954365730286, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 139, Loss: 0.11948861181735992, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 140, Loss: 0.2284182757139206, Train: 1.0000, Val: 0.5480, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 141, Loss: 0.26723793148994446, Train: 1.0000, Val: 0.5440, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 142, Loss: 0.2498730570077896, Train: 1.0000, Val: 0.5440, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 143, Loss: 0.2422436624765396, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 144, Loss: 0.2768661677837372, Train: 1.0000, Val: 0.5500, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 145, Loss: 0.13632969558238983, Train: 1.0000, Val: 0.5520, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 146, Loss: 0.2918289005756378, Train: 1.0000, Val: 0.5520, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 147, Loss: 0.29265162348747253, Train: 1.0000, Val: 0.5540, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 148, Loss: 0.18270090222358704, Train: 1.0000, Val: 0.5500, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 149, Loss: 0.32417333126068115, Train: 1.0000, Val: 0.5500, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 150, Loss: 0.11424785107374191, Train: 1.0000, Val: 0.5400, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 151, Loss: 0.30045005679130554, Train: 1.0000, Val: 0.5420, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 152, Loss: 0.22380782663822174, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 153, Loss: 0.2428203821182251, Train: 1.0000, Val: 0.5320, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 154, Loss: 0.17182905972003937, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 155, Loss: 0.10914542526006699, Train: 1.0000, Val: 0.5260, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 156, Loss: 0.18411828577518463, Train: 1.0000, Val: 0.5240, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 157, Loss: 0.12230461835861206, Train: 1.0000, Val: 0.5240, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 158, Loss: 0.2900349795818329, Train: 1.0000, Val: 0.5200, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 159, Loss: 0.26401153206825256, Train: 1.0000, Val: 0.5200, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 160, Loss: 0.21475665271282196, Train: 1.0000, Val: 0.5160, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 161, Loss: 0.2380552738904953, Train: 1.0000, Val: 0.5180, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 162, Loss: 0.13398362696170807, Train: 1.0000, Val: 0.5200, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 163, Loss: 0.11060990393161774, Train: 1.0000, Val: 0.5200, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 164, Loss: 0.14873825013637543, Train: 1.0000, Val: 0.5160, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 165, Loss: 0.21976155042648315, Train: 1.0000, Val: 0.5120, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 166, Loss: 0.11621004343032837, Train: 1.0000, Val: 0.5140, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 167, Loss: 0.1788521111011505, Train: 1.0000, Val: 0.5180, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 168, Loss: 0.13792894780635834, Train: 1.0000, Val: 0.5180, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 169, Loss: 0.21997591853141785, Train: 1.0000, Val: 0.5220, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 170, Loss: 0.1393297016620636, Train: 1.0000, Val: 0.5300, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 171, Loss: 0.24854683876037598, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 172, Loss: 0.15639689564704895, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 173, Loss: 0.2844761610031128, Train: 1.0000, Val: 0.5340, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 174, Loss: 0.1319083422422409, Train: 1.0000, Val: 0.5360, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 175, Loss: 0.24119898676872253, Train: 1.0000, Val: 0.5380, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 176, Loss: 0.153132826089859, Train: 1.0000, Val: 0.5460, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 177, Loss: 0.14923083782196045, Train: 1.0000, Val: 0.5480, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 178, Loss: 0.09860025346279144, Train: 1.0000, Val: 0.5520, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 179, Loss: 0.13257423043251038, Train: 1.0000, Val: 0.5560, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 180, Loss: 0.2695663869380951, Train: 1.0000, Val: 0.5580, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 181, Loss: 0.17615079879760742, Train: 1.0000, Val: 0.5580, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 182, Loss: 0.06726302951574326, Train: 1.0000, Val: 0.5600, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 183, Loss: 0.1456013023853302, Train: 1.0000, Val: 0.5580, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 184, Loss: 0.11389245837926865, Train: 1.0000, Val: 0.5580, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 185, Loss: 0.09355530887842178, Train: 1.0000, Val: 0.5560, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 186, Loss: 0.3866078853607178, Train: 1.0000, Val: 0.5540, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 187, Loss: 0.16341814398765564, Train: 1.0000, Val: 0.5580, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 188, Loss: 0.10601500421762466, Train: 1.0000, Val: 0.5600, Test: 0.5500, Loss2: 1.816428303718567\n",
      "Epoch: 189, Loss: 0.09364447742700577, Train: 1.0000, Val: 0.5660, Test: 0.5360, Loss2: 1.816428303718567\n",
      "Epoch: 190, Loss: 0.15965378284454346, Train: 1.0000, Val: 0.5720, Test: 0.5350, Loss2: 1.816428303718567\n",
      "Epoch: 191, Loss: 0.1369340866804123, Train: 1.0000, Val: 0.5680, Test: 0.5350, Loss2: 1.816428303718567\n",
      "Epoch: 192, Loss: 0.1509946882724762, Train: 1.0000, Val: 0.5740, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Epoch: 193, Loss: 0.14329271018505096, Train: 1.0000, Val: 0.5700, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Epoch: 194, Loss: 0.18921257555484772, Train: 1.0000, Val: 0.5700, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Epoch: 195, Loss: 0.13239237666130066, Train: 1.0000, Val: 0.5660, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Epoch: 196, Loss: 0.2682861387729645, Train: 1.0000, Val: 0.5620, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Epoch: 197, Loss: 0.08505788445472717, Train: 1.0000, Val: 0.5520, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Epoch: 198, Loss: 0.15625448524951935, Train: 1.0000, Val: 0.5500, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Epoch: 199, Loss: 0.17340993881225586, Train: 1.0000, Val: 0.5500, Test: 0.5390, Loss2: 1.816428303718567\n",
      "Best Val Acc: 0.5740 Test Acc: 0.5390\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 200):\n",
    "    \n",
    "    optimizer_gcn.zero_grad()\n",
    "    optimizer_alp.zero_grad()\n",
    "\n",
    "    # add 1% of unlabelled data to training data\n",
    "    new_train_mask = train_mask.clone()\n",
    "    to_add_mask = unlab_mask.clone()\n",
    "    # put 99% to 0\n",
    "    idx = to_add_mask.nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))][:int(0.98 * idx.size(0))]\n",
    "    to_add_mask[idx] = False\n",
    "    new_train_mask[to_add_mask] = True\n",
    "    # print(new_train_mask.sum())\n",
    "    \n",
    "    op_alp = model()\n",
    "    op = gcn(data.x, data.edge_index)\n",
    "    \n",
    "    # print(op_alp.shape, op.shape, data.y[train_mask].shape)\n",
    "    loss = F.cross_entropy(op[new_train_mask, :], op_alp[new_train_mask].argmax(dim=1))\n",
    "    # print(op_alp[train_mask].shape, data.y[train_mask].shape)\n",
    "    loss2 = F.cross_entropy(op_alp[train_mask], data.y[train_mask])\n",
    "    # print(loss.item(), loss2.item())\n",
    "    # for p in model.parameters():\n",
    "    #     print(p)\n",
    "    loss += 0.01 * loss2\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer_alp.step()\n",
    "    optimizer_gcn.step()\n",
    "    \n",
    "    train_acc, val_acc, tmp_test_acc = test(gcn, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc, Loss2 = loss2)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 100):\n",
    "    loss = train(gcn, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(gcn, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs.shape, outs[0, 0, :].sum(), (outs[0, 0, :] == outs[2, 0, :]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = final_test_acc = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, 100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc, val_acc, tmp_test_acc = model.test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

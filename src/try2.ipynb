{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "from utils import train, test, edgeindex2adj\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "# pubmed = Planetoid(root='.', name='Pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "from models import ALP, GCN\n",
    "dataset = cora\n",
    "data = dataset[0]\n",
    "\n",
    "# take k random training nodes for each class\n",
    "k = 1\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "print(data.train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "yshape = dataset[0].y.shape[0]\n",
    "model = ALP(num_layers=16, alpha=0.9, yshape=yshape)\n",
    "outs = model.lpa(dataset)\n",
    "\n",
    "gcn = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "optimizer_gcn = torch.optim.Adam(gcn.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "optimizer_alp = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.316, 0.322]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) tensor(500) tensor(1000) tensor(1201)\n"
     ]
    }
   ],
   "source": [
    "train_mask = data.train_mask.clone()\n",
    "val_mask = data.val_mask.clone()\n",
    "test_mask = data.test_mask.clone()\n",
    "\n",
    "unlab_mask = torch.ones_like(data.train_mask)\n",
    "unlab_mask[(data.train_mask == True) | (data.val_mask == True) | (data.test_mask == True)] = False\n",
    "\n",
    "print(train_mask.sum(), val_mask.sum(), test_mask.sum(), unlab_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9535118341445923, Train: 0.1429, Val: 0.1480, Test: 0.1510, Loss2: 1.764222502708435\n",
      "Epoch: 002, Loss: 1.9152154922485352, Train: 0.4286, Val: 0.1700, Test: 0.1580, Loss2: 1.764222502708435\n",
      "Epoch: 003, Loss: 1.8757795095443726, Train: 0.4286, Val: 0.1820, Test: 0.1760, Loss2: 1.764222502708435\n",
      "Epoch: 004, Loss: 1.8489302396774292, Train: 0.5714, Val: 0.1980, Test: 0.1820, Loss2: 1.764222502708435\n",
      "Epoch: 005, Loss: 1.8029606342315674, Train: 0.5714, Val: 0.2060, Test: 0.1930, Loss2: 1.764222502708435\n",
      "Epoch: 006, Loss: 1.755576252937317, Train: 0.5714, Val: 0.2140, Test: 0.2040, Loss2: 1.764222502708435\n",
      "Epoch: 007, Loss: 1.7288048267364502, Train: 0.4286, Val: 0.2160, Test: 0.2020, Loss2: 1.764222502708435\n",
      "Epoch: 008, Loss: 1.690131664276123, Train: 0.4286, Val: 0.2080, Test: 0.2020, Loss2: 1.764222502708435\n",
      "Epoch: 009, Loss: 1.691715955734253, Train: 0.5714, Val: 0.2160, Test: 0.2020, Loss2: 1.764222502708435\n",
      "Epoch: 010, Loss: 1.6302109956741333, Train: 0.5714, Val: 0.2180, Test: 0.2060, Loss2: 1.764222502708435\n",
      "Epoch: 011, Loss: 1.565959095954895, Train: 0.5714, Val: 0.2280, Test: 0.2140, Loss2: 1.764222502708435\n",
      "Epoch: 012, Loss: 1.5971364974975586, Train: 0.5714, Val: 0.2320, Test: 0.2210, Loss2: 1.764222502708435\n",
      "Epoch: 013, Loss: 1.5711690187454224, Train: 0.7143, Val: 0.2360, Test: 0.2330, Loss2: 1.764222502708435\n",
      "Epoch: 014, Loss: 1.519709587097168, Train: 0.7143, Val: 0.2640, Test: 0.2530, Loss2: 1.764222502708435\n",
      "Epoch: 015, Loss: 1.4839273691177368, Train: 0.7143, Val: 0.2820, Test: 0.2730, Loss2: 1.764222502708435\n",
      "Epoch: 016, Loss: 1.425759196281433, Train: 0.7143, Val: 0.2880, Test: 0.2850, Loss2: 1.764222502708435\n",
      "Epoch: 017, Loss: 1.4298677444458008, Train: 0.7143, Val: 0.2840, Test: 0.2850, Loss2: 1.764222502708435\n",
      "Epoch: 018, Loss: 1.3768779039382935, Train: 0.7143, Val: 0.2920, Test: 0.2930, Loss2: 1.764222502708435\n",
      "Epoch: 019, Loss: 1.4017263650894165, Train: 0.7143, Val: 0.2960, Test: 0.3010, Loss2: 1.764222502708435\n",
      "Epoch: 020, Loss: 1.3716408014297485, Train: 0.8571, Val: 0.3000, Test: 0.3070, Loss2: 1.764222502708435\n",
      "Epoch: 021, Loss: 1.3047447204589844, Train: 0.8571, Val: 0.3100, Test: 0.3140, Loss2: 1.764222502708435\n",
      "Epoch: 022, Loss: 1.289954423904419, Train: 0.8571, Val: 0.3240, Test: 0.3230, Loss2: 1.764222502708435\n",
      "Epoch: 023, Loss: 1.285693645477295, Train: 0.8571, Val: 0.3300, Test: 0.3290, Loss2: 1.764222502708435\n",
      "Epoch: 024, Loss: 1.2093477249145508, Train: 0.8571, Val: 0.3280, Test: 0.3290, Loss2: 1.764222502708435\n",
      "Epoch: 025, Loss: 1.206827998161316, Train: 1.0000, Val: 0.3300, Test: 0.3290, Loss2: 1.764222502708435\n",
      "Epoch: 026, Loss: 1.1250461339950562, Train: 1.0000, Val: 0.3320, Test: 0.3390, Loss2: 1.764222502708435\n",
      "Epoch: 027, Loss: 1.1661263704299927, Train: 1.0000, Val: 0.3380, Test: 0.3410, Loss2: 1.764222502708435\n",
      "Epoch: 028, Loss: 1.159052848815918, Train: 1.0000, Val: 0.3380, Test: 0.3410, Loss2: 1.764222502708435\n",
      "Epoch: 029, Loss: 1.113906979560852, Train: 1.0000, Val: 0.3340, Test: 0.3410, Loss2: 1.764222502708435\n",
      "Epoch: 030, Loss: 1.1526999473571777, Train: 1.0000, Val: 0.3380, Test: 0.3410, Loss2: 1.764222502708435\n",
      "Epoch: 031, Loss: 1.0668010711669922, Train: 1.0000, Val: 0.3400, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 032, Loss: 1.0818557739257812, Train: 1.0000, Val: 0.3400, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 033, Loss: 0.9788690805435181, Train: 1.0000, Val: 0.3380, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 034, Loss: 0.9819427132606506, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 035, Loss: 0.9685367345809937, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 036, Loss: 0.9704614877700806, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 037, Loss: 0.9503865838050842, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 038, Loss: 0.9937452673912048, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 039, Loss: 1.0410470962524414, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 040, Loss: 0.8667363524436951, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 041, Loss: 0.8868817090988159, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 042, Loss: 0.8617662787437439, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 043, Loss: 0.8981082439422607, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 044, Loss: 0.9090718626976013, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 045, Loss: 0.8026944398880005, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 046, Loss: 0.8010546565055847, Train: 1.0000, Val: 0.3160, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 047, Loss: 0.8207048177719116, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 048, Loss: 0.7723302245140076, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 049, Loss: 0.7036188244819641, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 050, Loss: 0.6724448204040527, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 051, Loss: 0.7643131613731384, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 052, Loss: 0.7202537655830383, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 053, Loss: 0.74801105260849, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 054, Loss: 0.8009753227233887, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 055, Loss: 0.6424250602722168, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 056, Loss: 0.7068697214126587, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 057, Loss: 0.6520267128944397, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 058, Loss: 0.6413514614105225, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 059, Loss: 0.6303274035453796, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 060, Loss: 0.6501465439796448, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 061, Loss: 0.6434341073036194, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 062, Loss: 0.5256224274635315, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 063, Loss: 0.6088224649429321, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 064, Loss: 0.5115107893943787, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 065, Loss: 0.56364905834198, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 066, Loss: 0.5810589790344238, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 067, Loss: 0.5179614424705505, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 068, Loss: 0.5324229001998901, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 069, Loss: 0.48021024465560913, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 070, Loss: 0.5251941680908203, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 071, Loss: 0.5736885070800781, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 072, Loss: 0.564685583114624, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 073, Loss: 0.5970882773399353, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 074, Loss: 0.4612744152545929, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 075, Loss: 0.49057888984680176, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 076, Loss: 0.5321874022483826, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 077, Loss: 0.5204770565032959, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 078, Loss: 0.5212973952293396, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 079, Loss: 0.47918909788131714, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 080, Loss: 0.5149634480476379, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 081, Loss: 0.462246835231781, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 082, Loss: 0.44447463750839233, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 083, Loss: 0.5037761926651001, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 084, Loss: 0.42643338441848755, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 085, Loss: 0.37486836314201355, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 086, Loss: 0.4427032768726349, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 087, Loss: 0.38587868213653564, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 088, Loss: 0.3928149938583374, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 089, Loss: 0.3635328710079193, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 090, Loss: 0.40520837903022766, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 091, Loss: 0.4624246060848236, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 092, Loss: 0.4292922914028168, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 093, Loss: 0.35794350504875183, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 094, Loss: 0.37855851650238037, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 095, Loss: 0.4464903473854065, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 096, Loss: 0.3704224228858948, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 097, Loss: 0.39923423528671265, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 098, Loss: 0.4020523726940155, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 099, Loss: 0.36441853642463684, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 100, Loss: 0.3784636855125427, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 101, Loss: 0.3565915524959564, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 102, Loss: 0.29058516025543213, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 103, Loss: 0.3430261015892029, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 104, Loss: 0.42693576216697693, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 105, Loss: 0.36455467343330383, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 106, Loss: 0.3144400119781494, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 107, Loss: 0.30344846844673157, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 108, Loss: 0.30321475863456726, Train: 1.0000, Val: 0.3160, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 109, Loss: 0.376679390668869, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 110, Loss: 0.36842307448387146, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 111, Loss: 0.31612154841423035, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 112, Loss: 0.36978766322135925, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 113, Loss: 0.3416564464569092, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 114, Loss: 0.35527145862579346, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 115, Loss: 0.28522583842277527, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 116, Loss: 0.321307510137558, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 117, Loss: 0.3108783960342407, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 118, Loss: 0.2549385726451874, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 119, Loss: 0.32698532938957214, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 120, Loss: 0.3044108748435974, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 121, Loss: 0.26258888840675354, Train: 1.0000, Val: 0.3160, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 122, Loss: 0.27811968326568604, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 123, Loss: 0.316768079996109, Train: 1.0000, Val: 0.3180, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 124, Loss: 0.3164212703704834, Train: 1.0000, Val: 0.3160, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 125, Loss: 0.25251322984695435, Train: 1.0000, Val: 0.3140, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 126, Loss: 0.27314522862434387, Train: 1.0000, Val: 0.3140, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 127, Loss: 0.2775833308696747, Train: 1.0000, Val: 0.3140, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 128, Loss: 0.36827272176742554, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 129, Loss: 0.29046958684921265, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 130, Loss: 0.2543609142303467, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 131, Loss: 0.27996277809143066, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 132, Loss: 0.24041517078876495, Train: 1.0000, Val: 0.3200, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 133, Loss: 0.27115023136138916, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 134, Loss: 0.28481030464172363, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 135, Loss: 0.26001405715942383, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 136, Loss: 0.27316927909851074, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 137, Loss: 0.19920043647289276, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 138, Loss: 0.2805407643318176, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 139, Loss: 0.29141074419021606, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 140, Loss: 0.19083508849143982, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 141, Loss: 0.27987635135650635, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 142, Loss: 0.23201781511306763, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 143, Loss: 0.23848649859428406, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 144, Loss: 0.23483560979366302, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 145, Loss: 0.2685835659503937, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 146, Loss: 0.22771531343460083, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 147, Loss: 0.19686418771743774, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 148, Loss: 0.23554390668869019, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 149, Loss: 0.23310910165309906, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 150, Loss: 0.24625693261623383, Train: 1.0000, Val: 0.3220, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 151, Loss: 0.25939226150512695, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 152, Loss: 0.24255409836769104, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 153, Loss: 0.22178789973258972, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 154, Loss: 0.2286013960838318, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 155, Loss: 0.17668023705482483, Train: 1.0000, Val: 0.3240, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 156, Loss: 0.21782439947128296, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 157, Loss: 0.22232192754745483, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 158, Loss: 0.22611744701862335, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 159, Loss: 0.18781138956546783, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 160, Loss: 0.20934586226940155, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 161, Loss: 0.1903633177280426, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 162, Loss: 0.20798224210739136, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 163, Loss: 0.2116914987564087, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 164, Loss: 0.19329549372196198, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 165, Loss: 0.19505847990512848, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 166, Loss: 0.22543005645275116, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 167, Loss: 0.16802701354026794, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 168, Loss: 0.253334105014801, Train: 1.0000, Val: 0.3380, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 169, Loss: 0.16163848340511322, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 170, Loss: 0.1571318805217743, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 171, Loss: 0.24412783980369568, Train: 1.0000, Val: 0.3360, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 172, Loss: 0.24611306190490723, Train: 1.0000, Val: 0.3380, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 173, Loss: 0.20011498034000397, Train: 1.0000, Val: 0.3380, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 174, Loss: 0.2209453582763672, Train: 1.0000, Val: 0.3400, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 175, Loss: 0.17015908658504486, Train: 1.0000, Val: 0.3380, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 176, Loss: 0.16279378533363342, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 177, Loss: 0.1913006603717804, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 178, Loss: 0.21455271542072296, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 179, Loss: 0.1730998307466507, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 180, Loss: 0.20680853724479675, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 181, Loss: 0.2067691832780838, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 182, Loss: 0.17815887928009033, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 183, Loss: 0.27519699931144714, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 184, Loss: 0.20062416791915894, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 185, Loss: 0.19097769260406494, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 186, Loss: 0.20983995497226715, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 187, Loss: 0.1659563034772873, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 188, Loss: 0.13529355823993683, Train: 1.0000, Val: 0.3340, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 189, Loss: 0.13598385453224182, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 190, Loss: 0.16301916539669037, Train: 1.0000, Val: 0.3320, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 191, Loss: 0.17176562547683716, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 192, Loss: 0.20065435767173767, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 193, Loss: 0.21083393692970276, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 194, Loss: 0.23643001914024353, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 195, Loss: 0.165310800075531, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 196, Loss: 0.2683136761188507, Train: 1.0000, Val: 0.3260, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 197, Loss: 0.1735740602016449, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 198, Loss: 0.20273302495479584, Train: 1.0000, Val: 0.3300, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Epoch: 199, Loss: 0.18951690196990967, Train: 1.0000, Val: 0.3280, Test: 0.3460, Loss2: 1.764222502708435\n",
      "Best Val Acc: 0.3400 Test Acc: 0.3460\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 200):\n",
    "    \n",
    "    optimizer_gcn.zero_grad()\n",
    "    optimizer_alp.zero_grad()\n",
    "\n",
    "    # add 1% of unlabelled data to training data\n",
    "    new_train_mask = train_mask.clone()\n",
    "    to_add_mask = unlab_mask.clone()\n",
    "    # put 99% to 0\n",
    "    idx = to_add_mask.nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))][:int(0.90 * idx.size(0))]\n",
    "    to_add_mask[idx] = False\n",
    "    new_train_mask[to_add_mask] = True\n",
    "    # print(new_train_mask.sum())\n",
    "    \n",
    "    op_alp = model()\n",
    "    op = gcn(data.x, data.edge_index)\n",
    "    \n",
    "    # print(op_alp.shape, op.shape, data.y[train_mask].shape)\n",
    "    loss = F.cross_entropy(op[new_train_mask, :], op_alp[new_train_mask].argmax(dim=1))\n",
    "    # print(op_alp[train_mask].shape, data.y[train_mask].shape)\n",
    "    loss2 = F.cross_entropy(op_alp[train_mask], data.y[train_mask])\n",
    "    # print(loss.item(), loss2.item())\n",
    "    # for p in model.parameters():\n",
    "    #     print(p)\n",
    "    # loss += 0.01 * loss2\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer_alp.step()\n",
    "    optimizer_gcn.step()\n",
    "    \n",
    "    train_acc, val_acc, tmp_test_acc = test(gcn, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc, Loss2 = loss2)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.0025246983859688044, Train: 1.0000, Val: 0.3300, Test: 0.3410\n",
      "Epoch: 002, Loss: 0.0024562592152506113, Train: 1.0000, Val: 0.3300, Test: 0.3410\n",
      "Epoch: 003, Loss: 0.002392831025645137, Train: 1.0000, Val: 0.3320, Test: 0.3410\n",
      "Epoch: 004, Loss: 0.0023339069448411465, Train: 1.0000, Val: 0.3320, Test: 0.3410\n",
      "Epoch: 005, Loss: 0.002279216656461358, Train: 1.0000, Val: 0.3320, Test: 0.3410\n",
      "Epoch: 006, Loss: 0.00222847331315279, Train: 1.0000, Val: 0.3320, Test: 0.3410\n",
      "Epoch: 007, Loss: 0.0021815600339323282, Train: 1.0000, Val: 0.3340, Test: 0.3430\n",
      "Epoch: 008, Loss: 0.00213805353268981, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 009, Loss: 0.002097937511280179, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 010, Loss: 0.002061026869341731, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 011, Loss: 0.002027220791205764, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 012, Loss: 0.0019971493165940046, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 013, Loss: 0.001970031764358282, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 014, Loss: 0.0019449840765446424, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 015, Loss: 0.0019218029920011759, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 016, Loss: 0.0019004384521394968, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 017, Loss: 0.0018807713640853763, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 018, Loss: 0.0018626488745212555, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 019, Loss: 0.001846037688665092, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 020, Loss: 0.0018308019498363137, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 021, Loss: 0.001816805568523705, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 022, Loss: 0.0018041341099888086, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 023, Loss: 0.001792566617950797, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 024, Loss: 0.0017819839995354414, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 025, Loss: 0.0017725226934999228, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 026, Loss: 0.0017640467267483473, Train: 1.0000, Val: 0.3380, Test: 0.3440\n",
      "Epoch: 027, Loss: 0.0017564028967171907, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 028, Loss: 0.0017495575593784451, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 029, Loss: 0.0017435279441997409, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 030, Loss: 0.001738092745654285, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 031, Loss: 0.001733388053253293, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 032, Loss: 0.001729210140183568, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 033, Loss: 0.0017257117433473468, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 034, Loss: 0.0017226722557097673, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 035, Loss: 0.0017201595474034548, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 036, Loss: 0.0017181053990498185, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 037, Loss: 0.001716459053568542, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 038, Loss: 0.0017156797694042325, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 039, Loss: 0.0017151383217424154, Train: 1.0000, Val: 0.3400, Test: 0.3460\n",
      "Epoch: 040, Loss: 0.0017148173647001386, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 041, Loss: 0.0017147001344710588, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 042, Loss: 0.0017146848840638995, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 043, Loss: 0.0017149582272395492, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 044, Loss: 0.0017152826767414808, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 045, Loss: 0.0017159468261525035, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 046, Loss: 0.0017168662743642926, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 047, Loss: 0.0017179043497890234, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 048, Loss: 0.0017190444050356746, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 049, Loss: 0.0017203204333782196, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 050, Loss: 0.0017216306878253818, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 051, Loss: 0.0017229914665222168, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 052, Loss: 0.0017244033515453339, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 053, Loss: 0.001725866342894733, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 054, Loss: 0.0017274143174290657, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 055, Loss: 0.001729115261696279, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 056, Loss: 0.0017308330861851573, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 057, Loss: 0.0017324995715171099, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 058, Loss: 0.001733996206894517, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 059, Loss: 0.0017354589654132724, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 060, Loss: 0.001737006357870996, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 061, Loss: 0.0017386049730703235, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 062, Loss: 0.0017402374651283026, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 063, Loss: 0.0017417512135580182, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 064, Loss: 0.0017433498287573457, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 065, Loss: 0.0017448461148887873, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 066, Loss: 0.0017463936237618327, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 067, Loss: 0.0017479922389611602, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 068, Loss: 0.0017496758373454213, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 069, Loss: 0.0017513252096250653, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 070, Loss: 0.0017529405886307359, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 071, Loss: 0.0017545901937410235, Train: 1.0000, Val: 0.3400, Test: 0.3470\n",
      "Epoch: 072, Loss: 0.0017562395660206676, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 073, Loss: 0.0017580081475898623, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 074, Loss: 0.0017596407560631633, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 075, Loss: 0.0017613412346690893, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 076, Loss: 0.0017629906069487333, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 077, Loss: 0.0017645377665758133, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 078, Loss: 0.0017660853918641806, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 079, Loss: 0.001767564914189279, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 080, Loss: 0.0017690954264253378, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 081, Loss: 0.0017705747159197927, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 082, Loss: 0.0017721221083775163, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 083, Loss: 0.0017735165311023593, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 084, Loss: 0.001774808974005282, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 085, Loss: 0.001776084303855896, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 086, Loss: 0.0017773086437955499, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 087, Loss: 0.0017785669770091772, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 088, Loss: 0.0017797064501792192, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 089, Loss: 0.001780811813659966, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 090, Loss: 0.001781883300282061, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 091, Loss: 0.001782988547347486, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 092, Loss: 0.0017839920474216342, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 093, Loss: 0.0017851144075393677, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 094, Loss: 0.0017861007945612073, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 095, Loss: 0.0017870701849460602, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 096, Loss: 0.0017880905652418733, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 097, Loss: 0.001789144822396338, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 098, Loss: 0.0017900801030918956, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Epoch: 099, Loss: 0.0017909473972395062, Train: 1.0000, Val: 0.3420, Test: 0.3470\n",
      "Best Val Acc: 0.3420 Test Acc: 0.3470\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 100):\n",
    "    loss = train(gcn, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(gcn, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take k random training nodes for each class\n",
    "# data = dataset[0]\n",
    "# k = 2\n",
    "# for c in data.y.unique():\n",
    "#     idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "#     idx = idx[torch.randperm(idx.size(0))]\n",
    "#     idx = idx[k:]\n",
    "#     data.train_mask[idx] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 2.660041332244873, Train: 1.0000, Val: 0.4980, Test: 0.4870, Loss2: 1.290296196937561\n",
      "Epoch: 040, Loss: 2.4553756713867188, Train: 1.0000, Val: 0.4740, Test: 0.5050, Loss2: 1.2859899997711182\n",
      "Epoch: 060, Loss: 2.4502530097961426, Train: 1.0000, Val: 0.4740, Test: 0.5050, Loss2: 1.28054678440094\n",
      "Epoch: 080, Loss: 2.440019130706787, Train: 1.0000, Val: 0.4640, Test: 0.5050, Loss2: 1.274596929550171\n",
      "Epoch: 100, Loss: 2.4339590072631836, Train: 1.0000, Val: 0.4420, Test: 0.5050, Loss2: 1.2685365676879883\n",
      "Epoch: 120, Loss: 2.4279847145080566, Train: 1.0000, Val: 0.4380, Test: 0.5050, Loss2: 1.2625625133514404\n",
      "Epoch: 140, Loss: 2.422208786010742, Train: 1.0000, Val: 0.4300, Test: 0.5050, Loss2: 1.256786584854126\n",
      "Epoch: 160, Loss: 2.4167556762695312, Train: 1.0000, Val: 0.4240, Test: 0.5050, Loss2: 1.2512913942337036\n",
      "Epoch: 180, Loss: 2.411555290222168, Train: 1.0000, Val: 0.4200, Test: 0.5050, Loss2: 1.246132731437683\n",
      "Best Val Acc: 0.5280 Test Acc: 0.5050\n"
     ]
    }
   ],
   "source": [
    "from models import GCNLPA\n",
    "\n",
    "dataset = cora\n",
    "data = dataset[0]\n",
    "\n",
    "k = 2\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "model = GCNLPA(dataset.num_features, 16, dataset.num_classes, dataset.edge_index.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 200):\n",
    "    op, op_lpa = model(data)\n",
    "    \n",
    "    loss1 = F.cross_entropy(op[data.train_mask], data.y[data.train_mask])\n",
    "    # print(op_alp[train_mask].shape, data.y[train_mask].shape)\n",
    "    loss2 = F.cross_entropy(op_lpa[data.train_mask], data.y[data.train_mask])\n",
    "    # print(loss.item(), loss2.item())\n",
    "    # for p in model.parameters():\n",
    "    #     print(p)\n",
    "    loss = loss1 + loss2\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "    train_acc, val_acc, tmp_test_acc = model.test(data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    if epoch % 20 == 0:\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc, Loss2 = loss2)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs.shape, outs[0, 0, :].sum(), (outs[0, 0, :] == outs[2, 0, :]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = final_test_acc = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, 100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc, val_acc, tmp_test_acc = model.test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

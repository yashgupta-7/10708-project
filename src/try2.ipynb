{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "from utils import train, test, edgeindex2adj\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "# pubmed = Planetoid(root='.', name='Pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30)\n"
     ]
    }
   ],
   "source": [
    "from models import ALP, GCN\n",
    "dataset = citeseer\n",
    "data = dataset[0]\n",
    "\n",
    "# take k random training nodes for each class\n",
    "k = 5\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "print(data.train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yshape = dataset[0].y.shape[0]\n",
    "model = ALP(num_layers=16, alpha=0.9, yshape=yshape)\n",
    "outs = model.lpa(dataset)\n",
    "\n",
    "gcn = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "optimizer_gcn = torch.optim.Adam(gcn.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "optimizer_alp = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.386, 0.351]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30) tensor(500) tensor(1000) tensor(1797)\n"
     ]
    }
   ],
   "source": [
    "train_mask = data.train_mask.clone()\n",
    "val_mask = data.val_mask.clone()\n",
    "test_mask = data.test_mask.clone()\n",
    "\n",
    "unlab_mask = torch.ones_like(data.train_mask)\n",
    "unlab_mask[(data.train_mask == True) | (data.val_mask == True) | (data.test_mask == True)] = False\n",
    "\n",
    "print(train_mask.sum(), val_mask.sum(), test_mask.sum(), unlab_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.7877354621887207, Train: 0.1667, Val: 0.2340, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 002, Loss: 1.7337859869003296, Train: 0.1667, Val: 0.2320, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 003, Loss: 1.6900779008865356, Train: 0.1667, Val: 0.2320, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 004, Loss: 1.612727165222168, Train: 0.2000, Val: 0.2320, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 005, Loss: 1.5861579179763794, Train: 0.2333, Val: 0.2320, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 006, Loss: 1.5180915594100952, Train: 0.2333, Val: 0.2340, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 007, Loss: 1.4751678705215454, Train: 0.2333, Val: 0.2340, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 008, Loss: 1.4293628931045532, Train: 0.2667, Val: 0.2340, Test: 0.1820, Loss2: 1.5568794012069702\n",
      "Epoch: 009, Loss: 1.3959996700286865, Train: 0.3000, Val: 0.2440, Test: 0.1900, Loss2: 1.5568794012069702\n",
      "Epoch: 010, Loss: 1.3738919496536255, Train: 0.3000, Val: 0.2860, Test: 0.2260, Loss2: 1.5568794012069702\n",
      "Epoch: 011, Loss: 1.3315176963806152, Train: 0.3667, Val: 0.3300, Test: 0.2570, Loss2: 1.5568794012069702\n",
      "Epoch: 012, Loss: 1.2482763528823853, Train: 0.5000, Val: 0.3500, Test: 0.2870, Loss2: 1.5568794012069702\n",
      "Epoch: 013, Loss: 1.2237468957901, Train: 0.5333, Val: 0.3700, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 014, Loss: 1.127253770828247, Train: 0.5667, Val: 0.3660, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 015, Loss: 1.1840195655822754, Train: 0.7000, Val: 0.3580, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 016, Loss: 1.116086721420288, Train: 0.7333, Val: 0.3500, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 017, Loss: 1.055341124534607, Train: 0.7667, Val: 0.3520, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 018, Loss: 1.049507737159729, Train: 0.8000, Val: 0.3480, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 019, Loss: 0.9567543864250183, Train: 0.8000, Val: 0.3380, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 020, Loss: 0.9088644981384277, Train: 0.8000, Val: 0.3380, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 021, Loss: 0.8745272159576416, Train: 0.7667, Val: 0.3340, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 022, Loss: 0.8129775524139404, Train: 0.7667, Val: 0.3340, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 023, Loss: 0.8250231146812439, Train: 0.7667, Val: 0.3360, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 024, Loss: 0.7909556031227112, Train: 0.7667, Val: 0.3340, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 025, Loss: 0.6950749158859253, Train: 0.7667, Val: 0.3340, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 026, Loss: 0.7658217549324036, Train: 0.8000, Val: 0.3340, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 027, Loss: 0.6657402515411377, Train: 0.8000, Val: 0.3380, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 028, Loss: 0.6980000734329224, Train: 0.8333, Val: 0.3380, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 029, Loss: 0.594762921333313, Train: 0.8333, Val: 0.3400, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 030, Loss: 0.5284069776535034, Train: 0.8333, Val: 0.3480, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 031, Loss: 0.6001794934272766, Train: 0.8333, Val: 0.3560, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 032, Loss: 0.5626647472381592, Train: 0.8333, Val: 0.3620, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 033, Loss: 0.5404878258705139, Train: 0.8333, Val: 0.3680, Test: 0.3230, Loss2: 1.5568794012069702\n",
      "Epoch: 034, Loss: 0.5089483857154846, Train: 0.8333, Val: 0.3760, Test: 0.3370, Loss2: 1.5568794012069702\n",
      "Epoch: 035, Loss: 0.4820949137210846, Train: 0.8333, Val: 0.3760, Test: 0.3370, Loss2: 1.5568794012069702\n",
      "Epoch: 036, Loss: 0.4842182397842407, Train: 0.8333, Val: 0.3780, Test: 0.3380, Loss2: 1.5568794012069702\n",
      "Epoch: 037, Loss: 0.4691372215747833, Train: 0.8000, Val: 0.3860, Test: 0.3390, Loss2: 1.5568794012069702\n",
      "Epoch: 038, Loss: 0.44331803917884827, Train: 0.8000, Val: 0.3860, Test: 0.3390, Loss2: 1.5568794012069702\n",
      "Epoch: 039, Loss: 0.3771511912345886, Train: 0.8000, Val: 0.3860, Test: 0.3390, Loss2: 1.5568794012069702\n",
      "Epoch: 040, Loss: 0.3624124526977539, Train: 0.8000, Val: 0.3880, Test: 0.3420, Loss2: 1.5568794012069702\n",
      "Epoch: 041, Loss: 0.3622438609600067, Train: 0.8000, Val: 0.3880, Test: 0.3420, Loss2: 1.5568794012069702\n",
      "Epoch: 042, Loss: 0.38697847723960876, Train: 0.8333, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 043, Loss: 0.3506712317466736, Train: 0.8333, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 044, Loss: 0.33554473519325256, Train: 0.8333, Val: 0.3940, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 045, Loss: 0.32529017329216003, Train: 0.8333, Val: 0.3960, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 046, Loss: 0.3330082595348358, Train: 0.8333, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 047, Loss: 0.29533007740974426, Train: 0.9000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 048, Loss: 0.28371188044548035, Train: 0.9333, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 049, Loss: 0.27722838521003723, Train: 0.9667, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 050, Loss: 0.31234028935432434, Train: 0.9667, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 051, Loss: 0.27976280450820923, Train: 0.9667, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 052, Loss: 0.2671048045158386, Train: 0.9667, Val: 0.3940, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 053, Loss: 0.24421891570091248, Train: 0.9667, Val: 0.3940, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 054, Loss: 0.2756389081478119, Train: 0.9667, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 055, Loss: 0.23198699951171875, Train: 0.9667, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 056, Loss: 0.2640310227870941, Train: 0.9667, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 057, Loss: 0.22501398622989655, Train: 0.9667, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 058, Loss: 0.20719420909881592, Train: 0.9667, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 059, Loss: 0.23609717190265656, Train: 0.9667, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 060, Loss: 0.20762217044830322, Train: 0.9667, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 061, Loss: 0.23302990198135376, Train: 0.9667, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 062, Loss: 0.22196589410305023, Train: 0.9667, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 063, Loss: 0.20440301299095154, Train: 0.9667, Val: 0.3820, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 064, Loss: 0.1904396265745163, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 065, Loss: 0.16060632467269897, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 066, Loss: 0.2249125838279724, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 067, Loss: 0.1917923092842102, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 068, Loss: 0.15183275938034058, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 069, Loss: 0.18149633705615997, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 070, Loss: 0.18428857624530792, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 071, Loss: 0.1493816077709198, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 072, Loss: 0.13948296010494232, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 073, Loss: 0.15185503661632538, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 074, Loss: 0.1347116082906723, Train: 1.0000, Val: 0.3820, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 075, Loss: 0.14969423413276672, Train: 1.0000, Val: 0.3820, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 076, Loss: 0.1313028484582901, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 077, Loss: 0.12239371985197067, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 078, Loss: 0.11939256638288498, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 079, Loss: 0.11261352151632309, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 080, Loss: 0.10607197135686874, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 081, Loss: 0.10445015877485275, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 082, Loss: 0.11496758460998535, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 083, Loss: 0.128406822681427, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 084, Loss: 0.1054489016532898, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 085, Loss: 0.12029918283224106, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 086, Loss: 0.10796599835157394, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 087, Loss: 0.1028556376695633, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 088, Loss: 0.12080204486846924, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 089, Loss: 0.1073589026927948, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 090, Loss: 0.09319654852151871, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 091, Loss: 0.09949146211147308, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 092, Loss: 0.08858618885278702, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 093, Loss: 0.11291096359491348, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 094, Loss: 0.10966615378856659, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 095, Loss: 0.08812140673398972, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 096, Loss: 0.09473428875207901, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 097, Loss: 0.11472457647323608, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 098, Loss: 0.10397545993328094, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 099, Loss: 0.09456157684326172, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 100, Loss: 0.09021978080272675, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 101, Loss: 0.06819775700569153, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 102, Loss: 0.09368850290775299, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 103, Loss: 0.09973111003637314, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 104, Loss: 0.07861517369747162, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 105, Loss: 0.0909774899482727, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 106, Loss: 0.0837322473526001, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 107, Loss: 0.08808007091283798, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 108, Loss: 0.10495584458112717, Train: 1.0000, Val: 0.3940, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 109, Loss: 0.10785439610481262, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 110, Loss: 0.06459498405456543, Train: 1.0000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 111, Loss: 0.07947871834039688, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 112, Loss: 0.0873887687921524, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 113, Loss: 0.08618868887424469, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 114, Loss: 0.07298462092876434, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 115, Loss: 0.07561364024877548, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 116, Loss: 0.07950231432914734, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 117, Loss: 0.0911390632390976, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 118, Loss: 0.08509520441293716, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 119, Loss: 0.10340280830860138, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 120, Loss: 0.07445856928825378, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 121, Loss: 0.07334645837545395, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 122, Loss: 0.06443727761507034, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 123, Loss: 0.06909140944480896, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 124, Loss: 0.06929132342338562, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 125, Loss: 0.057457782328128815, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 126, Loss: 0.06801555305719376, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 127, Loss: 0.0579434335231781, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 128, Loss: 0.06750555336475372, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 129, Loss: 0.06404786556959152, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 130, Loss: 0.07220669090747833, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 131, Loss: 0.076250359416008, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 132, Loss: 0.05294479802250862, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 133, Loss: 0.07251255959272385, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 134, Loss: 0.09143084287643433, Train: 1.0000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 135, Loss: 0.08933039754629135, Train: 1.0000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 136, Loss: 0.05510424077510834, Train: 1.0000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 137, Loss: 0.06699938327074051, Train: 1.0000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 138, Loss: 0.06005672365427017, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 139, Loss: 0.06061265245079994, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 140, Loss: 0.07391658425331116, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 141, Loss: 0.06993301212787628, Train: 1.0000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 142, Loss: 0.07128515839576721, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 143, Loss: 0.05748719722032547, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 144, Loss: 0.05634243041276932, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 145, Loss: 0.06600884348154068, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 146, Loss: 0.06506410241127014, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 147, Loss: 0.05952129885554314, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 148, Loss: 0.052569590508937836, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 149, Loss: 0.057252783328294754, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 150, Loss: 0.06750208884477615, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 151, Loss: 0.058617837727069855, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 152, Loss: 0.07575695961713791, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 153, Loss: 0.06655862927436829, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 154, Loss: 0.055643413215875626, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 155, Loss: 0.06288757175207138, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 156, Loss: 0.04612842574715614, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 157, Loss: 0.061075609177351, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 158, Loss: 0.057744577527046204, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 159, Loss: 0.047324106097221375, Train: 1.0000, Val: 0.3820, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 160, Loss: 0.056344952434301376, Train: 1.0000, Val: 0.3820, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 161, Loss: 0.058065496385097504, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 162, Loss: 0.048409681767225266, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 163, Loss: 0.04958546161651611, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 164, Loss: 0.05137209966778755, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 165, Loss: 0.061974529176950455, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 166, Loss: 0.05844002217054367, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 167, Loss: 0.050661612302064896, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 168, Loss: 0.06021418794989586, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 169, Loss: 0.04958002269268036, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 170, Loss: 0.05628109350800514, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 171, Loss: 0.07599484920501709, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 172, Loss: 0.0569528266787529, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 173, Loss: 0.05518975108861923, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 174, Loss: 0.05354699119925499, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 175, Loss: 0.06698974221944809, Train: 1.0000, Val: 0.3840, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 176, Loss: 0.07251109927892685, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 177, Loss: 0.061482276767492294, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 178, Loss: 0.053838834166526794, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 179, Loss: 0.04977192357182503, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 180, Loss: 0.058021582663059235, Train: 1.0000, Val: 0.3920, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 181, Loss: 0.050512347370386124, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 182, Loss: 0.044775743037462234, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 183, Loss: 0.043051302433013916, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 184, Loss: 0.048593487590551376, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 185, Loss: 0.05317372828722, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 186, Loss: 0.053965192288160324, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 187, Loss: 0.03487745672464371, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 188, Loss: 0.044053155928850174, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 189, Loss: 0.04493958503007889, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 190, Loss: 0.04125136137008667, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 191, Loss: 0.040279097855091095, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 192, Loss: 0.050462376326322556, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 193, Loss: 0.047040753066539764, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 194, Loss: 0.04233201965689659, Train: 1.0000, Val: 0.3860, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 195, Loss: 0.04336269944906235, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 196, Loss: 0.04950803518295288, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 197, Loss: 0.052199527621269226, Train: 1.0000, Val: 0.3880, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 198, Loss: 0.048444319516420364, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Epoch: 199, Loss: 0.04484041407704353, Train: 1.0000, Val: 0.3900, Test: 0.3430, Loss2: 1.5568794012069702\n",
      "Best Val Acc: 0.3960 Test Acc: 0.3430\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 200):\n",
    "    \n",
    "    optimizer_gcn.zero_grad()\n",
    "    optimizer_alp.zero_grad()\n",
    "\n",
    "    # add 1% of unlabelled data to training data\n",
    "    new_train_mask = train_mask.clone()\n",
    "    to_add_mask = unlab_mask.clone()\n",
    "    # put 99% to 0\n",
    "    idx = to_add_mask.nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))][:int(0.90 * idx.size(0))]\n",
    "    to_add_mask[idx] = False\n",
    "    new_train_mask[to_add_mask] = True\n",
    "    # print(new_train_mask.sum())\n",
    "    \n",
    "    op_alp = model()\n",
    "    op = gcn(data.x, data.edge_index)\n",
    "    \n",
    "    # print(op_alp.shape, op.shape, data.y[train_mask].shape)\n",
    "    loss = F.cross_entropy(op[new_train_mask, :], op_alp[new_train_mask].argmax(dim=1))\n",
    "    # print(op_alp[train_mask].shape, data.y[train_mask].shape)\n",
    "    loss2 = F.cross_entropy(op_alp[train_mask], data.y[train_mask])\n",
    "    # print(loss.item(), loss2.item())\n",
    "    # for p in model.parameters():\n",
    "    #     print(p)\n",
    "    # loss += 0.01 * loss2\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer_alp.step()\n",
    "    optimizer_gcn.step()\n",
    "    \n",
    "    train_acc, val_acc, tmp_test_acc = test(gcn, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc, Loss2 = loss2)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.0236421637237072, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 002, Loss: 0.022779876366257668, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 003, Loss: 0.021996216848492622, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 004, Loss: 0.021289195865392685, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 005, Loss: 0.020636949688196182, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 006, Loss: 0.020023580640554428, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 007, Loss: 0.01945175975561142, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 008, Loss: 0.018910497426986694, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 009, Loss: 0.01840626448392868, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 010, Loss: 0.017919963225722313, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 011, Loss: 0.0174688920378685, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 012, Loss: 0.01704259403049946, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 013, Loss: 0.016639430075883865, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 014, Loss: 0.016256647184491158, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 015, Loss: 0.015892481431365013, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 016, Loss: 0.015546214766800404, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 017, Loss: 0.015218464657664299, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 018, Loss: 0.014909183606505394, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 019, Loss: 0.014611548744142056, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 020, Loss: 0.014325886033475399, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 021, Loss: 0.01405305229127407, Train: 1.0000, Val: 0.3880, Test: 0.3430\n",
      "Epoch: 022, Loss: 0.013792263343930244, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 023, Loss: 0.013545273803174496, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 024, Loss: 0.01330997422337532, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 025, Loss: 0.013085132464766502, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 026, Loss: 0.012870400212705135, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 027, Loss: 0.012664980255067348, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 028, Loss: 0.01246734894812107, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 029, Loss: 0.012278181500732899, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 030, Loss: 0.012096179649233818, Train: 1.0000, Val: 0.3900, Test: 0.3430\n",
      "Epoch: 031, Loss: 0.011922717094421387, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 032, Loss: 0.011756731197237968, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 033, Loss: 0.011597511358559132, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 034, Loss: 0.011445102281868458, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 035, Loss: 0.011297529563307762, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 036, Loss: 0.011155798099935055, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 037, Loss: 0.011019501835107803, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 038, Loss: 0.01088786218315363, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 039, Loss: 0.010761136189103127, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 040, Loss: 0.010638892650604248, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 041, Loss: 0.010520437732338905, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 042, Loss: 0.01040599960833788, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 043, Loss: 0.010295315645635128, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 044, Loss: 0.010188919492065907, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 045, Loss: 0.010086149908602238, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 046, Loss: 0.009986587800085545, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 047, Loss: 0.00989018939435482, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 048, Loss: 0.009796734899282455, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 049, Loss: 0.009706628508865833, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 050, Loss: 0.009618865326046944, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 051, Loss: 0.00953349843621254, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 052, Loss: 0.00945083238184452, Train: 1.0000, Val: 0.3920, Test: 0.3460\n",
      "Epoch: 053, Loss: 0.009371130727231503, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 054, Loss: 0.009293599985539913, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 055, Loss: 0.009220321662724018, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 056, Loss: 0.009145112708210945, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 057, Loss: 0.009074006229639053, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 058, Loss: 0.009004605934023857, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 059, Loss: 0.00893723126500845, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 060, Loss: 0.00887233391404152, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 061, Loss: 0.008809631690382957, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 062, Loss: 0.008747545070946217, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 063, Loss: 0.008686007000505924, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 064, Loss: 0.008626564405858517, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 065, Loss: 0.008568303659558296, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 066, Loss: 0.008511011488735676, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 067, Loss: 0.008455165661871433, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 068, Loss: 0.008401313796639442, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 069, Loss: 0.008348648436367512, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 070, Loss: 0.008298330008983612, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 071, Loss: 0.00824730284512043, Train: 1.0000, Val: 0.3940, Test: 0.3470\n",
      "Epoch: 072, Loss: 0.008198421448469162, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 073, Loss: 0.008150198496878147, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 074, Loss: 0.008103066124022007, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 075, Loss: 0.008056383579969406, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 076, Loss: 0.008010868914425373, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 077, Loss: 0.00796655286103487, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 078, Loss: 0.007923430763185024, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 079, Loss: 0.007880703546106815, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 080, Loss: 0.007838945835828781, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 081, Loss: 0.007797916419804096, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 082, Loss: 0.007757312618196011, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 083, Loss: 0.007717841304838657, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 084, Loss: 0.007679037284106016, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 085, Loss: 0.007640928961336613, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 086, Loss: 0.007603236939758062, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 087, Loss: 0.007566383574157953, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 088, Loss: 0.007531158160418272, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 089, Loss: 0.007494748570024967, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 090, Loss: 0.00745967123657465, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 091, Loss: 0.007425145246088505, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 092, Loss: 0.0073911570943892, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 093, Loss: 0.007358021102845669, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 094, Loss: 0.007324935868382454, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 095, Loss: 0.007292161229997873, Train: 1.0000, Val: 0.3980, Test: 0.3500\n",
      "Epoch: 096, Loss: 0.007260305341333151, Train: 1.0000, Val: 0.4000, Test: 0.3530\n",
      "Epoch: 097, Loss: 0.00722925690934062, Train: 1.0000, Val: 0.4000, Test: 0.3530\n",
      "Epoch: 098, Loss: 0.007198319770395756, Train: 1.0000, Val: 0.4000, Test: 0.3530\n",
      "Epoch: 099, Loss: 0.007167734205722809, Train: 1.0000, Val: 0.4000, Test: 0.3530\n",
      "Best Val Acc: 0.4000 Test Acc: 0.3530\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 100):\n",
    "    loss = train(gcn, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(gcn, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take k random training nodes for each class\n",
    "# data = dataset[0]\n",
    "# k = 2\n",
    "# for c in data.y.unique():\n",
    "#     idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "#     idx = idx[torch.randperm(idx.size(0))]\n",
    "#     idx = idx[k:]\n",
    "#     data.train_mask[idx] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 2.8510704040527344, Train: 0.9286, Val: 0.2360, Test: 0.2470, Loss2: 1.4198877811431885\n",
      "Epoch: 040, Loss: 2.5801124572753906, Train: 1.0000, Val: 0.2520, Test: 0.2470, Loss2: 1.4051464796066284\n",
      "Epoch: 060, Loss: 2.5570645332336426, Train: 1.0000, Val: 0.3680, Test: 0.3610, Loss2: 1.388647437095642\n",
      "Epoch: 080, Loss: 2.53702712059021, Train: 1.0000, Val: 0.4420, Test: 0.4390, Loss2: 1.3714197874069214\n",
      "Epoch: 100, Loss: 2.520162343978882, Train: 1.0000, Val: 0.4280, Test: 0.4390, Loss2: 1.3543814420700073\n",
      "Epoch: 120, Loss: 2.503467082977295, Train: 1.0000, Val: 0.3860, Test: 0.4390, Loss2: 1.3380448818206787\n",
      "Epoch: 140, Loss: 2.488126516342163, Train: 1.0000, Val: 0.3520, Test: 0.4390, Loss2: 1.3227041959762573\n",
      "Epoch: 160, Loss: 2.4739537239074707, Train: 1.0000, Val: 0.3320, Test: 0.4390, Loss2: 1.308531403541565\n",
      "Epoch: 180, Loss: 2.4610157012939453, Train: 1.0000, Val: 0.3120, Test: 0.4390, Loss2: 1.295593500137329\n",
      "Best Val Acc: 0.4500 Test Acc: 0.4390\n"
     ]
    }
   ],
   "source": [
    "from models import GCNLPA\n",
    "\n",
    "dataset = cora\n",
    "data = dataset[0]\n",
    "\n",
    "k = 2\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "model = GCNLPA(dataset.num_features, 16, dataset.num_classes, dataset.edge_index.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 200):\n",
    "    op, op_lpa = model(data)\n",
    "    \n",
    "    loss1 = F.cross_entropy(op[data.train_mask], data.y[data.train_mask])\n",
    "    # print(op_alp[train_mask].shape, data.y[train_mask].shape)\n",
    "    loss2 = F.cross_entropy(op_lpa[data.train_mask], data.y[data.train_mask])\n",
    "    # print(loss.item(), loss2.item())\n",
    "    # for p in model.parameters():\n",
    "    #     print(p)\n",
    "    loss = loss1 + loss2\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "    train_acc, val_acc, tmp_test_acc = model.test(data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    if epoch % 20 == 0:\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc, Loss2 = loss2)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs.shape, outs[0, 0, :].sum(), (outs[0, 0, :] == outs[2, 0, :]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GCNLPA' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/yashgupta/Desktop/10708-project/src/try2.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try2.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try2.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try2.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try2.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try2.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GCNLPA' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "best_val_acc = final_test_acc = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, 100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc, val_acc, tmp_test_acc = model.test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

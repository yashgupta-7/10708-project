{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, edgeindex2adj\n",
    "from models import GCN, GAT\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 24, dataset.num_classes)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "dataset = cora\n",
    "ei = dataset[0].edge_index\n",
    "adj = edgeindex2adj(ei, dataset.x.shape[0])\n",
    "model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4, adj=adj)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.2466883659362793, Train: 0.3500, Val: 0.2700, Test: 0.2780\n",
      "Epoch: 002, Loss: 2.14047908782959, Train: 0.3571, Val: 0.2940, Test: 0.2660\n",
      "Epoch: 003, Loss: 2.0529446601867676, Train: 0.4143, Val: 0.3200, Test: 0.3020\n",
      "Epoch: 004, Loss: 1.954749345779419, Train: 0.4786, Val: 0.3880, Test: 0.3550\n",
      "Epoch: 005, Loss: 1.9607235193252563, Train: 0.5786, Val: 0.4240, Test: 0.4040\n",
      "Epoch: 006, Loss: 1.9328062534332275, Train: 0.7000, Val: 0.5100, Test: 0.4780\n",
      "Epoch: 007, Loss: 1.8627859354019165, Train: 0.7857, Val: 0.5740, Test: 0.5520\n",
      "Epoch: 008, Loss: 1.8405307531356812, Train: 0.8643, Val: 0.6400, Test: 0.6380\n",
      "Epoch: 009, Loss: 1.8063565492630005, Train: 0.9000, Val: 0.6880, Test: 0.6820\n",
      "Epoch: 010, Loss: 1.7706124782562256, Train: 0.9429, Val: 0.7240, Test: 0.7350\n",
      "Epoch: 011, Loss: 1.784828543663025, Train: 0.9500, Val: 0.7320, Test: 0.7470\n",
      "Epoch: 012, Loss: 1.7852877378463745, Train: 0.9571, Val: 0.7360, Test: 0.7530\n",
      "Epoch: 013, Loss: 1.782850980758667, Train: 0.9786, Val: 0.7560, Test: 0.7580\n",
      "Epoch: 014, Loss: 1.7275893688201904, Train: 0.9786, Val: 0.7560, Test: 0.7580\n",
      "Epoch: 015, Loss: 1.6578807830810547, Train: 0.9786, Val: 0.7620, Test: 0.7740\n",
      "Epoch: 016, Loss: 1.6891827583312988, Train: 0.9786, Val: 0.7700, Test: 0.7760\n",
      "Epoch: 017, Loss: 1.6340996026992798, Train: 0.9786, Val: 0.7760, Test: 0.7820\n",
      "Epoch: 018, Loss: 1.659802794456482, Train: 0.9786, Val: 0.7800, Test: 0.7900\n",
      "Epoch: 019, Loss: 1.6149170398712158, Train: 0.9786, Val: 0.7820, Test: 0.7940\n",
      "Epoch: 020, Loss: 1.6935275793075562, Train: 0.9786, Val: 0.7880, Test: 0.8000\n",
      "Epoch: 021, Loss: 1.614951729774475, Train: 0.9857, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 022, Loss: 1.5881741046905518, Train: 0.9857, Val: 0.8000, Test: 0.8000\n",
      "Epoch: 023, Loss: 1.6036455631256104, Train: 0.9857, Val: 0.8020, Test: 0.8030\n",
      "Epoch: 024, Loss: 1.6035844087600708, Train: 0.9857, Val: 0.8040, Test: 0.8010\n",
      "Epoch: 025, Loss: 1.620519995689392, Train: 0.9857, Val: 0.8020, Test: 0.8010\n",
      "Epoch: 026, Loss: 1.618908405303955, Train: 0.9857, Val: 0.8020, Test: 0.8010\n",
      "Epoch: 027, Loss: 1.5921247005462646, Train: 0.9857, Val: 0.8020, Test: 0.8010\n",
      "Epoch: 028, Loss: 1.6149828433990479, Train: 0.9857, Val: 0.8020, Test: 0.8010\n",
      "Epoch: 029, Loss: 1.5651285648345947, Train: 0.9857, Val: 0.7940, Test: 0.8010\n",
      "Epoch: 030, Loss: 1.5855646133422852, Train: 0.9857, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 031, Loss: 1.5285305976867676, Train: 0.9857, Val: 0.7840, Test: 0.8010\n",
      "Epoch: 032, Loss: 1.5286530256271362, Train: 0.9857, Val: 0.7820, Test: 0.8010\n",
      "Epoch: 033, Loss: 1.591142177581787, Train: 0.9857, Val: 0.7800, Test: 0.8010\n",
      "Epoch: 034, Loss: 1.5926600694656372, Train: 0.9857, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 035, Loss: 1.603090763092041, Train: 0.9857, Val: 0.7760, Test: 0.8010\n",
      "Epoch: 036, Loss: 1.569638967514038, Train: 0.9857, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 037, Loss: 1.5366441011428833, Train: 0.9857, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 038, Loss: 1.5385830402374268, Train: 0.9857, Val: 0.7820, Test: 0.8010\n",
      "Epoch: 039, Loss: 1.4337974786758423, Train: 0.9857, Val: 0.7860, Test: 0.8010\n",
      "Epoch: 040, Loss: 1.4931514263153076, Train: 0.9857, Val: 0.7920, Test: 0.8010\n",
      "Epoch: 041, Loss: 1.5121572017669678, Train: 0.9857, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 042, Loss: 1.595883846282959, Train: 0.9929, Val: 0.7900, Test: 0.8010\n",
      "Epoch: 043, Loss: 1.5511687994003296, Train: 0.9929, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 044, Loss: 1.480749487876892, Train: 0.9929, Val: 0.7860, Test: 0.8010\n",
      "Epoch: 045, Loss: 1.4829198122024536, Train: 0.9929, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 046, Loss: 1.4853453636169434, Train: 0.9929, Val: 0.7900, Test: 0.8010\n",
      "Epoch: 047, Loss: 1.560770034790039, Train: 0.9929, Val: 0.7900, Test: 0.8010\n",
      "Epoch: 048, Loss: 1.4793269634246826, Train: 0.9929, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 049, Loss: 1.5178769826889038, Train: 0.9929, Val: 0.7840, Test: 0.8010\n",
      "Epoch: 050, Loss: 1.4984350204467773, Train: 0.9929, Val: 0.7820, Test: 0.8010\n",
      "Epoch: 051, Loss: 1.5697368383407593, Train: 0.9929, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 052, Loss: 1.5170800685882568, Train: 0.9929, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 053, Loss: 1.4710184335708618, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 054, Loss: 1.405608892440796, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 055, Loss: 1.5707080364227295, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 056, Loss: 1.442740559577942, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 057, Loss: 1.5351835489273071, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 058, Loss: 1.5204994678497314, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 059, Loss: 1.5225450992584229, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 060, Loss: 1.4145362377166748, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 061, Loss: 1.5249608755111694, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 062, Loss: 1.527675747871399, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 063, Loss: 1.3896384239196777, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 064, Loss: 1.4881035089492798, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 065, Loss: 1.4723175764083862, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 066, Loss: 1.4581332206726074, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 067, Loss: 1.4733364582061768, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 068, Loss: 1.4237322807312012, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 069, Loss: 1.4889050722122192, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 070, Loss: 1.531812071800232, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 071, Loss: 1.4517850875854492, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 072, Loss: 1.4665546417236328, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 073, Loss: 1.4613962173461914, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 074, Loss: 1.4472689628601074, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 075, Loss: 1.437133550643921, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 076, Loss: 1.470359444618225, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 077, Loss: 1.4708760976791382, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 078, Loss: 1.5308382511138916, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 079, Loss: 1.5209441184997559, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 080, Loss: 1.4543365240097046, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 081, Loss: 1.4069920778274536, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 082, Loss: 1.4160819053649902, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 083, Loss: 1.4051673412322998, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 084, Loss: 1.464881420135498, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 085, Loss: 1.42008638381958, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 086, Loss: 1.4356580972671509, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 087, Loss: 1.4490727186203003, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 088, Loss: 1.4215707778930664, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 089, Loss: 1.4089455604553223, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 090, Loss: 1.4402769804000854, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 091, Loss: 1.4762078523635864, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 092, Loss: 1.4960392713546753, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 093, Loss: 1.455122470855713, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 094, Loss: 1.5282405614852905, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 095, Loss: 1.5654274225234985, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 096, Loss: 1.4308182001113892, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 097, Loss: 1.413316011428833, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 098, Loss: 1.3239637613296509, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 099, Loss: 1.4351235628128052, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 100, Loss: 1.4254941940307617, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 101, Loss: 1.4829801321029663, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 102, Loss: 1.4625451564788818, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 103, Loss: 1.4725419282913208, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 104, Loss: 1.431872010231018, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 105, Loss: 1.3962563276290894, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 106, Loss: 1.459855556488037, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 107, Loss: 1.439823031425476, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 108, Loss: 1.5381646156311035, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 109, Loss: 1.4690911769866943, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 110, Loss: 1.378752589225769, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 111, Loss: 1.4694634675979614, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 112, Loss: 1.4553159475326538, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 113, Loss: 1.4946130514144897, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 114, Loss: 1.4235728979110718, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 115, Loss: 1.4838390350341797, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 116, Loss: 1.4764654636383057, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 117, Loss: 1.4411896467208862, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 118, Loss: 1.38863205909729, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 119, Loss: 1.4266124963760376, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 120, Loss: 1.4519884586334229, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 121, Loss: 1.442466378211975, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 122, Loss: 1.5007516145706177, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 123, Loss: 1.4022985696792603, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 124, Loss: 1.4714092016220093, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 125, Loss: 1.4198626279830933, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 126, Loss: 1.4680368900299072, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 127, Loss: 1.4192969799041748, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 128, Loss: 1.3852537870407104, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 129, Loss: 1.3324257135391235, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 130, Loss: 1.5158109664916992, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 131, Loss: 1.361486554145813, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 132, Loss: 1.4402015209197998, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 133, Loss: 1.5303460359573364, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 134, Loss: 1.4041929244995117, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 135, Loss: 1.4153181314468384, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 136, Loss: 1.4281389713287354, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 137, Loss: 1.4153416156768799, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 138, Loss: 1.3493762016296387, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 139, Loss: 1.4079272747039795, Train: 0.9929, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 140, Loss: 1.3992892503738403, Train: 0.9929, Val: 0.7760, Test: 0.8010\n",
      "Epoch: 141, Loss: 1.3534443378448486, Train: 0.9929, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 142, Loss: 1.2929565906524658, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 143, Loss: 1.479906439781189, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 144, Loss: 1.4444868564605713, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 145, Loss: 1.4436638355255127, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 146, Loss: 1.4973348379135132, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 147, Loss: 1.4966802597045898, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 148, Loss: 1.4407072067260742, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 149, Loss: 1.4094343185424805, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 150, Loss: 1.379122018814087, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 151, Loss: 1.4138487577438354, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 152, Loss: 1.4401700496673584, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 153, Loss: 1.4163355827331543, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 154, Loss: 1.398336410522461, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 155, Loss: 1.3574188947677612, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 156, Loss: 1.4334161281585693, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 157, Loss: 1.4615485668182373, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 158, Loss: 1.3764123916625977, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 159, Loss: 1.3487516641616821, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 160, Loss: 1.386707067489624, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 161, Loss: 1.451829433441162, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 162, Loss: 1.4008883237838745, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 163, Loss: 1.421284556388855, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 164, Loss: 1.4302340745925903, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 165, Loss: 1.3900575637817383, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 166, Loss: 1.390990972518921, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 167, Loss: 1.4410203695297241, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 168, Loss: 1.4293214082717896, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 169, Loss: 1.4536526203155518, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 170, Loss: 1.486297369003296, Train: 1.0000, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 171, Loss: 1.4433611631393433, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 172, Loss: 1.3706458806991577, Train: 1.0000, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 173, Loss: 1.4806214570999146, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 174, Loss: 1.4242655038833618, Train: 1.0000, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 175, Loss: 1.4663556814193726, Train: 1.0000, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 176, Loss: 1.4515024423599243, Train: 1.0000, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 177, Loss: 1.4007971286773682, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 178, Loss: 1.4408587217330933, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 179, Loss: 1.4354455471038818, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 180, Loss: 1.4128000736236572, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 181, Loss: 1.3821015357971191, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 182, Loss: 1.376638412475586, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 183, Loss: 1.4125617742538452, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 184, Loss: 1.3736056089401245, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 185, Loss: 1.408411979675293, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 186, Loss: 1.4473105669021606, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 187, Loss: 1.4054079055786133, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 188, Loss: 1.4138953685760498, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 189, Loss: 1.3226919174194336, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 190, Loss: 1.3807427883148193, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 191, Loss: 1.3994091749191284, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 192, Loss: 1.4119288921356201, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 193, Loss: 1.3489933013916016, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 194, Loss: 1.4117918014526367, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 195, Loss: 1.407097339630127, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 196, Loss: 1.4249342679977417, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 197, Loss: 1.3517628908157349, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 198, Loss: 1.4327445030212402, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 199, Loss: 1.3694179058074951, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 200, Loss: 1.4083017110824585, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 201, Loss: 1.421249270439148, Train: 0.9929, Val: 0.7760, Test: 0.8010\n",
      "Epoch: 202, Loss: 1.4150183200836182, Train: 0.9929, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 203, Loss: 1.4686298370361328, Train: 0.9929, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 204, Loss: 1.3789855241775513, Train: 0.9929, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 205, Loss: 1.4656254053115845, Train: 0.9929, Val: 0.7760, Test: 0.8010\n",
      "Epoch: 206, Loss: 1.4073560237884521, Train: 0.9929, Val: 0.7760, Test: 0.8010\n",
      "Epoch: 207, Loss: 1.3826810121536255, Train: 0.9929, Val: 0.7820, Test: 0.8010\n",
      "Epoch: 208, Loss: 1.409501075744629, Train: 0.9929, Val: 0.7800, Test: 0.8010\n",
      "Epoch: 209, Loss: 1.4303154945373535, Train: 0.9929, Val: 0.7800, Test: 0.8010\n",
      "Epoch: 210, Loss: 1.358738899230957, Train: 0.9929, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 211, Loss: 1.4178625345230103, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 212, Loss: 1.367413878440857, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 213, Loss: 1.325629711151123, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 214, Loss: 1.3608187437057495, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 215, Loss: 1.3509060144424438, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 216, Loss: 1.3543223142623901, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 217, Loss: 1.3817378282546997, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 218, Loss: 1.3619199991226196, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 219, Loss: 1.448764443397522, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 220, Loss: 1.431246042251587, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 221, Loss: 1.4057056903839111, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 222, Loss: 1.414389729499817, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 223, Loss: 1.4778797626495361, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 224, Loss: 1.3907432556152344, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 225, Loss: 1.3415274620056152, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 226, Loss: 1.4634349346160889, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 227, Loss: 1.396596908569336, Train: 1.0000, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 228, Loss: 1.4170764684677124, Train: 1.0000, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 229, Loss: 1.4358543157577515, Train: 1.0000, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 230, Loss: 1.393831491470337, Train: 1.0000, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 231, Loss: 1.4054417610168457, Train: 1.0000, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 232, Loss: 1.4218906164169312, Train: 1.0000, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 233, Loss: 1.4187285900115967, Train: 1.0000, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 234, Loss: 1.4094898700714111, Train: 1.0000, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 235, Loss: 1.4005686044692993, Train: 1.0000, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 236, Loss: 1.3409323692321777, Train: 1.0000, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 237, Loss: 1.455037236213684, Train: 1.0000, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 238, Loss: 1.3607549667358398, Train: 1.0000, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 239, Loss: 1.4549628496170044, Train: 1.0000, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 240, Loss: 1.401459813117981, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 241, Loss: 1.3068135976791382, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 242, Loss: 1.2983982563018799, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 243, Loss: 1.4396907091140747, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 244, Loss: 1.4362051486968994, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 245, Loss: 1.313925862312317, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 246, Loss: 1.406568169593811, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 247, Loss: 1.3759186267852783, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 248, Loss: 1.4178059101104736, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 249, Loss: 1.3567190170288086, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 250, Loss: 1.3620564937591553, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 251, Loss: 1.3918216228485107, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 252, Loss: 1.3686987161636353, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 253, Loss: 1.4188271760940552, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 254, Loss: 1.3508915901184082, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 255, Loss: 1.4076818227767944, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 256, Loss: 1.3689568042755127, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 257, Loss: 1.3529852628707886, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 258, Loss: 1.4659419059753418, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 259, Loss: 1.453415036201477, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 260, Loss: 1.3432646989822388, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 261, Loss: 1.4213809967041016, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 262, Loss: 1.3614976406097412, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 263, Loss: 1.3520355224609375, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 264, Loss: 1.390489935874939, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 265, Loss: 1.32313871383667, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 266, Loss: 1.384117603302002, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 267, Loss: 1.3730556964874268, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 268, Loss: 1.4029955863952637, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 269, Loss: 1.3407511711120605, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 270, Loss: 1.409196138381958, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 271, Loss: 1.3244147300720215, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 272, Loss: 1.3105859756469727, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 273, Loss: 1.3550275564193726, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 274, Loss: 1.417427659034729, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 275, Loss: 1.4057222604751587, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 276, Loss: 1.4182425737380981, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 277, Loss: 1.470541000366211, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 278, Loss: 1.4682681560516357, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 279, Loss: 1.4228944778442383, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 280, Loss: 1.3548088073730469, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 281, Loss: 1.3777737617492676, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 282, Loss: 1.397005558013916, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 283, Loss: 1.4066588878631592, Train: 0.9929, Val: 0.7700, Test: 0.8010\n",
      "Epoch: 284, Loss: 1.346062183380127, Train: 0.9929, Val: 0.7720, Test: 0.8010\n",
      "Epoch: 285, Loss: 1.4493426084518433, Train: 0.9929, Val: 0.7740, Test: 0.8010\n",
      "Epoch: 286, Loss: 1.3667165040969849, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 287, Loss: 1.431553840637207, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 288, Loss: 1.46451735496521, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 289, Loss: 1.3945468664169312, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 290, Loss: 1.438636064529419, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 291, Loss: 1.4285353422164917, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 292, Loss: 1.3960011005401611, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 293, Loss: 1.3944103717803955, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 294, Loss: 1.3910287618637085, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 295, Loss: 1.4693617820739746, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 296, Loss: 1.392285943031311, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 297, Loss: 1.3536649942398071, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 298, Loss: 1.3535557985305786, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 299, Loss: 1.3694316148757935, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 300, Loss: 1.3712098598480225, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 301, Loss: 1.418972134590149, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 302, Loss: 1.4805152416229248, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 303, Loss: 1.3896867036819458, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 304, Loss: 1.3701534271240234, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 305, Loss: 1.3956654071807861, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 306, Loss: 1.3101648092269897, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 307, Loss: 1.4131743907928467, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 308, Loss: 1.4800231456756592, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 309, Loss: 1.442589521408081, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 310, Loss: 1.3943259716033936, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 311, Loss: 1.449447751045227, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 312, Loss: 1.3800493478775024, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 313, Loss: 1.3878185749053955, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 314, Loss: 1.400404691696167, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 315, Loss: 1.4242653846740723, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 316, Loss: 1.4731414318084717, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 317, Loss: 1.426453709602356, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 318, Loss: 1.4302842617034912, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 319, Loss: 1.2991697788238525, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 320, Loss: 1.3994532823562622, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 321, Loss: 1.3836387395858765, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 322, Loss: 1.3958042860031128, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 323, Loss: 1.427416205406189, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 324, Loss: 1.3575880527496338, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 325, Loss: 1.359766960144043, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 326, Loss: 1.3841981887817383, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 327, Loss: 1.356367588043213, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 328, Loss: 1.371546983718872, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 329, Loss: 1.3671890497207642, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 330, Loss: 1.4263883829116821, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 331, Loss: 1.3888182640075684, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 332, Loss: 1.4099688529968262, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 333, Loss: 1.372018575668335, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 334, Loss: 1.395644187927246, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 335, Loss: 1.4409244060516357, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 336, Loss: 1.3812696933746338, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 337, Loss: 1.4052097797393799, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 338, Loss: 1.521517276763916, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 339, Loss: 1.3615338802337646, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 340, Loss: 1.3509182929992676, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 341, Loss: 1.350584626197815, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 342, Loss: 1.4098771810531616, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 343, Loss: 1.4071285724639893, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 344, Loss: 1.364865779876709, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 345, Loss: 1.4368419647216797, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 346, Loss: 1.4672696590423584, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 347, Loss: 1.3244463205337524, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 348, Loss: 1.3773006200790405, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 349, Loss: 1.3912967443466187, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 350, Loss: 1.3525272607803345, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 351, Loss: 1.4437754154205322, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 352, Loss: 1.4264627695083618, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 353, Loss: 1.4512157440185547, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 354, Loss: 1.2989912033081055, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 355, Loss: 1.350356936454773, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 356, Loss: 1.4280740022659302, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 357, Loss: 1.321825385093689, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 358, Loss: 1.3978092670440674, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 359, Loss: 1.390026330947876, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 360, Loss: 1.4396401643753052, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 361, Loss: 1.3951319456100464, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 362, Loss: 1.3487390279769897, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 363, Loss: 1.3841955661773682, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 364, Loss: 1.3726364374160767, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 365, Loss: 1.3397506475448608, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 366, Loss: 1.3663420677185059, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 367, Loss: 1.3511741161346436, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 368, Loss: 1.3717750310897827, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 369, Loss: 1.376420021057129, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 370, Loss: 1.4433621168136597, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 371, Loss: 1.340566873550415, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 372, Loss: 1.4722174406051636, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 373, Loss: 1.3472297191619873, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 374, Loss: 1.3813228607177734, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 375, Loss: 1.392853021621704, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 376, Loss: 1.4555904865264893, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 377, Loss: 1.3858537673950195, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 378, Loss: 1.4309360980987549, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 379, Loss: 1.3578993082046509, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 380, Loss: 1.3551901578903198, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 381, Loss: 1.4379827976226807, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 382, Loss: 1.312457799911499, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 383, Loss: 1.3305268287658691, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 384, Loss: 1.4278819561004639, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 385, Loss: 1.3399901390075684, Train: 0.9929, Val: 0.7680, Test: 0.8010\n",
      "Epoch: 386, Loss: 1.4819716215133667, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 387, Loss: 1.425744891166687, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 388, Loss: 1.3978009223937988, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 389, Loss: 1.3778903484344482, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 390, Loss: 1.4434734582901, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 391, Loss: 1.3775089979171753, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 392, Loss: 1.3857625722885132, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 393, Loss: 1.3983004093170166, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 394, Loss: 1.396946907043457, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 395, Loss: 1.416258454322815, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 396, Loss: 1.3569227457046509, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 397, Loss: 1.3272758722305298, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 398, Loss: 1.3744227886199951, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 399, Loss: 1.4131550788879395, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 400, Loss: 1.4602935314178467, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 401, Loss: 1.3582921028137207, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 402, Loss: 1.3839354515075684, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 403, Loss: 1.3876203298568726, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 404, Loss: 1.4285504817962646, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 405, Loss: 1.40432870388031, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 406, Loss: 1.3886635303497314, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 407, Loss: 1.3586221933364868, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 408, Loss: 1.4589827060699463, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 409, Loss: 1.3548671007156372, Train: 0.9929, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 410, Loss: 1.396483302116394, Train: 1.0000, Val: 0.7660, Test: 0.8010\n",
      "Epoch: 411, Loss: 1.4249027967453003, Train: 1.0000, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 412, Loss: 1.4319157600402832, Train: 1.0000, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 413, Loss: 1.378982424736023, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 414, Loss: 1.4403362274169922, Train: 1.0000, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 415, Loss: 1.3368258476257324, Train: 1.0000, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 416, Loss: 1.4208983182907104, Train: 1.0000, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 417, Loss: 1.3991856575012207, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 418, Loss: 1.3326551914215088, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 419, Loss: 1.4305739402770996, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 420, Loss: 1.4212414026260376, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 421, Loss: 1.3481969833374023, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 422, Loss: 1.4437953233718872, Train: 1.0000, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 423, Loss: 1.4797133207321167, Train: 1.0000, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 424, Loss: 1.383831262588501, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 425, Loss: 1.4890477657318115, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 426, Loss: 1.412609338760376, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 427, Loss: 1.3576260805130005, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 428, Loss: 1.4083420038223267, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 429, Loss: 1.275464415550232, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 430, Loss: 1.3780269622802734, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 431, Loss: 1.3917111158370972, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 432, Loss: 1.432547926902771, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 433, Loss: 1.37018883228302, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 434, Loss: 1.3144440650939941, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 435, Loss: 1.3802434206008911, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 436, Loss: 1.3578938245773315, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 437, Loss: 1.3948874473571777, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 438, Loss: 1.347656488418579, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 439, Loss: 1.4109092950820923, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 440, Loss: 1.418205976486206, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 441, Loss: 1.4288508892059326, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 442, Loss: 1.3973314762115479, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 443, Loss: 1.4070338010787964, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 444, Loss: 1.4118542671203613, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 445, Loss: 1.3558562994003296, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 446, Loss: 1.3732049465179443, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 447, Loss: 1.4365941286087036, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 448, Loss: 1.2996264696121216, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 449, Loss: 1.3637902736663818, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 450, Loss: 1.3875727653503418, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 451, Loss: 1.4137415885925293, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 452, Loss: 1.477347493171692, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 453, Loss: 1.3815480470657349, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 454, Loss: 1.364259123802185, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 455, Loss: 1.457982063293457, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 456, Loss: 1.3326059579849243, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 457, Loss: 1.3570603132247925, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 458, Loss: 1.3109517097473145, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 459, Loss: 1.3618979454040527, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 460, Loss: 1.355377435684204, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 461, Loss: 1.3825430870056152, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 462, Loss: 1.4000074863433838, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 463, Loss: 1.338631272315979, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 464, Loss: 1.4574346542358398, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 465, Loss: 1.3779610395431519, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 466, Loss: 1.4411706924438477, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 467, Loss: 1.4728208780288696, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 468, Loss: 1.4223779439926147, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 469, Loss: 1.3121469020843506, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 470, Loss: 1.3237719535827637, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 471, Loss: 1.3375098705291748, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 472, Loss: 1.4201674461364746, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 473, Loss: 1.3879327774047852, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 474, Loss: 1.3722808361053467, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 475, Loss: 1.3925608396530151, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 476, Loss: 1.374809741973877, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 477, Loss: 1.4063029289245605, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 478, Loss: 1.3762885332107544, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 479, Loss: 1.4212188720703125, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 480, Loss: 1.3943755626678467, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 481, Loss: 1.3641835451126099, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 482, Loss: 1.4162347316741943, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 483, Loss: 1.3977611064910889, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 484, Loss: 1.3598417043685913, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 485, Loss: 1.3372819423675537, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 486, Loss: 1.3449935913085938, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 487, Loss: 1.3239163160324097, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 488, Loss: 1.397336721420288, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 489, Loss: 1.3541085720062256, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 490, Loss: 1.4415817260742188, Train: 0.9929, Val: 0.7640, Test: 0.8010\n",
      "Epoch: 491, Loss: 1.425282597541809, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 492, Loss: 1.4681975841522217, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 493, Loss: 1.4241379499435425, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 494, Loss: 1.4309031963348389, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 495, Loss: 1.3414852619171143, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 496, Loss: 1.3571587800979614, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 497, Loss: 1.407161831855774, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 498, Loss: 1.2719141244888306, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 499, Loss: 1.4613018035888672, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 500, Loss: 1.345072865486145, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 501, Loss: 1.4312033653259277, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 502, Loss: 1.4007686376571655, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 503, Loss: 1.3838629722595215, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 504, Loss: 1.3814820051193237, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 505, Loss: 1.4043856859207153, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 506, Loss: 1.3937084674835205, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 507, Loss: 1.452440857887268, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 508, Loss: 1.4369359016418457, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 509, Loss: 1.3617932796478271, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 510, Loss: 1.3952471017837524, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 511, Loss: 1.3113150596618652, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 512, Loss: 1.3868625164031982, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 513, Loss: 1.375260829925537, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 514, Loss: 1.363661766052246, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 515, Loss: 1.36214280128479, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 516, Loss: 1.3943183422088623, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 517, Loss: 1.3526920080184937, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 518, Loss: 1.3935050964355469, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 519, Loss: 1.422439694404602, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 520, Loss: 1.393730640411377, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 521, Loss: 1.379019021987915, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 522, Loss: 1.3893511295318604, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 523, Loss: 1.406370759010315, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 524, Loss: 1.3846768140792847, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 525, Loss: 1.373396396636963, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 526, Loss: 1.430419683456421, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 527, Loss: 1.4076555967330933, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 528, Loss: 1.4260499477386475, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 529, Loss: 1.4132153987884521, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 530, Loss: 1.3914990425109863, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 531, Loss: 1.4099639654159546, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 532, Loss: 1.3709917068481445, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 533, Loss: 1.3525129556655884, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 534, Loss: 1.3899794816970825, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 535, Loss: 1.3590776920318604, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 536, Loss: 1.5026216506958008, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 537, Loss: 1.3923652172088623, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 538, Loss: 1.304771900177002, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 539, Loss: 1.374925136566162, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 540, Loss: 1.375840663909912, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 541, Loss: 1.3622324466705322, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 542, Loss: 1.358243465423584, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 543, Loss: 1.362708330154419, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 544, Loss: 1.406175136566162, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 545, Loss: 1.4387435913085938, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 546, Loss: 1.3997634649276733, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 547, Loss: 1.4807336330413818, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 548, Loss: 1.3854262828826904, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 549, Loss: 1.3572659492492676, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 550, Loss: 1.4272277355194092, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 551, Loss: 1.4087356328964233, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 552, Loss: 1.3591251373291016, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 553, Loss: 1.4417750835418701, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 554, Loss: 1.4453704357147217, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 555, Loss: 1.4166171550750732, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 556, Loss: 1.444145917892456, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 557, Loss: 1.3867638111114502, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 558, Loss: 1.396409511566162, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 559, Loss: 1.3801051378250122, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 560, Loss: 1.4190829992294312, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 561, Loss: 1.3510478734970093, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 562, Loss: 1.4155791997909546, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 563, Loss: 1.3611584901809692, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 564, Loss: 1.396543025970459, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 565, Loss: 1.4339796304702759, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 566, Loss: 1.3520398139953613, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 567, Loss: 1.3907909393310547, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 568, Loss: 1.3698101043701172, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 569, Loss: 1.407638669013977, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 570, Loss: 1.4075158834457397, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 571, Loss: 1.4369055032730103, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 572, Loss: 1.3866899013519287, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 573, Loss: 1.3882402181625366, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 574, Loss: 1.348798155784607, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 575, Loss: 1.373727798461914, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 576, Loss: 1.4180269241333008, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 577, Loss: 1.4035167694091797, Train: 1.0000, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 578, Loss: 1.3694664239883423, Train: 1.0000, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 579, Loss: 1.372817873954773, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 580, Loss: 1.3500339984893799, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 581, Loss: 1.3175686597824097, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 582, Loss: 1.394748330116272, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 583, Loss: 1.3711025714874268, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 584, Loss: 1.3260482549667358, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 585, Loss: 1.4284430742263794, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 586, Loss: 1.3476660251617432, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 587, Loss: 1.278183102607727, Train: 0.9929, Val: 0.7160, Test: 0.8010\n",
      "Epoch: 588, Loss: 1.3677829504013062, Train: 0.9929, Val: 0.7120, Test: 0.8010\n",
      "Epoch: 589, Loss: 1.40182626247406, Train: 0.9929, Val: 0.7100, Test: 0.8010\n",
      "Epoch: 590, Loss: 1.3118085861206055, Train: 0.9929, Val: 0.7100, Test: 0.8010\n",
      "Epoch: 591, Loss: 1.4172346591949463, Train: 0.9929, Val: 0.7120, Test: 0.8010\n",
      "Epoch: 592, Loss: 1.403288722038269, Train: 0.9929, Val: 0.7140, Test: 0.8010\n",
      "Epoch: 593, Loss: 1.3301539421081543, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 594, Loss: 1.4248350858688354, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 595, Loss: 1.3656350374221802, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 596, Loss: 1.401100516319275, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 597, Loss: 1.4172860383987427, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 598, Loss: 1.353649377822876, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 599, Loss: 1.35374116897583, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 600, Loss: 1.41110098361969, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 601, Loss: 1.3331173658370972, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 602, Loss: 1.3843810558319092, Train: 0.9929, Val: 0.7180, Test: 0.8010\n",
      "Epoch: 603, Loss: 1.4243061542510986, Train: 0.9929, Val: 0.7180, Test: 0.8010\n",
      "Epoch: 604, Loss: 1.4210412502288818, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 605, Loss: 1.4154603481292725, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 606, Loss: 1.381793737411499, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 607, Loss: 1.291784405708313, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 608, Loss: 1.3723469972610474, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 609, Loss: 1.3751771450042725, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 610, Loss: 1.366124153137207, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 611, Loss: 1.3835703134536743, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 612, Loss: 1.3405473232269287, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 613, Loss: 1.4440923929214478, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 614, Loss: 1.3836643695831299, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 615, Loss: 1.3625949621200562, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 616, Loss: 1.405666470527649, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 617, Loss: 1.4215271472930908, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 618, Loss: 1.3730289936065674, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 619, Loss: 1.3862634897232056, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 620, Loss: 1.2775979042053223, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 621, Loss: 1.4075847864151, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 622, Loss: 1.3359178304672241, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 623, Loss: 1.4448152780532837, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 624, Loss: 1.3403548002243042, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 625, Loss: 1.4252498149871826, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 626, Loss: 1.4893864393234253, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 627, Loss: 1.3415687084197998, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 628, Loss: 1.3210593461990356, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 629, Loss: 1.373842716217041, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 630, Loss: 1.3350571393966675, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 631, Loss: 1.371078610420227, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 632, Loss: 1.4185371398925781, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 633, Loss: 1.3439757823944092, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 634, Loss: 1.3210309743881226, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 635, Loss: 1.337988257408142, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 636, Loss: 1.3148411512374878, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 637, Loss: 1.3793867826461792, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 638, Loss: 1.4293822050094604, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 639, Loss: 1.353289246559143, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 640, Loss: 1.3746675252914429, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 641, Loss: 1.3448288440704346, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 642, Loss: 1.349508285522461, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 643, Loss: 1.392902135848999, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 644, Loss: 1.4708549976348877, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 645, Loss: 1.396451473236084, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 646, Loss: 1.382459282875061, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 647, Loss: 1.3672462701797485, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 648, Loss: 1.35722017288208, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 649, Loss: 1.329725980758667, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 650, Loss: 1.4343594312667847, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 651, Loss: 1.3934235572814941, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 652, Loss: 1.3521828651428223, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 653, Loss: 1.3279225826263428, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 654, Loss: 1.3554219007492065, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 655, Loss: 1.37628173828125, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 656, Loss: 1.3082823753356934, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 657, Loss: 1.395957350730896, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 658, Loss: 1.3864240646362305, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 659, Loss: 1.4210110902786255, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 660, Loss: 1.4680052995681763, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 661, Loss: 1.409803032875061, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 662, Loss: 1.3582268953323364, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 663, Loss: 1.4072730541229248, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 664, Loss: 1.3536760807037354, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 665, Loss: 1.413877248764038, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 666, Loss: 1.3933483362197876, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 667, Loss: 1.4024851322174072, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 668, Loss: 1.3402178287506104, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 669, Loss: 1.3690024614334106, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 670, Loss: 1.3473528623580933, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 671, Loss: 1.400820255279541, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 672, Loss: 1.4010721445083618, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 673, Loss: 1.3235176801681519, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 674, Loss: 1.3695802688598633, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 675, Loss: 1.2973031997680664, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 676, Loss: 1.419081211090088, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 677, Loss: 1.3925975561141968, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 678, Loss: 1.394801378250122, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 679, Loss: 1.3634862899780273, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 680, Loss: 1.400390386581421, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 681, Loss: 1.3392536640167236, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 682, Loss: 1.3755528926849365, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 683, Loss: 1.3372546434402466, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 684, Loss: 1.395065426826477, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 685, Loss: 1.36293625831604, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 686, Loss: 1.3136134147644043, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 687, Loss: 1.3339707851409912, Train: 0.9929, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 688, Loss: 1.3832733631134033, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 689, Loss: 1.3885328769683838, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 690, Loss: 1.3245980739593506, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 691, Loss: 1.38870370388031, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 692, Loss: 1.4330813884735107, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 693, Loss: 1.3823977708816528, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 694, Loss: 1.4288649559020996, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 695, Loss: 1.3384997844696045, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 696, Loss: 1.4126406908035278, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 697, Loss: 1.442314624786377, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 698, Loss: 1.3378562927246094, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 699, Loss: 1.3549116849899292, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 700, Loss: 1.4433361291885376, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 701, Loss: 1.4878063201904297, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 702, Loss: 1.3149025440216064, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 703, Loss: 1.3872437477111816, Train: 0.9929, Val: 0.7180, Test: 0.8010\n",
      "Epoch: 704, Loss: 1.3588849306106567, Train: 0.9929, Val: 0.7100, Test: 0.8010\n",
      "Epoch: 705, Loss: 1.358626127243042, Train: 0.9929, Val: 0.7100, Test: 0.8010\n",
      "Epoch: 706, Loss: 1.3235654830932617, Train: 0.9929, Val: 0.7140, Test: 0.8010\n",
      "Epoch: 707, Loss: 1.3439851999282837, Train: 0.9929, Val: 0.7100, Test: 0.8010\n",
      "Epoch: 708, Loss: 1.3804799318313599, Train: 0.9929, Val: 0.7160, Test: 0.8010\n",
      "Epoch: 709, Loss: 1.416060447692871, Train: 0.9929, Val: 0.7160, Test: 0.8010\n",
      "Epoch: 710, Loss: 1.4168500900268555, Train: 0.9929, Val: 0.7140, Test: 0.8010\n",
      "Epoch: 711, Loss: 1.3213086128234863, Train: 0.9929, Val: 0.7120, Test: 0.8010\n",
      "Epoch: 712, Loss: 1.3567767143249512, Train: 0.9929, Val: 0.7120, Test: 0.8010\n",
      "Epoch: 713, Loss: 1.4239156246185303, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 714, Loss: 1.3861716985702515, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 715, Loss: 1.4576013088226318, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 716, Loss: 1.4087648391723633, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 717, Loss: 1.3360323905944824, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 718, Loss: 1.3549182415008545, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 719, Loss: 1.415325403213501, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 720, Loss: 1.3558461666107178, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 721, Loss: 1.3649868965148926, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 722, Loss: 1.4389020204544067, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 723, Loss: 1.4102396965026855, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 724, Loss: 1.3700554370880127, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 725, Loss: 1.4341615438461304, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 726, Loss: 1.3067678213119507, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 727, Loss: 1.4194895029067993, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 728, Loss: 1.3370610475540161, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 729, Loss: 1.410847783088684, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 730, Loss: 1.3860455751419067, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 731, Loss: 1.3865150213241577, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 732, Loss: 1.3579133749008179, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 733, Loss: 1.3892664909362793, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 734, Loss: 1.3656156063079834, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 735, Loss: 1.3631834983825684, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 736, Loss: 1.4027585983276367, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 737, Loss: 1.3948123455047607, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 738, Loss: 1.3523014783859253, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 739, Loss: 1.3995611667633057, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 740, Loss: 1.4614806175231934, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 741, Loss: 1.3717944622039795, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 742, Loss: 1.4856624603271484, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 743, Loss: 1.4481337070465088, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 744, Loss: 1.354924201965332, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 745, Loss: 1.3028861284255981, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 746, Loss: 1.3826689720153809, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 747, Loss: 1.376611590385437, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 748, Loss: 1.4405099153518677, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 749, Loss: 1.3936805725097656, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 750, Loss: 1.4248406887054443, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 751, Loss: 1.4259700775146484, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 752, Loss: 1.3576223850250244, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 753, Loss: 1.2479023933410645, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 754, Loss: 1.356834888458252, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 755, Loss: 1.4301395416259766, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 756, Loss: 1.489563226699829, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 757, Loss: 1.3917746543884277, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 758, Loss: 1.3118207454681396, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 759, Loss: 1.4110400676727295, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 760, Loss: 1.44271981716156, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 761, Loss: 1.3545982837677002, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 762, Loss: 1.4262481927871704, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 763, Loss: 1.3718230724334717, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 764, Loss: 1.3743950128555298, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 765, Loss: 1.3849021196365356, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 766, Loss: 1.505754828453064, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 767, Loss: 1.3584762811660767, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 768, Loss: 1.353670597076416, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 769, Loss: 1.4500954151153564, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 770, Loss: 1.3723690509796143, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 771, Loss: 1.4022340774536133, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 772, Loss: 1.4054850339889526, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 773, Loss: 1.304462194442749, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 774, Loss: 1.4097239971160889, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 775, Loss: 1.3749622106552124, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 776, Loss: 1.462257981300354, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 777, Loss: 1.364013910293579, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 778, Loss: 1.3529937267303467, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 779, Loss: 1.3489867448806763, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 780, Loss: 1.4037672281265259, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 781, Loss: 1.361487627029419, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 782, Loss: 1.3531806468963623, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 783, Loss: 1.4141108989715576, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 784, Loss: 1.349452257156372, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 785, Loss: 1.4066674709320068, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 786, Loss: 1.357741355895996, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 787, Loss: 1.432366132736206, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 788, Loss: 1.4180394411087036, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 789, Loss: 1.3575727939605713, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 790, Loss: 1.3422166109085083, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 791, Loss: 1.3503726720809937, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 792, Loss: 1.387573003768921, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 793, Loss: 1.407470464706421, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 794, Loss: 1.396958351135254, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 795, Loss: 1.3637974262237549, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 796, Loss: 1.411550760269165, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 797, Loss: 1.310286283493042, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 798, Loss: 1.3855419158935547, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 799, Loss: 1.3523471355438232, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 800, Loss: 1.311894416809082, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 801, Loss: 1.343298077583313, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 802, Loss: 1.3945032358169556, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 803, Loss: 1.3794382810592651, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 804, Loss: 1.4212037324905396, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 805, Loss: 1.4707694053649902, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 806, Loss: 1.4109135866165161, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 807, Loss: 1.324356198310852, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 808, Loss: 1.3669713735580444, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 809, Loss: 1.3065533638000488, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 810, Loss: 1.379619836807251, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 811, Loss: 1.32855224609375, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 812, Loss: 1.3135161399841309, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 813, Loss: 1.438631534576416, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 814, Loss: 1.4698293209075928, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 815, Loss: 1.4783509969711304, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 816, Loss: 1.3215999603271484, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 817, Loss: 1.3721716403961182, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 818, Loss: 1.3707258701324463, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 819, Loss: 1.4071121215820312, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 820, Loss: 1.3680667877197266, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 821, Loss: 1.3905625343322754, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 822, Loss: 1.4252816438674927, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 823, Loss: 1.442452311515808, Train: 1.0000, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 824, Loss: 1.452901005744934, Train: 1.0000, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 825, Loss: 1.3129973411560059, Train: 1.0000, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 826, Loss: 1.3956021070480347, Train: 1.0000, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 827, Loss: 1.3301395177841187, Train: 1.0000, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 828, Loss: 1.3279120922088623, Train: 1.0000, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 829, Loss: 1.4350829124450684, Train: 1.0000, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 830, Loss: 1.441460132598877, Train: 1.0000, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 831, Loss: 1.3990848064422607, Train: 1.0000, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 832, Loss: 1.3838852643966675, Train: 1.0000, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 833, Loss: 1.3402875661849976, Train: 1.0000, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 834, Loss: 1.462071180343628, Train: 1.0000, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 835, Loss: 1.294064998626709, Train: 1.0000, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 836, Loss: 1.3432475328445435, Train: 1.0000, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 837, Loss: 1.4159575700759888, Train: 1.0000, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 838, Loss: 1.4545037746429443, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 839, Loss: 1.3249847888946533, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 840, Loss: 1.373874306678772, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 841, Loss: 1.4382503032684326, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 842, Loss: 1.4242843389511108, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 843, Loss: 1.2918131351470947, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 844, Loss: 1.4025108814239502, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 845, Loss: 1.3767532110214233, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 846, Loss: 1.3506016731262207, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 847, Loss: 1.3884117603302002, Train: 1.0000, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 848, Loss: 1.4116932153701782, Train: 1.0000, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 849, Loss: 1.3672211170196533, Train: 1.0000, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 850, Loss: 1.4280897378921509, Train: 1.0000, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 851, Loss: 1.4766168594360352, Train: 1.0000, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 852, Loss: 1.2776479721069336, Train: 1.0000, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 853, Loss: 1.3558919429779053, Train: 1.0000, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 854, Loss: 1.4021689891815186, Train: 1.0000, Val: 0.7620, Test: 0.8010\n",
      "Epoch: 855, Loss: 1.4004539251327515, Train: 1.0000, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 856, Loss: 1.3444149494171143, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 857, Loss: 1.3162903785705566, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 858, Loss: 1.4166829586029053, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 859, Loss: 1.3123807907104492, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 860, Loss: 1.4482980966567993, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 861, Loss: 1.398391842842102, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 862, Loss: 1.4599764347076416, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 863, Loss: 1.3977667093276978, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 864, Loss: 1.421718955039978, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 865, Loss: 1.4104362726211548, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 866, Loss: 1.3645970821380615, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 867, Loss: 1.2867014408111572, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 868, Loss: 1.351344108581543, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 869, Loss: 1.4466922283172607, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 870, Loss: 1.3319488763809204, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 871, Loss: 1.3631114959716797, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 872, Loss: 1.4043564796447754, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 873, Loss: 1.3540745973587036, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 874, Loss: 1.3169419765472412, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 875, Loss: 1.339181661605835, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 876, Loss: 1.350036382675171, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 877, Loss: 1.325483798980713, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 878, Loss: 1.3944255113601685, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 879, Loss: 1.403283715248108, Train: 0.9929, Val: 0.7160, Test: 0.8010\n",
      "Epoch: 880, Loss: 1.4043233394622803, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 881, Loss: 1.399188756942749, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 882, Loss: 1.4095914363861084, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 883, Loss: 1.3902873992919922, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 884, Loss: 1.3313602209091187, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 885, Loss: 1.3901050090789795, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 886, Loss: 1.426639199256897, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 887, Loss: 1.4004864692687988, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 888, Loss: 1.3965582847595215, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 889, Loss: 1.3388434648513794, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 890, Loss: 1.3753740787506104, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 891, Loss: 1.3177887201309204, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 892, Loss: 1.4709734916687012, Train: 0.9929, Val: 0.7140, Test: 0.8010\n",
      "Epoch: 893, Loss: 1.376230001449585, Train: 0.9929, Val: 0.7120, Test: 0.8010\n",
      "Epoch: 894, Loss: 1.3981822729110718, Train: 0.9929, Val: 0.7120, Test: 0.8010\n",
      "Epoch: 895, Loss: 1.2640122175216675, Train: 0.9929, Val: 0.7140, Test: 0.8010\n",
      "Epoch: 896, Loss: 1.3705430030822754, Train: 0.9929, Val: 0.7160, Test: 0.8010\n",
      "Epoch: 897, Loss: 1.4217555522918701, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 898, Loss: 1.3544278144836426, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 899, Loss: 1.3813917636871338, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 900, Loss: 1.3353310823440552, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 901, Loss: 1.4196304082870483, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 902, Loss: 1.3538057804107666, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 903, Loss: 1.369401216506958, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 904, Loss: 1.3740124702453613, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 905, Loss: 1.3988738059997559, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 906, Loss: 1.3980239629745483, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 907, Loss: 1.3960590362548828, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 908, Loss: 1.3748552799224854, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 909, Loss: 1.370647668838501, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 910, Loss: 1.4126710891723633, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 911, Loss: 1.379990816116333, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 912, Loss: 1.4493833780288696, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 913, Loss: 1.4435898065567017, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 914, Loss: 1.3973479270935059, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 915, Loss: 1.4162092208862305, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 916, Loss: 1.4043101072311401, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 917, Loss: 1.3933346271514893, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 918, Loss: 1.4090111255645752, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 919, Loss: 1.4019235372543335, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 920, Loss: 1.3883280754089355, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 921, Loss: 1.4144566059112549, Train: 0.9929, Val: 0.7200, Test: 0.8010\n",
      "Epoch: 922, Loss: 1.3430713415145874, Train: 0.9929, Val: 0.7220, Test: 0.8010\n",
      "Epoch: 923, Loss: 1.34549880027771, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 924, Loss: 1.409637451171875, Train: 0.9929, Val: 0.7260, Test: 0.8010\n",
      "Epoch: 925, Loss: 1.3635709285736084, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 926, Loss: 1.4656985998153687, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 927, Loss: 1.3581860065460205, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 928, Loss: 1.4175091981887817, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 929, Loss: 1.319381833076477, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 930, Loss: 1.4411232471466064, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 931, Loss: 1.4289186000823975, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 932, Loss: 1.3318047523498535, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 933, Loss: 1.3534599542617798, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 934, Loss: 1.326106309890747, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 935, Loss: 1.4200242757797241, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 936, Loss: 1.378099799156189, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 937, Loss: 1.3387904167175293, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 938, Loss: 1.4052976369857788, Train: 0.9929, Val: 0.7300, Test: 0.8010\n",
      "Epoch: 939, Loss: 1.3612221479415894, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 940, Loss: 1.3892736434936523, Train: 0.9929, Val: 0.7240, Test: 0.8010\n",
      "Epoch: 941, Loss: 1.3844184875488281, Train: 0.9929, Val: 0.7280, Test: 0.8010\n",
      "Epoch: 942, Loss: 1.3173601627349854, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 943, Loss: 1.3492143154144287, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 944, Loss: 1.3162484169006348, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 945, Loss: 1.3884375095367432, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 946, Loss: 1.4387041330337524, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 947, Loss: 1.4134262800216675, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 948, Loss: 1.456983208656311, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 949, Loss: 1.3292820453643799, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 950, Loss: 1.3603825569152832, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 951, Loss: 1.3888425827026367, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 952, Loss: 1.3562135696411133, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 953, Loss: 1.3028109073638916, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 954, Loss: 1.3957788944244385, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 955, Loss: 1.4019042253494263, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 956, Loss: 1.3804843425750732, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 957, Loss: 1.3352508544921875, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 958, Loss: 1.3910160064697266, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 959, Loss: 1.4244121313095093, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 960, Loss: 1.383090615272522, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 961, Loss: 1.4649980068206787, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 962, Loss: 1.2595086097717285, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 963, Loss: 1.3635060787200928, Train: 0.9929, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 964, Loss: 1.3553199768066406, Train: 1.0000, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 965, Loss: 1.2717746496200562, Train: 1.0000, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 966, Loss: 1.5082753896713257, Train: 1.0000, Val: 0.7500, Test: 0.8010\n",
      "Epoch: 967, Loss: 1.3546491861343384, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 968, Loss: 1.3218590021133423, Train: 1.0000, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 969, Loss: 1.4256831407546997, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 970, Loss: 1.3206264972686768, Train: 0.9929, Val: 0.7560, Test: 0.8010\n",
      "Epoch: 971, Loss: 1.3561153411865234, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 972, Loss: 1.3498094081878662, Train: 0.9929, Val: 0.7600, Test: 0.8010\n",
      "Epoch: 973, Loss: 1.3727951049804688, Train: 0.9929, Val: 0.7580, Test: 0.8010\n",
      "Epoch: 974, Loss: 1.346222996711731, Train: 0.9929, Val: 0.7540, Test: 0.8010\n",
      "Epoch: 975, Loss: 1.44739830493927, Train: 0.9929, Val: 0.7520, Test: 0.8010\n",
      "Epoch: 976, Loss: 1.3449370861053467, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 977, Loss: 1.3634603023529053, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 978, Loss: 1.423964023590088, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 979, Loss: 1.3545475006103516, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 980, Loss: 1.3618168830871582, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 981, Loss: 1.342571496963501, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 982, Loss: 1.3403568267822266, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 983, Loss: 1.2969404458999634, Train: 0.9929, Val: 0.7480, Test: 0.8010\n",
      "Epoch: 984, Loss: 1.3839813470840454, Train: 0.9929, Val: 0.7440, Test: 0.8010\n",
      "Epoch: 985, Loss: 1.3913606405258179, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 986, Loss: 1.3404812812805176, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 987, Loss: 1.3589465618133545, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 988, Loss: 1.2963855266571045, Train: 0.9929, Val: 0.7320, Test: 0.8010\n",
      "Epoch: 989, Loss: 1.3281735181808472, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 990, Loss: 1.3511974811553955, Train: 0.9929, Val: 0.7340, Test: 0.8010\n",
      "Epoch: 991, Loss: 1.3121408224105835, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 992, Loss: 1.3733240365982056, Train: 0.9929, Val: 0.7400, Test: 0.8010\n",
      "Epoch: 993, Loss: 1.3920183181762695, Train: 0.9929, Val: 0.7460, Test: 0.8010\n",
      "Epoch: 994, Loss: 1.329548716545105, Train: 0.9929, Val: 0.7420, Test: 0.8010\n",
      "Epoch: 995, Loss: 1.3788261413574219, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 996, Loss: 1.3796207904815674, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 997, Loss: 1.3718841075897217, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Epoch: 998, Loss: 1.3734121322631836, Train: 0.9929, Val: 0.7360, Test: 0.8010\n",
      "Epoch: 999, Loss: 1.394458532333374, Train: 0.9929, Val: 0.7380, Test: 0.8010\n",
      "Best Val Acc: 0.8040 Test Acc: 0.8010\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 1000):\n",
    "    loss = train(model, data, optimizer, loss='smooth_label')\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe8eaf950b0cf64c1b70de22759d9a144a2595c541b4003711edd1f96d908e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

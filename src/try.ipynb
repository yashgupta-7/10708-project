{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, edgeindex2adj\n",
    "from models import GCN, GAT, LP\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "pubmed = Planetoid(root='.', name='Pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 24, dataset.num_classes)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21)\n"
     ]
    }
   ],
   "source": [
    "from models import ALP, GCN\n",
    "dataset = cora\n",
    "data = dataset[0]\n",
    "\n",
    "# take k random training nodes for each class\n",
    "k = 3\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "print(data.train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 1.8628408908843994, Train: 0.3810, Val: 0.1580, Test: 0.2980\n",
      "Epoch: 050, Loss: 1.802296757698059, Train: 0.3810, Val: 0.1160, Test: 0.2980\n",
      "Epoch: 075, Loss: 1.7939027547836304, Train: 0.3810, Val: 0.0740, Test: 0.2980\n",
      "Epoch: 100, Loss: 1.793192744255066, Train: 0.3810, Val: 0.1580, Test: 0.2980\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yashgupta/Desktop/10708-project/src/try.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m250\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, data, optimizer, scheduler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msmooth_label\u001b[39m\u001b[39m'\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     train_acc, val_acc, tmp_test_acc \u001b[39m=\u001b[39m test(model, data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m val_acc \u001b[39m>\u001b[39m best_val_acc:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yashgupta/Desktop/10708-project/src/try.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         best_val_acc \u001b[39m=\u001b[39m val_acc\n",
      "File \u001b[0;32m~/Desktop/10708-project/src/utils.py:58\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(model, data): \u001b[39m# get accuracy on train, val, and test sets\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 58\u001b[0m     pred \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m     accs \u001b[39m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m [data\u001b[39m.\u001b[39mtrain_mask, data\u001b[39m.\u001b[39mval_mask, data\u001b[39m.\u001b[39mtest_mask]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/10708-project/src/models.py:16\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, edge_index: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     14\u001b[0m     \u001b[39m# x: Node feature matrix of shape [num_nodes, in_channels]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# edge_index: Graph connectivity matrix of shape [2, num_edges]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index)\u001b[39m.\u001b[39mrelu()\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index)\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:229\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m             edge_index \u001b[39m=\u001b[39m cache\n\u001b[0;32m--> 229\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin(x)\n\u001b[1;32m    231\u001b[0m \u001b[39m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m    232\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mx, edge_weight\u001b[39m=\u001b[39medge_weight,\n\u001b[1;32m    233\u001b[0m                      size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:132\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    128\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run for 5 seeds\n",
    "av_acc = 0\n",
    "\n",
    "for seed in range(5):\n",
    "    torch.manual_seed(seed)\n",
    "    model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=50, min_lr=0.0001)\n",
    "\n",
    "    best_val_acc = final_test_acc = 0\n",
    "    for epoch in range(1, 250):\n",
    "        loss = train(model, data, optimizer, scheduler=None, loss='smooth_label', alpha=20)\n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n",
    "    av_acc += test_acc\n",
    "print(f'Average Test Acc: {av_acc/5:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 0.1498504877090454, Train: 1.0000, Val: 0.4900, Test: 0.5050\n",
      "Epoch: 050, Loss: 0.00931228045374155, Train: 1.0000, Val: 0.5380, Test: 0.5520\n",
      "Epoch: 075, Loss: 0.007171915378421545, Train: 1.0000, Val: 0.5620, Test: 0.5710\n",
      "Epoch: 100, Loss: 0.007477668579667807, Train: 1.0000, Val: 0.5600, Test: 0.5710\n",
      "Epoch: 125, Loss: 0.007068636827170849, Train: 1.0000, Val: 0.5620, Test: 0.5710\n",
      "Best Val Acc: 0.5740 Test Acc: 0.5910\n",
      "Epoch: 025, Loss: 0.0841929242014885, Train: 1.0000, Val: 0.4880, Test: 0.5240\n",
      "Epoch: 050, Loss: 0.009557830169796944, Train: 1.0000, Val: 0.5300, Test: 0.5490\n",
      "Epoch: 075, Loss: 0.007854974828660488, Train: 1.0000, Val: 0.5440, Test: 0.5590\n",
      "Epoch: 100, Loss: 0.007982530631124973, Train: 1.0000, Val: 0.5500, Test: 0.5620\n",
      "Epoch: 125, Loss: 0.007707900833338499, Train: 1.0000, Val: 0.5620, Test: 0.5790\n",
      "Best Val Acc: 0.5820 Test Acc: 0.5890\n",
      "Epoch: 025, Loss: 0.1472441405057907, Train: 1.0000, Val: 0.4940, Test: 0.4930\n",
      "Epoch: 050, Loss: 0.009941385127604008, Train: 1.0000, Val: 0.5720, Test: 0.5810\n",
      "Epoch: 075, Loss: 0.007330987602472305, Train: 1.0000, Val: 0.5660, Test: 0.5810\n",
      "Epoch: 100, Loss: 0.0077163539826869965, Train: 1.0000, Val: 0.5760, Test: 0.5980\n",
      "Epoch: 125, Loss: 0.007385372184216976, Train: 1.0000, Val: 0.5900, Test: 0.6010\n",
      "Best Val Acc: 0.5960 Test Acc: 0.6070\n",
      "Epoch: 025, Loss: 0.41486209630966187, Train: 0.9048, Val: 0.3260, Test: 0.3410\n",
      "Epoch: 050, Loss: 0.020769458264112473, Train: 1.0000, Val: 0.5440, Test: 0.5430\n",
      "Epoch: 075, Loss: 0.008341072127223015, Train: 1.0000, Val: 0.5860, Test: 0.5910\n",
      "Epoch: 100, Loss: 0.007824216969311237, Train: 1.0000, Val: 0.5820, Test: 0.5910\n",
      "Epoch: 125, Loss: 0.007432083133608103, Train: 1.0000, Val: 0.5880, Test: 0.5910\n",
      "Best Val Acc: 0.5920 Test Acc: 0.6210\n",
      "Epoch: 025, Loss: 0.36480623483657837, Train: 1.0000, Val: 0.4420, Test: 0.4220\n",
      "Epoch: 050, Loss: 0.027651485055685043, Train: 1.0000, Val: 0.5380, Test: 0.5240\n",
      "Epoch: 075, Loss: 0.011899203062057495, Train: 1.0000, Val: 0.5460, Test: 0.5360\n",
      "Epoch: 100, Loss: 0.010590321384370327, Train: 1.0000, Val: 0.5640, Test: 0.5470\n",
      "Epoch: 125, Loss: 0.009806178510189056, Train: 1.0000, Val: 0.5760, Test: 0.5530\n",
      "Best Val Acc: 0.5820 Test Acc: 0.5830\n",
      "Average Test Acc: 0.5982\n"
     ]
    }
   ],
   "source": [
    "# run for 5 seeds\n",
    "av_acc = 0\n",
    "\n",
    "for seed in range(5):\n",
    "    torch.manual_seed(seed)\n",
    "    model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=50, min_lr=0.0001)\n",
    "\n",
    "    best_val_acc = final_test_acc = 0\n",
    "    for epoch in range(1, 150):\n",
    "        loss = train(model, data, optimizer, scheduler=scheduler, loss='cross_entropy')\n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n",
    "    av_acc += test_acc\n",
    "print(f'Average Test Acc: {av_acc/5:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Val Acc: 0.7260 Test Acc: 0.7140 Best l: 16 Best a: 0.99\n"
     ]
    }
   ],
   "source": [
    "# dataset = citeseer\n",
    "# dataset = cora\n",
    "dataset = pubmed\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "best_l = best_a = 0\n",
    "\n",
    "for l in [1, 2, 4, 8, 16, 32]:\n",
    "    for a in [0.05, 0.1, 0.3, 0.6, 0.8, 0.9, 0.95, 0.99, 1]:\n",
    "        model = LP(num_layers=l, alpha=a)\n",
    "        outs = model.train(dataset)\n",
    "        train_acc, val_acc, tmp_test_acc = model.test()\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "            best_l = l\n",
    "            best_a = a\n",
    "            \n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}', f'Best l: {best_l}', f'Best a: {best_a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8833333333333333, 0.712, 0.707]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = model.test()\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19717, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 2, 0, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 2, 0, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe8eaf950b0cf64c1b70de22759d9a144a2595c541b4003711edd1f96d908e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, edgeindex2adj\n",
    "from models import GCN, GAT\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 24, dataset.num_classes)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "dataset = cora\n",
    "model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei = dataset[0].edge_index\n",
    "adj = edgeindex2adj(ei, dataset.x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.115322232246399, Train: 0.9929, Val: 0.7360, Test: 0.7430\n",
      "Epoch: 002, Loss: 1.0996752977371216, Train: 0.9929, Val: 0.7360, Test: 0.7430\n",
      "Epoch: 003, Loss: 1.1474813222885132, Train: 0.9929, Val: 0.7340, Test: 0.7430\n",
      "Epoch: 004, Loss: 1.1475460529327393, Train: 0.9929, Val: 0.7360, Test: 0.7430\n",
      "Epoch: 005, Loss: 0.9929359555244446, Train: 0.9929, Val: 0.7400, Test: 0.7540\n",
      "Epoch: 006, Loss: 1.0271676778793335, Train: 0.9929, Val: 0.7480, Test: 0.7550\n",
      "Epoch: 007, Loss: 1.0305854082107544, Train: 0.9929, Val: 0.7500, Test: 0.7630\n",
      "Epoch: 008, Loss: 0.9526212215423584, Train: 0.9929, Val: 0.7540, Test: 0.7610\n",
      "Epoch: 009, Loss: 0.9710186719894409, Train: 0.9929, Val: 0.7540, Test: 0.7610\n",
      "Epoch: 010, Loss: 0.941103458404541, Train: 0.9929, Val: 0.7560, Test: 0.7670\n",
      "Epoch: 011, Loss: 0.9073953628540039, Train: 0.9929, Val: 0.7600, Test: 0.7710\n",
      "Epoch: 012, Loss: 0.8683645129203796, Train: 0.9929, Val: 0.7600, Test: 0.7710\n",
      "Epoch: 013, Loss: 0.9524100422859192, Train: 0.9929, Val: 0.7600, Test: 0.7710\n",
      "Epoch: 014, Loss: 0.8099448084831238, Train: 0.9929, Val: 0.7660, Test: 0.7810\n",
      "Epoch: 015, Loss: 0.763312816619873, Train: 0.9929, Val: 0.7700, Test: 0.7870\n",
      "Epoch: 016, Loss: 0.7842506766319275, Train: 0.9929, Val: 0.7760, Test: 0.7910\n",
      "Epoch: 017, Loss: 0.7461850047111511, Train: 0.9929, Val: 0.7780, Test: 0.7930\n",
      "Epoch: 018, Loss: 0.7679919004440308, Train: 0.9929, Val: 0.7760, Test: 0.7930\n",
      "Epoch: 019, Loss: 0.68526691198349, Train: 0.9929, Val: 0.7760, Test: 0.7930\n",
      "Epoch: 020, Loss: 0.7275732159614563, Train: 0.9929, Val: 0.7760, Test: 0.7930\n",
      "Epoch: 021, Loss: 0.6718446016311646, Train: 0.9929, Val: 0.7720, Test: 0.7930\n",
      "Epoch: 022, Loss: 0.5862208604812622, Train: 0.9929, Val: 0.7740, Test: 0.7930\n",
      "Epoch: 023, Loss: 0.6608850359916687, Train: 0.9929, Val: 0.7740, Test: 0.7930\n",
      "Epoch: 024, Loss: 0.6391991972923279, Train: 0.9929, Val: 0.7760, Test: 0.7930\n",
      "Epoch: 025, Loss: 0.7661548852920532, Train: 0.9929, Val: 0.7740, Test: 0.7930\n",
      "Epoch: 026, Loss: 0.6345316767692566, Train: 0.9929, Val: 0.7740, Test: 0.7930\n",
      "Epoch: 027, Loss: 0.5753963589668274, Train: 0.9929, Val: 0.7780, Test: 0.7930\n",
      "Epoch: 028, Loss: 0.6685505509376526, Train: 0.9929, Val: 0.7780, Test: 0.7930\n",
      "Epoch: 029, Loss: 0.575133204460144, Train: 0.9929, Val: 0.7780, Test: 0.7930\n",
      "Epoch: 030, Loss: 0.6035388112068176, Train: 0.9929, Val: 0.7780, Test: 0.7930\n",
      "Epoch: 031, Loss: 0.49036696553230286, Train: 0.9929, Val: 0.7760, Test: 0.7930\n",
      "Epoch: 032, Loss: 0.5848809480667114, Train: 0.9929, Val: 0.7760, Test: 0.7930\n",
      "Epoch: 033, Loss: 0.6071422100067139, Train: 0.9929, Val: 0.7780, Test: 0.7930\n",
      "Epoch: 034, Loss: 0.6075214743614197, Train: 0.9929, Val: 0.7780, Test: 0.7930\n",
      "Epoch: 035, Loss: 0.47996947169303894, Train: 0.9929, Val: 0.7800, Test: 0.8050\n",
      "Epoch: 036, Loss: 0.6118153929710388, Train: 0.9929, Val: 0.7800, Test: 0.8050\n",
      "Epoch: 037, Loss: 0.5029280185699463, Train: 0.9929, Val: 0.7780, Test: 0.8050\n",
      "Epoch: 038, Loss: 0.5794435143470764, Train: 0.9929, Val: 0.7780, Test: 0.8050\n",
      "Epoch: 039, Loss: 0.6303645968437195, Train: 0.9929, Val: 0.7780, Test: 0.8050\n",
      "Epoch: 040, Loss: 0.5650227069854736, Train: 0.9929, Val: 0.7740, Test: 0.8050\n",
      "Epoch: 041, Loss: 0.5183127522468567, Train: 0.9929, Val: 0.7740, Test: 0.8050\n",
      "Epoch: 042, Loss: 0.563232958316803, Train: 1.0000, Val: 0.7740, Test: 0.8050\n",
      "Epoch: 043, Loss: 0.5177139043807983, Train: 1.0000, Val: 0.7740, Test: 0.8050\n",
      "Epoch: 044, Loss: 0.5758517384529114, Train: 1.0000, Val: 0.7780, Test: 0.8050\n",
      "Epoch: 045, Loss: 0.5026523470878601, Train: 1.0000, Val: 0.7820, Test: 0.8040\n",
      "Epoch: 046, Loss: 0.6208954453468323, Train: 1.0000, Val: 0.7860, Test: 0.8040\n",
      "Epoch: 047, Loss: 0.536065936088562, Train: 1.0000, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 048, Loss: 0.7003918886184692, Train: 1.0000, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 049, Loss: 0.5176642537117004, Train: 1.0000, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 050, Loss: 0.5278012752532959, Train: 0.9929, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 051, Loss: 0.49510133266448975, Train: 0.9929, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 052, Loss: 0.5024110674858093, Train: 0.9929, Val: 0.7940, Test: 0.8070\n",
      "Epoch: 053, Loss: 0.49350404739379883, Train: 1.0000, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 054, Loss: 0.4776066243648529, Train: 1.0000, Val: 0.8000, Test: 0.8060\n",
      "Epoch: 055, Loss: 0.545518696308136, Train: 1.0000, Val: 0.8040, Test: 0.8120\n",
      "Epoch: 056, Loss: 0.5752191543579102, Train: 1.0000, Val: 0.8000, Test: 0.8120\n",
      "Epoch: 057, Loss: 0.5248309969902039, Train: 1.0000, Val: 0.8020, Test: 0.8120\n",
      "Epoch: 058, Loss: 0.4247893989086151, Train: 1.0000, Val: 0.8000, Test: 0.8120\n",
      "Epoch: 059, Loss: 0.44903379678726196, Train: 1.0000, Val: 0.7980, Test: 0.8120\n",
      "Epoch: 060, Loss: 0.5441162586212158, Train: 1.0000, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 061, Loss: 0.5258384346961975, Train: 1.0000, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 062, Loss: 0.4562816023826599, Train: 1.0000, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 063, Loss: 0.4808877110481262, Train: 1.0000, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 064, Loss: 0.3721727430820465, Train: 1.0000, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 065, Loss: 0.37740665674209595, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 066, Loss: 0.4161434769630432, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 067, Loss: 0.5275201797485352, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 068, Loss: 0.4683562219142914, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 069, Loss: 0.5406513810157776, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 070, Loss: 0.480993390083313, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 071, Loss: 0.47941452264785767, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 072, Loss: 0.4742443561553955, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 073, Loss: 0.45229002833366394, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 074, Loss: 0.5590383410453796, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 075, Loss: 0.49588629603385925, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 076, Loss: 0.44485220313072205, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 077, Loss: 0.47132471203804016, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 078, Loss: 0.47158193588256836, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 079, Loss: 0.4311753213405609, Train: 0.9929, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 080, Loss: 0.5473914742469788, Train: 0.9929, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 081, Loss: 0.46598440408706665, Train: 0.9929, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 082, Loss: 0.44005241990089417, Train: 0.9929, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 083, Loss: 0.5421097874641418, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 084, Loss: 0.48302820324897766, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 085, Loss: 0.47779834270477295, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 086, Loss: 0.4558851420879364, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 087, Loss: 0.4758102297782898, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 088, Loss: 0.5314579606056213, Train: 1.0000, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 089, Loss: 0.44381237030029297, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 090, Loss: 0.46925780177116394, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 091, Loss: 0.4830388128757477, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 092, Loss: 0.44902148842811584, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 093, Loss: 0.5124993920326233, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 094, Loss: 0.5025720596313477, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 095, Loss: 0.4912734925746918, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 096, Loss: 0.47279056906700134, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 097, Loss: 0.412275105714798, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 098, Loss: 0.5109966993331909, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 099, Loss: 0.4431079030036926, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 100, Loss: 0.5424521565437317, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 101, Loss: 0.3977786600589752, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 102, Loss: 0.45766618847846985, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 103, Loss: 0.42655423283576965, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 104, Loss: 0.4430479109287262, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 105, Loss: 0.45420536398887634, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 106, Loss: 0.4702649712562561, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 107, Loss: 0.4375118315219879, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 108, Loss: 0.4743795096874237, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 109, Loss: 0.575918972492218, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 110, Loss: 0.41468092799186707, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 111, Loss: 0.5221818685531616, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 112, Loss: 0.476807177066803, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 113, Loss: 0.43102458119392395, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 114, Loss: 0.41085827350616455, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 115, Loss: 0.45092758536338806, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 116, Loss: 0.3997562527656555, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 117, Loss: 0.3755987882614136, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 118, Loss: 0.43709394335746765, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 119, Loss: 0.43335968255996704, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 120, Loss: 0.4027608335018158, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 121, Loss: 0.44806790351867676, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 122, Loss: 0.4938514828681946, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 123, Loss: 0.4693031311035156, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 124, Loss: 0.48389703035354614, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 125, Loss: 0.4502866566181183, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 126, Loss: 0.4237809479236603, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 127, Loss: 0.4163022041320801, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 128, Loss: 0.4839054048061371, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 129, Loss: 0.36023619771003723, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 130, Loss: 0.39900699257850647, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 131, Loss: 0.44830718636512756, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 132, Loss: 0.44951102137565613, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 133, Loss: 0.4423675239086151, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 134, Loss: 0.4886128008365631, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 135, Loss: 0.46467551589012146, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 136, Loss: 0.4619467556476593, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 137, Loss: 0.4325924813747406, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 138, Loss: 0.3839403986930847, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 139, Loss: 0.41878658533096313, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 140, Loss: 0.3746597170829773, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 141, Loss: 0.4125460386276245, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 142, Loss: 0.5002367496490479, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 143, Loss: 0.4683378040790558, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 144, Loss: 0.39947688579559326, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 145, Loss: 0.3700941801071167, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 146, Loss: 0.36040207743644714, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 147, Loss: 0.5815854072570801, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 148, Loss: 0.3352399468421936, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 149, Loss: 0.45744001865386963, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 150, Loss: 0.4951343536376953, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 151, Loss: 0.4730699360370636, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 152, Loss: 0.45896318554878235, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 153, Loss: 0.4175373315811157, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 154, Loss: 0.4197607934474945, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 155, Loss: 0.36150315403938293, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 156, Loss: 0.5068330764770508, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 157, Loss: 0.46590298414230347, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 158, Loss: 0.47897258400917053, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 159, Loss: 0.4949599802494049, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 160, Loss: 0.4956830143928528, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 161, Loss: 0.5231279134750366, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 162, Loss: 0.5790331959724426, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 163, Loss: 0.46124187111854553, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 164, Loss: 0.366176962852478, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 165, Loss: 0.5101598501205444, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 166, Loss: 0.28990817070007324, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 167, Loss: 0.42195630073547363, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 168, Loss: 0.3875006437301636, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 169, Loss: 0.5049809217453003, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 170, Loss: 0.4238487780094147, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 171, Loss: 0.38189786672592163, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 172, Loss: 0.31515783071517944, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 173, Loss: 0.43897804617881775, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 174, Loss: 0.5602269768714905, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 175, Loss: 0.43476602435112, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 176, Loss: 0.3324506878852844, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 177, Loss: 0.40475478768348694, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 178, Loss: 0.39016732573509216, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 179, Loss: 0.3829772472381592, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 180, Loss: 0.4700874984264374, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 181, Loss: 0.37678980827331543, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 182, Loss: 0.3882814943790436, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 183, Loss: 0.362218976020813, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 184, Loss: 0.428955614566803, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 185, Loss: 0.41337841749191284, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 186, Loss: 0.3939279615879059, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 187, Loss: 0.2883771061897278, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 188, Loss: 0.3543688654899597, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 189, Loss: 0.4832121729850769, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 190, Loss: 0.49039730429649353, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 191, Loss: 0.4338093101978302, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 192, Loss: 0.6031613945960999, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 193, Loss: 0.2962069511413574, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 194, Loss: 0.47375357151031494, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 195, Loss: 0.4305938482284546, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 196, Loss: 0.3795951306819916, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 197, Loss: 0.4522796869277954, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 198, Loss: 0.49308809638023376, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 199, Loss: 0.3780125379562378, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 200, Loss: 0.5031267404556274, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 201, Loss: 0.5700585842132568, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 202, Loss: 0.34470653533935547, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 203, Loss: 0.42888301610946655, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 204, Loss: 0.343001127243042, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 205, Loss: 0.4547556936740875, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 206, Loss: 0.5262876749038696, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 207, Loss: 0.4150868058204651, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 208, Loss: 0.37653738260269165, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 209, Loss: 0.43666404485702515, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 210, Loss: 0.37333691120147705, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 211, Loss: 0.4986524283885956, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 212, Loss: 0.3476666212081909, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 213, Loss: 0.46027833223342896, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 214, Loss: 0.5202829241752625, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 215, Loss: 0.5389657616615295, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 216, Loss: 0.3047834038734436, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 217, Loss: 0.4796639084815979, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 218, Loss: 0.41931769251823425, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 219, Loss: 0.3610325753688812, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 220, Loss: 0.37549009919166565, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 221, Loss: 0.4502898156642914, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 222, Loss: 0.3699425160884857, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 223, Loss: 0.34330132603645325, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 224, Loss: 0.39789527654647827, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 225, Loss: 0.5401037931442261, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 226, Loss: 0.4398409128189087, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 227, Loss: 0.4193887412548065, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 228, Loss: 0.5405508875846863, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 229, Loss: 0.41587352752685547, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 230, Loss: 0.5065637826919556, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 231, Loss: 0.4049989879131317, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 232, Loss: 0.4253770709037781, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 233, Loss: 0.36921030282974243, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 234, Loss: 0.3629453182220459, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 235, Loss: 0.354278028011322, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 236, Loss: 0.5531483292579651, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 237, Loss: 0.3652682900428772, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 238, Loss: 0.35154059529304504, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 239, Loss: 0.46730345487594604, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 240, Loss: 0.4618868827819824, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 241, Loss: 0.4005747139453888, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 242, Loss: 0.3984014689922333, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 243, Loss: 0.4539965093135834, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 244, Loss: 0.3870096504688263, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 245, Loss: 0.336382120847702, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 246, Loss: 0.5044251084327698, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 247, Loss: 0.4634827673435211, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 248, Loss: 0.39606013894081116, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 249, Loss: 0.510169267654419, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 250, Loss: 0.4294678270816803, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 251, Loss: 0.4058228135108948, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 252, Loss: 0.45006701350212097, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 253, Loss: 0.5050212740898132, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 254, Loss: 0.412045955657959, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 255, Loss: 0.4134872257709503, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 256, Loss: 0.3677141070365906, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 257, Loss: 0.4106957018375397, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 258, Loss: 0.5780763030052185, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 259, Loss: 0.4616986811161041, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 260, Loss: 0.4239490330219269, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 261, Loss: 0.36916667222976685, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 262, Loss: 0.43733105063438416, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 263, Loss: 0.4255914092063904, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 264, Loss: 0.45203426480293274, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 265, Loss: 0.41387274861335754, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 266, Loss: 0.37262919545173645, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 267, Loss: 0.4026598632335663, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 268, Loss: 0.4114300608634949, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 269, Loss: 0.40622052550315857, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 270, Loss: 0.4019785523414612, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 271, Loss: 0.47933125495910645, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 272, Loss: 0.3543119728565216, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 273, Loss: 0.48949486017227173, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 274, Loss: 0.36886751651763916, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 275, Loss: 0.3289296329021454, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 276, Loss: 0.41809481382369995, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 277, Loss: 0.35246026515960693, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 278, Loss: 0.42564257979393005, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 279, Loss: 0.38271650671958923, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 280, Loss: 0.4745330810546875, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 281, Loss: 0.44330763816833496, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 282, Loss: 0.3465968668460846, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 283, Loss: 0.42466065287590027, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 284, Loss: 0.38157567381858826, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 285, Loss: 0.46178731322288513, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 286, Loss: 0.5501717329025269, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 287, Loss: 0.34226560592651367, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 288, Loss: 0.44433388113975525, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 289, Loss: 0.5430891513824463, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 290, Loss: 0.3986738622188568, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 291, Loss: 0.37507694959640503, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 292, Loss: 0.3381558954715729, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 293, Loss: 0.41893282532691956, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 294, Loss: 0.4962987005710602, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 295, Loss: 0.3876391053199768, Train: 1.0000, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 296, Loss: 0.3476862907409668, Train: 1.0000, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 297, Loss: 0.33601078391075134, Train: 1.0000, Val: 0.7900, Test: 0.8120\n",
      "Epoch: 298, Loss: 0.46408790349960327, Train: 1.0000, Val: 0.7920, Test: 0.8120\n",
      "Epoch: 299, Loss: 0.4346350431442261, Train: 1.0000, Val: 0.7900, Test: 0.8120\n",
      "Epoch: 300, Loss: 0.39152276515960693, Train: 1.0000, Val: 0.7900, Test: 0.8120\n",
      "Epoch: 301, Loss: 0.4588333070278168, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 302, Loss: 0.36143913865089417, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 303, Loss: 0.4856659471988678, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 304, Loss: 0.3850058317184448, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 305, Loss: 0.3970281779766083, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 306, Loss: 0.315361350774765, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 307, Loss: 0.48870882391929626, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 308, Loss: 0.39571917057037354, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 309, Loss: 0.38386812806129456, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 310, Loss: 0.4503677189350128, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 311, Loss: 0.43592146039009094, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 312, Loss: 0.4267021119594574, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 313, Loss: 0.3274584710597992, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 314, Loss: 0.43110665678977966, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 315, Loss: 0.47830817103385925, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 316, Loss: 0.4653718173503876, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 317, Loss: 0.5559044480323792, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 318, Loss: 0.382406622171402, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 319, Loss: 0.4136001765727997, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 320, Loss: 0.5378037095069885, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 321, Loss: 0.4471331834793091, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 322, Loss: 0.377554327249527, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 323, Loss: 0.40634000301361084, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 324, Loss: 0.511102020740509, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 325, Loss: 0.3886551558971405, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 326, Loss: 0.5106178522109985, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 327, Loss: 0.3303930461406708, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 328, Loss: 0.43136873841285706, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 329, Loss: 0.3552459478378296, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 330, Loss: 0.41274023056030273, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 331, Loss: 0.4319227337837219, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 332, Loss: 0.3930845558643341, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 333, Loss: 0.47168874740600586, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 334, Loss: 0.4699404537677765, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 335, Loss: 0.34460702538490295, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 336, Loss: 0.48644399642944336, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 337, Loss: 0.447820782661438, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 338, Loss: 0.39892151951789856, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 339, Loss: 0.4080319404602051, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 340, Loss: 0.41736170649528503, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 341, Loss: 0.3366880714893341, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 342, Loss: 0.454073041677475, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 343, Loss: 0.4403197765350342, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 344, Loss: 0.45775341987609863, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 345, Loss: 0.4136640131473541, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 346, Loss: 0.37984544038772583, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 347, Loss: 0.3619014322757721, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 348, Loss: 0.357203871011734, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 349, Loss: 0.4262271821498871, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 350, Loss: 0.4480629563331604, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 351, Loss: 0.5141379833221436, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 352, Loss: 0.4810032844543457, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 353, Loss: 0.41178327798843384, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 354, Loss: 0.3811924457550049, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 355, Loss: 0.32852062582969666, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 356, Loss: 0.4104113280773163, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 357, Loss: 0.3583945035934448, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 358, Loss: 0.5363250374794006, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 359, Loss: 0.4132821261882782, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 360, Loss: 0.3558664619922638, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 361, Loss: 0.3304610848426819, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 362, Loss: 0.34545570611953735, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 363, Loss: 0.4806448221206665, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 364, Loss: 0.4617832601070404, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 365, Loss: 0.4396158754825592, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 366, Loss: 0.4631248414516449, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 367, Loss: 0.4805136024951935, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 368, Loss: 0.43629318475723267, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 369, Loss: 0.5225075483322144, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 370, Loss: 0.3565499186515808, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 371, Loss: 0.36024272441864014, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 372, Loss: 0.4518931210041046, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 373, Loss: 0.41898486018180847, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 374, Loss: 0.3203396797180176, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 375, Loss: 0.3710571825504303, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 376, Loss: 0.4585500955581665, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 377, Loss: 0.39758795499801636, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 378, Loss: 0.37865373492240906, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 379, Loss: 0.4349612295627594, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 380, Loss: 0.42103731632232666, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 381, Loss: 0.4436766505241394, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 382, Loss: 0.5054556727409363, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 383, Loss: 0.3848396837711334, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 384, Loss: 0.3924814462661743, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 385, Loss: 0.42513930797576904, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 386, Loss: 0.4271285831928253, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 387, Loss: 0.32838699221611023, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 388, Loss: 0.3935338258743286, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 389, Loss: 0.40129226446151733, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 390, Loss: 0.605190634727478, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 391, Loss: 0.4092503786087036, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 392, Loss: 0.3874434232711792, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 393, Loss: 0.33283838629722595, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 394, Loss: 0.4039350748062134, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 395, Loss: 0.45713233947753906, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 396, Loss: 0.39538249373435974, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 397, Loss: 0.435408353805542, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 398, Loss: 0.3577880263328552, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 399, Loss: 0.39184561371803284, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 400, Loss: 0.4546191096305847, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 401, Loss: 0.3557002544403076, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 402, Loss: 0.36728864908218384, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 403, Loss: 0.45695924758911133, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 404, Loss: 0.464926153421402, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 405, Loss: 0.37340492010116577, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 406, Loss: 0.28018718957901, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 407, Loss: 0.39235877990722656, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 408, Loss: 0.42913687229156494, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 409, Loss: 0.5008155703544617, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 410, Loss: 0.4374311864376068, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 411, Loss: 0.4972184896469116, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 412, Loss: 0.3760993778705597, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 413, Loss: 0.4218917191028595, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 414, Loss: 0.4744773805141449, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 415, Loss: 0.2930305004119873, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 416, Loss: 0.3068826496601105, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 417, Loss: 0.346927285194397, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 418, Loss: 0.4216538965702057, Train: 1.0000, Val: 0.7500, Test: 0.8120\n",
      "Epoch: 419, Loss: 0.5021653771400452, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 420, Loss: 0.4642165005207062, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 421, Loss: 0.3489975929260254, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 422, Loss: 0.4055795669555664, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 423, Loss: 0.3137148320674896, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 424, Loss: 0.43107524514198303, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 425, Loss: 0.3605416417121887, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 426, Loss: 0.3661964237689972, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 427, Loss: 0.408565491437912, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 428, Loss: 0.3837762176990509, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 429, Loss: 0.5019593238830566, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 430, Loss: 0.47390812635421753, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 431, Loss: 0.4473333954811096, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 432, Loss: 0.4499466121196747, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 433, Loss: 0.3900342583656311, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 434, Loss: 0.3949190080165863, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 435, Loss: 0.46333983540534973, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 436, Loss: 0.4027101993560791, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 437, Loss: 0.3278016746044159, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 438, Loss: 0.3894236385822296, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 439, Loss: 0.36679312586784363, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 440, Loss: 0.3820556402206421, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 441, Loss: 0.3993898034095764, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 442, Loss: 0.4668842852115631, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 443, Loss: 0.38752487301826477, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 444, Loss: 0.3306839168071747, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 445, Loss: 0.35361984372138977, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 446, Loss: 0.42546072602272034, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 447, Loss: 0.38317418098449707, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 448, Loss: 0.5183569192886353, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 449, Loss: 0.41916248202323914, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 450, Loss: 0.4829813838005066, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 451, Loss: 0.5042232871055603, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 452, Loss: 0.4242474436759949, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 453, Loss: 0.4501146674156189, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 454, Loss: 0.37270525097846985, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 455, Loss: 0.39700472354888916, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 456, Loss: 0.31606635451316833, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 457, Loss: 0.33857113122940063, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 458, Loss: 0.45209255814552307, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 459, Loss: 0.47968220710754395, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 460, Loss: 0.46695181727409363, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 461, Loss: 0.5006148815155029, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 462, Loss: 0.38330405950546265, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 463, Loss: 0.4626162350177765, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 464, Loss: 0.3769563138484955, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 465, Loss: 0.41994601488113403, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 466, Loss: 0.4071688652038574, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 467, Loss: 0.38692501187324524, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 468, Loss: 0.3053818643093109, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 469, Loss: 0.4329802393913269, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 470, Loss: 0.3153505027294159, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 471, Loss: 0.358564168214798, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 472, Loss: 0.37223413586616516, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 473, Loss: 0.4298036992549896, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 474, Loss: 0.482872873544693, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 475, Loss: 0.3462677597999573, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 476, Loss: 0.43439173698425293, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 477, Loss: 0.42589473724365234, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 478, Loss: 0.42831477522850037, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 479, Loss: 0.39945188164711, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 480, Loss: 0.5139809846878052, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 481, Loss: 0.45077207684516907, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 482, Loss: 0.35830751061439514, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 483, Loss: 0.46705392003059387, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 484, Loss: 0.255528062582016, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 485, Loss: 0.31407448649406433, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 486, Loss: 0.3467411994934082, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 487, Loss: 0.3229370713233948, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 488, Loss: 0.4360158145427704, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 489, Loss: 0.3825971186161041, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 490, Loss: 0.37524059414863586, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 491, Loss: 0.28231510519981384, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 492, Loss: 0.4328056275844574, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 493, Loss: 0.50008624792099, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 494, Loss: 0.4070794880390167, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 495, Loss: 0.3259075880050659, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 496, Loss: 0.4364182651042938, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 497, Loss: 0.34162670373916626, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 498, Loss: 0.32060256600379944, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 499, Loss: 0.3973313868045807, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 500, Loss: 0.4372390806674957, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 501, Loss: 0.45463791489601135, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 502, Loss: 0.46392419934272766, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 503, Loss: 0.4300471842288971, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 504, Loss: 0.47082987427711487, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 505, Loss: 0.48699891567230225, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 506, Loss: 0.41390159726142883, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 507, Loss: 0.33203354477882385, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 508, Loss: 0.42899224162101746, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 509, Loss: 0.44164666533470154, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 510, Loss: 0.39414840936660767, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 511, Loss: 0.48248863220214844, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 512, Loss: 0.4127173125743866, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 513, Loss: 0.37012597918510437, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 514, Loss: 0.4033099412918091, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 515, Loss: 0.399016797542572, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 516, Loss: 0.32082927227020264, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 517, Loss: 0.37178945541381836, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 518, Loss: 0.40891140699386597, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 519, Loss: 0.3561106324195862, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 520, Loss: 0.4613487720489502, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 521, Loss: 0.3966262936592102, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 522, Loss: 0.5220317840576172, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 523, Loss: 0.372883141040802, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 524, Loss: 0.5810652375221252, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 525, Loss: 0.416476309299469, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 526, Loss: 0.4305436611175537, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 527, Loss: 0.435638427734375, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 528, Loss: 0.4196076989173889, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 529, Loss: 0.3862205147743225, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 530, Loss: 0.34826532006263733, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 531, Loss: 0.3392995297908783, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 532, Loss: 0.3991676867008209, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 533, Loss: 0.4572889506816864, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 534, Loss: 0.502309262752533, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 535, Loss: 0.2826286256313324, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 536, Loss: 0.31639203429222107, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 537, Loss: 0.45429736375808716, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 538, Loss: 0.2957606911659241, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 539, Loss: 0.30389097332954407, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 540, Loss: 0.4131334722042084, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 541, Loss: 0.4247966706752777, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 542, Loss: 0.25112807750701904, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 543, Loss: 0.5351131558418274, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 544, Loss: 0.4311544597148895, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 545, Loss: 0.4133526384830475, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 546, Loss: 0.4099001884460449, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 547, Loss: 0.33874163031578064, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 548, Loss: 0.4276859760284424, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 549, Loss: 0.48437052965164185, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 550, Loss: 0.4107491672039032, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 551, Loss: 0.3590344190597534, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 552, Loss: 0.32394683361053467, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 553, Loss: 0.3277435302734375, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 554, Loss: 0.3427237272262573, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 555, Loss: 0.39341574907302856, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 556, Loss: 0.4754997193813324, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 557, Loss: 0.3170047998428345, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 558, Loss: 0.41681617498397827, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 559, Loss: 0.47805726528167725, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 560, Loss: 0.40332165360450745, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 561, Loss: 0.36744657158851624, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 562, Loss: 0.36616572737693787, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 563, Loss: 0.4972471594810486, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 564, Loss: 0.3443593978881836, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 565, Loss: 0.40994301438331604, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 566, Loss: 0.30721592903137207, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 567, Loss: 0.4007684886455536, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 568, Loss: 0.510063886642456, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 569, Loss: 0.4546920359134674, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 570, Loss: 0.3434380888938904, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 571, Loss: 0.36546945571899414, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 572, Loss: 0.47427570819854736, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 573, Loss: 0.32007336616516113, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 574, Loss: 0.5205867290496826, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 575, Loss: 0.41094970703125, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 576, Loss: 0.30117955803871155, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 577, Loss: 0.3381177484989166, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 578, Loss: 0.4182133972644806, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 579, Loss: 0.4515245854854584, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 580, Loss: 0.5225366353988647, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 581, Loss: 0.4977138936519623, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 582, Loss: 0.34203457832336426, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 583, Loss: 0.3420925736427307, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 584, Loss: 0.3950206935405731, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 585, Loss: 0.3307719826698303, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 586, Loss: 0.38889792561531067, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 587, Loss: 0.3981967866420746, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 588, Loss: 0.39261963963508606, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 589, Loss: 0.48609864711761475, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 590, Loss: 0.32528653740882874, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 591, Loss: 0.33235177397727966, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 592, Loss: 0.3880237340927124, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 593, Loss: 0.41206738352775574, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 594, Loss: 0.4044327437877655, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 595, Loss: 0.3880177438259125, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 596, Loss: 0.353547602891922, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 597, Loss: 0.4528467357158661, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 598, Loss: 0.31510865688323975, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 599, Loss: 0.37587758898735046, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 600, Loss: 0.40456563234329224, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 601, Loss: 0.4455709755420685, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 602, Loss: 0.3846718370914459, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 603, Loss: 0.36257901787757874, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 604, Loss: 0.462064266204834, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 605, Loss: 0.29667654633522034, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 606, Loss: 0.4211268723011017, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 607, Loss: 0.4634894132614136, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 608, Loss: 0.3959486484527588, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 609, Loss: 0.3666996955871582, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 610, Loss: 0.3968740701675415, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 611, Loss: 0.4525468051433563, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 612, Loss: 0.3895590901374817, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 613, Loss: 0.4806877672672272, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 614, Loss: 0.3432181775569916, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 615, Loss: 0.4583300054073334, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 616, Loss: 0.4086298942565918, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 617, Loss: 0.4513411223888397, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 618, Loss: 0.42552050948143005, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 619, Loss: 0.4023098945617676, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 620, Loss: 0.36788100004196167, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 621, Loss: 0.5626620054244995, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 622, Loss: 0.39754369854927063, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 623, Loss: 0.5116851329803467, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 624, Loss: 0.33951374888420105, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 625, Loss: 0.3553629517555237, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 626, Loss: 0.41298770904541016, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 627, Loss: 0.48323434591293335, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 628, Loss: 0.36267203092575073, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 629, Loss: 0.44794270396232605, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 630, Loss: 0.4901154041290283, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 631, Loss: 0.49318695068359375, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 632, Loss: 0.39122140407562256, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 633, Loss: 0.39824891090393066, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 634, Loss: 0.409252792596817, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 635, Loss: 0.493423193693161, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 636, Loss: 0.5084176063537598, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 637, Loss: 0.38837090134620667, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 638, Loss: 0.391981303691864, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 639, Loss: 0.38492104411125183, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 640, Loss: 0.36079704761505127, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 641, Loss: 0.35184937715530396, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 642, Loss: 0.42668119072914124, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 643, Loss: 0.4025877118110657, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 644, Loss: 0.3195134103298187, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 645, Loss: 0.42332106828689575, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 646, Loss: 0.3311605751514435, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 647, Loss: 0.39148324728012085, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 648, Loss: 0.36656439304351807, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 649, Loss: 0.41373831033706665, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 650, Loss: 0.41007620096206665, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 651, Loss: 0.5427172183990479, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 652, Loss: 0.42948752641677856, Train: 1.0000, Val: 0.7520, Test: 0.8120\n",
      "Epoch: 653, Loss: 0.2953208386898041, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 654, Loss: 0.3975681960582733, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 655, Loss: 0.4296239912509918, Train: 1.0000, Val: 0.7540, Test: 0.8120\n",
      "Epoch: 656, Loss: 0.37054744362831116, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 657, Loss: 0.4514785706996918, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 658, Loss: 0.3314571976661682, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 659, Loss: 0.41459861397743225, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 660, Loss: 0.4240531623363495, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 661, Loss: 0.3153853416442871, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 662, Loss: 0.37009957432746887, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 663, Loss: 0.44052162766456604, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 664, Loss: 0.4644591808319092, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 665, Loss: 0.3439713716506958, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 666, Loss: 0.44708213210105896, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 667, Loss: 0.41953960061073303, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 668, Loss: 0.394479364156723, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 669, Loss: 0.4306568205356598, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 670, Loss: 0.39815253019332886, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 671, Loss: 0.3988015353679657, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 672, Loss: 0.3630602955818176, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 673, Loss: 0.4215773046016693, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 674, Loss: 0.3791739046573639, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 675, Loss: 0.5074751973152161, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 676, Loss: 0.32248327136039734, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 677, Loss: 0.3980788290500641, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 678, Loss: 0.48347553610801697, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 679, Loss: 0.3819602429866791, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 680, Loss: 0.5566943287849426, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 681, Loss: 0.4375174939632416, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 682, Loss: 0.4305768311023712, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 683, Loss: 0.404481440782547, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 684, Loss: 0.3283643424510956, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 685, Loss: 0.37757378816604614, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 686, Loss: 0.38070598244667053, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 687, Loss: 0.4846948981285095, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 688, Loss: 0.3551732301712036, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 689, Loss: 0.3926181197166443, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 690, Loss: 0.3224914073944092, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 691, Loss: 0.3958565294742584, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 692, Loss: 0.4449007213115692, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 693, Loss: 0.4333076477050781, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 694, Loss: 0.3823119103908539, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 695, Loss: 0.49438172578811646, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 696, Loss: 0.47267237305641174, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 697, Loss: 0.3804820477962494, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 698, Loss: 0.3528425693511963, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 699, Loss: 0.4096510112285614, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 700, Loss: 0.4135545492172241, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 701, Loss: 0.43224480748176575, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 702, Loss: 0.46646547317504883, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 703, Loss: 0.3986653983592987, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 704, Loss: 0.4050436019897461, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 705, Loss: 0.41007548570632935, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 706, Loss: 0.3720436096191406, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 707, Loss: 0.36887675523757935, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 708, Loss: 0.42366933822631836, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 709, Loss: 0.42430001497268677, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 710, Loss: 0.303843230009079, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 711, Loss: 0.3482646346092224, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 712, Loss: 0.4086095988750458, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 713, Loss: 0.3948476016521454, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 714, Loss: 0.3597572147846222, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 715, Loss: 0.37679794430732727, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 716, Loss: 0.47410884499549866, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 717, Loss: 0.42346757650375366, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 718, Loss: 0.441389799118042, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 719, Loss: 0.3522261083126068, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 720, Loss: 0.27065715193748474, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 721, Loss: 0.42920970916748047, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 722, Loss: 0.42377781867980957, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 723, Loss: 0.3930017650127411, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 724, Loss: 0.4768398702144623, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 725, Loss: 0.2894417345523834, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 726, Loss: 0.37726351618766785, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 727, Loss: 0.4911799728870392, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 728, Loss: 0.5011088252067566, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 729, Loss: 0.37787988781929016, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 730, Loss: 0.3896249532699585, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 731, Loss: 0.3056178092956543, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 732, Loss: 0.48545724153518677, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 733, Loss: 0.37570786476135254, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 734, Loss: 0.4546393156051636, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 735, Loss: 0.3800351619720459, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 736, Loss: 0.43179506063461304, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 737, Loss: 0.3985476791858673, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 738, Loss: 0.45905041694641113, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 739, Loss: 0.4731014370918274, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 740, Loss: 0.4030725061893463, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 741, Loss: 0.43557989597320557, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 742, Loss: 0.5051489472389221, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 743, Loss: 0.46111994981765747, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 744, Loss: 0.4614165723323822, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 745, Loss: 0.42199110984802246, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 746, Loss: 0.3913350999355316, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 747, Loss: 0.45108985900878906, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 748, Loss: 0.407912015914917, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 749, Loss: 0.47171151638031006, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 750, Loss: 0.4724891781806946, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 751, Loss: 0.37687867879867554, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 752, Loss: 0.36921897530555725, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 753, Loss: 0.44549694657325745, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 754, Loss: 0.3421630561351776, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 755, Loss: 0.4316239356994629, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 756, Loss: 0.36625975370407104, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 757, Loss: 0.3102782070636749, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 758, Loss: 0.39644408226013184, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 759, Loss: 0.3764367699623108, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 760, Loss: 0.3872131109237671, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 761, Loss: 0.3900741636753082, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 762, Loss: 0.42493173480033875, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 763, Loss: 0.4130082130432129, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 764, Loss: 0.4654300808906555, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 765, Loss: 0.3006700873374939, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 766, Loss: 0.4323104918003082, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 767, Loss: 0.39849674701690674, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 768, Loss: 0.36865097284317017, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 769, Loss: 0.34878864884376526, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 770, Loss: 0.4920364320278168, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 771, Loss: 0.38658344745635986, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 772, Loss: 0.436541348695755, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 773, Loss: 0.36238643527030945, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 774, Loss: 0.47402268648147583, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 775, Loss: 0.3776971697807312, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 776, Loss: 0.3031095266342163, Train: 1.0000, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 777, Loss: 0.3655102849006653, Train: 1.0000, Val: 0.7880, Test: 0.8120\n",
      "Epoch: 778, Loss: 0.5391657948493958, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 779, Loss: 0.3925631642341614, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 780, Loss: 0.4168301224708557, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 781, Loss: 0.4490167498588562, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 782, Loss: 0.44611531496047974, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 783, Loss: 0.3458435833454132, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 784, Loss: 0.3421100676059723, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 785, Loss: 0.4556073546409607, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 786, Loss: 0.4065639078617096, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 787, Loss: 0.3326423466205597, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 788, Loss: 0.44413015246391296, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 789, Loss: 0.3348916172981262, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 790, Loss: 0.45616161823272705, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 791, Loss: 0.47822532057762146, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 792, Loss: 0.32070472836494446, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 793, Loss: 0.34343981742858887, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 794, Loss: 0.34280115365982056, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 795, Loss: 0.5079408288002014, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 796, Loss: 0.33653420209884644, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 797, Loss: 0.40448546409606934, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 798, Loss: 0.39416369795799255, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 799, Loss: 0.42550018429756165, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 800, Loss: 0.5227640867233276, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 801, Loss: 0.4671182930469513, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 802, Loss: 0.5058083534240723, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 803, Loss: 0.3583156168460846, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 804, Loss: 0.3058885633945465, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 805, Loss: 0.33069688081741333, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 806, Loss: 0.4240586757659912, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 807, Loss: 0.4286861717700958, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 808, Loss: 0.3057750165462494, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 809, Loss: 0.370158851146698, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 810, Loss: 0.4624088406562805, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 811, Loss: 0.3075281083583832, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 812, Loss: 0.5702017545700073, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 813, Loss: 0.39123591780662537, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 814, Loss: 0.37741655111312866, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 815, Loss: 0.44311413168907166, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 816, Loss: 0.2662002146244049, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 817, Loss: 0.6138961315155029, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 818, Loss: 0.3414977192878723, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 819, Loss: 0.3903588056564331, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 820, Loss: 0.40287792682647705, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 821, Loss: 0.46769022941589355, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 822, Loss: 0.33617663383483887, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 823, Loss: 0.41171860694885254, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 824, Loss: 0.3926551640033722, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 825, Loss: 0.32824885845184326, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 826, Loss: 0.41748538613319397, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 827, Loss: 0.45353108644485474, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 828, Loss: 0.46632614731788635, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 829, Loss: 0.4065427780151367, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 830, Loss: 0.38424065709114075, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 831, Loss: 0.40068233013153076, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 832, Loss: 0.47651687264442444, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 833, Loss: 0.3305378556251526, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 834, Loss: 0.37920472025871277, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 835, Loss: 0.3337539732456207, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 836, Loss: 0.48763808608055115, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 837, Loss: 0.47239553928375244, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 838, Loss: 0.38002121448516846, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 839, Loss: 0.39378732442855835, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 840, Loss: 0.39498990774154663, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 841, Loss: 0.31157100200653076, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 842, Loss: 0.4073949158191681, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 843, Loss: 0.4452836513519287, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 844, Loss: 0.3306654691696167, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 845, Loss: 0.3604413866996765, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 846, Loss: 0.3709765672683716, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 847, Loss: 0.43757328391075134, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 848, Loss: 0.27914130687713623, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 849, Loss: 0.39413079619407654, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 850, Loss: 0.4026564955711365, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 851, Loss: 0.3521052300930023, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 852, Loss: 0.4779745936393738, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 853, Loss: 0.38199812173843384, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 854, Loss: 0.4213448166847229, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 855, Loss: 0.4715956151485443, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 856, Loss: 0.41543829441070557, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 857, Loss: 0.4090358316898346, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 858, Loss: 0.5337656140327454, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 859, Loss: 0.33908480405807495, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 860, Loss: 0.4263916313648224, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 861, Loss: 0.4794667959213257, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 862, Loss: 0.3262495994567871, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 863, Loss: 0.4212794005870819, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 864, Loss: 0.4572031795978546, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 865, Loss: 0.40980520844459534, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 866, Loss: 0.39420780539512634, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 867, Loss: 0.37356022000312805, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 868, Loss: 0.5685035586357117, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 869, Loss: 0.4692554473876953, Train: 1.0000, Val: 0.7840, Test: 0.8120\n",
      "Epoch: 870, Loss: 0.44700103998184204, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 871, Loss: 0.3247731029987335, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 872, Loss: 0.43489548563957214, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 873, Loss: 0.32650431990623474, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 874, Loss: 0.3519643545150757, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 875, Loss: 0.44020044803619385, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 876, Loss: 0.5472298860549927, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 877, Loss: 0.37539616227149963, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 878, Loss: 0.4246051013469696, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 879, Loss: 0.47785377502441406, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 880, Loss: 0.45098206400871277, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 881, Loss: 0.4910663068294525, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 882, Loss: 0.3973809480667114, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 883, Loss: 0.46187034249305725, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 884, Loss: 0.3672958314418793, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 885, Loss: 0.33800873160362244, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 886, Loss: 0.3893822133541107, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 887, Loss: 0.29117727279663086, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 888, Loss: 0.3300853371620178, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 889, Loss: 0.4228939414024353, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 890, Loss: 0.3514224886894226, Train: 1.0000, Val: 0.7560, Test: 0.8120\n",
      "Epoch: 891, Loss: 0.45324933528900146, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 892, Loss: 0.4392128586769104, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 893, Loss: 0.3791712820529938, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 894, Loss: 0.4766785800457001, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 895, Loss: 0.4185207486152649, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 896, Loss: 0.3362528383731842, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 897, Loss: 0.43783602118492126, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 898, Loss: 0.582308292388916, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 899, Loss: 0.42485377192497253, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 900, Loss: 0.5158657431602478, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 901, Loss: 0.33903974294662476, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 902, Loss: 0.33149704337120056, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 903, Loss: 0.3659243583679199, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 904, Loss: 0.3698849081993103, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 905, Loss: 0.4177311956882477, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 906, Loss: 0.43300092220306396, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 907, Loss: 0.45677849650382996, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 908, Loss: 0.36289453506469727, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 909, Loss: 0.35080549120903015, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 910, Loss: 0.49615129828453064, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 911, Loss: 0.4656738340854645, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 912, Loss: 0.3363836705684662, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 913, Loss: 0.4735657274723053, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 914, Loss: 0.46476271748542786, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 915, Loss: 0.5243233442306519, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 916, Loss: 0.4354528784751892, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 917, Loss: 0.44522154331207275, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 918, Loss: 0.5318512320518494, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 919, Loss: 0.47351640462875366, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 920, Loss: 0.4259272515773773, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 921, Loss: 0.31327900290489197, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 922, Loss: 0.36011043190956116, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 923, Loss: 0.5249149203300476, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 924, Loss: 0.39426150918006897, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 925, Loss: 0.5465576648712158, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 926, Loss: 0.34047731757164, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 927, Loss: 0.3843957483768463, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 928, Loss: 0.4097650349140167, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 929, Loss: 0.39441314339637756, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 930, Loss: 0.4505836069583893, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 931, Loss: 0.48303189873695374, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 932, Loss: 0.3185102045536041, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 933, Loss: 0.4772109389305115, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 934, Loss: 0.3255061209201813, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 935, Loss: 0.5446406602859497, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 936, Loss: 0.3857535421848297, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 937, Loss: 0.40478894114494324, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 938, Loss: 0.43990570306777954, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 939, Loss: 0.4107123613357544, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 940, Loss: 0.4449595808982849, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 941, Loss: 0.3952978253364563, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 942, Loss: 0.3695174753665924, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 943, Loss: 0.46207374334335327, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 944, Loss: 0.3821052312850952, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 945, Loss: 0.35709428787231445, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 946, Loss: 0.35538220405578613, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 947, Loss: 0.3411487936973572, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 948, Loss: 0.481638103723526, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 949, Loss: 0.5128692984580994, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 950, Loss: 0.3036012053489685, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 951, Loss: 0.41191157698631287, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 952, Loss: 0.35951557755470276, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 953, Loss: 0.450501948595047, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 954, Loss: 0.4572337567806244, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 955, Loss: 0.4518159329891205, Train: 1.0000, Val: 0.7800, Test: 0.8120\n",
      "Epoch: 956, Loss: 0.34843969345092773, Train: 1.0000, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 957, Loss: 0.41042956709861755, Train: 1.0000, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 958, Loss: 0.43963029980659485, Train: 1.0000, Val: 0.7780, Test: 0.8120\n",
      "Epoch: 959, Loss: 0.4368976950645447, Train: 1.0000, Val: 0.7760, Test: 0.8120\n",
      "Epoch: 960, Loss: 0.4225350618362427, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 961, Loss: 0.4853903651237488, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 962, Loss: 0.38228902220726013, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 963, Loss: 0.4292507767677307, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 964, Loss: 0.35036730766296387, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 965, Loss: 0.4036044776439667, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 966, Loss: 0.4582405686378479, Train: 1.0000, Val: 0.7700, Test: 0.8120\n",
      "Epoch: 967, Loss: 0.3863176703453064, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 968, Loss: 0.3159162998199463, Train: 1.0000, Val: 0.7720, Test: 0.8120\n",
      "Epoch: 969, Loss: 0.3316519260406494, Train: 1.0000, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 970, Loss: 0.46668902039527893, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 971, Loss: 0.42614489793777466, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 972, Loss: 0.4506761133670807, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 973, Loss: 0.3568520247936249, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 974, Loss: 0.49916255474090576, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 975, Loss: 0.4845873713493347, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Epoch: 976, Loss: 0.4481995403766632, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 977, Loss: 0.4209569990634918, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 978, Loss: 0.4188779592514038, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 979, Loss: 0.3550834059715271, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 980, Loss: 0.43683409690856934, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 981, Loss: 0.40252047777175903, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 982, Loss: 0.3994881510734558, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 983, Loss: 0.38077253103256226, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 984, Loss: 0.4199376702308655, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 985, Loss: 0.429350882768631, Train: 1.0000, Val: 0.7680, Test: 0.8120\n",
      "Epoch: 986, Loss: 0.44073647260665894, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 987, Loss: 0.41024795174598694, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 988, Loss: 0.2789391577243805, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 989, Loss: 0.4663602411746979, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 990, Loss: 0.39843201637268066, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 991, Loss: 0.3757876753807068, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 992, Loss: 0.3950596749782562, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 993, Loss: 0.4156211018562317, Train: 1.0000, Val: 0.7620, Test: 0.8120\n",
      "Epoch: 994, Loss: 0.42440253496170044, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 995, Loss: 0.3444312512874603, Train: 1.0000, Val: 0.7580, Test: 0.8120\n",
      "Epoch: 996, Loss: 0.26627376675605774, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 997, Loss: 0.39717280864715576, Train: 1.0000, Val: 0.7600, Test: 0.8120\n",
      "Epoch: 998, Loss: 0.3562919497489929, Train: 1.0000, Val: 0.7660, Test: 0.8120\n",
      "Epoch: 999, Loss: 0.43275150656700134, Train: 1.0000, Val: 0.7640, Test: 0.8120\n",
      "Best Val Acc: 0.8040 Test Acc: 0.8120\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 1000):\n",
    "    loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe8eaf950b0cf64c1b70de22759d9a144a2595c541b4003711edd1f96d908e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

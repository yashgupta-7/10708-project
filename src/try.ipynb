{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, edgeindex2adj\n",
    "from models import GCN, GAT, LP\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "pubmed = Planetoid(root='.', name='Pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 24, dataset.num_classes)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "from models import ALP, GCN\n",
    "dataset = cora\n",
    "data = dataset[0]\n",
    "\n",
    "# take k random training nodes for each class\n",
    "k = 2\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "print(data.train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.945435643196106, Train: 0.6429, Val: 0.2560, Test: 0.2320\n",
      "Epoch: 002, Loss: 1.8319387435913086, Train: 0.6429, Val: 0.3140, Test: 0.3160\n",
      "Epoch: 003, Loss: 1.7161442041397095, Train: 0.7857, Val: 0.3200, Test: 0.3490\n",
      "Epoch: 004, Loss: 1.5864388942718506, Train: 0.5714, Val: 0.3160, Test: 0.3490\n",
      "Epoch: 005, Loss: 1.4705387353897095, Train: 0.5714, Val: 0.3300, Test: 0.3310\n",
      "Epoch: 006, Loss: 1.3599216938018799, Train: 0.7143, Val: 0.3360, Test: 0.3350\n",
      "Epoch: 007, Loss: 1.2562779188156128, Train: 0.7857, Val: 0.3320, Test: 0.3350\n",
      "Epoch: 008, Loss: 1.162876009941101, Train: 0.8571, Val: 0.3240, Test: 0.3350\n",
      "Epoch: 009, Loss: 1.0791798830032349, Train: 0.8571, Val: 0.3260, Test: 0.3350\n",
      "Epoch: 010, Loss: 1.0050922632217407, Train: 0.8571, Val: 0.3140, Test: 0.3350\n",
      "Epoch: 011, Loss: 0.9391421675682068, Train: 0.8571, Val: 0.3140, Test: 0.3350\n",
      "Epoch: 012, Loss: 0.8810887932777405, Train: 0.8571, Val: 0.3000, Test: 0.3350\n",
      "Epoch: 013, Loss: 0.828885555267334, Train: 0.7857, Val: 0.2920, Test: 0.3350\n",
      "Epoch: 014, Loss: 0.7825105786323547, Train: 0.7857, Val: 0.2860, Test: 0.3350\n",
      "Epoch: 015, Loss: 0.738869309425354, Train: 0.8571, Val: 0.2940, Test: 0.3350\n",
      "Epoch: 016, Loss: 0.6988738775253296, Train: 0.7857, Val: 0.2940, Test: 0.3350\n",
      "Epoch: 017, Loss: 0.6621667146682739, Train: 0.7857, Val: 0.2960, Test: 0.3350\n",
      "Epoch: 018, Loss: 0.6279042363166809, Train: 0.7857, Val: 0.2980, Test: 0.3350\n",
      "Epoch: 019, Loss: 0.5956424474716187, Train: 0.8571, Val: 0.3100, Test: 0.3350\n",
      "Epoch: 020, Loss: 0.5650885701179504, Train: 0.9286, Val: 0.3220, Test: 0.3350\n",
      "Epoch: 021, Loss: 0.5363749265670776, Train: 0.9286, Val: 0.3300, Test: 0.3350\n",
      "Epoch: 022, Loss: 0.5094362497329712, Train: 0.9286, Val: 0.3360, Test: 0.3350\n",
      "Epoch: 023, Loss: 0.48389098048210144, Train: 1.0000, Val: 0.3500, Test: 0.3550\n",
      "Epoch: 024, Loss: 0.45989125967025757, Train: 1.0000, Val: 0.3560, Test: 0.3650\n",
      "Epoch: 025, Loss: 0.43731504678726196, Train: 1.0000, Val: 0.3700, Test: 0.3790\n",
      "Epoch: 026, Loss: 0.41621899604797363, Train: 1.0000, Val: 0.3800, Test: 0.3840\n",
      "Epoch: 027, Loss: 0.3964628279209137, Train: 0.9286, Val: 0.3980, Test: 0.3900\n",
      "Epoch: 028, Loss: 0.378483384847641, Train: 1.0000, Val: 0.4020, Test: 0.3980\n",
      "Epoch: 029, Loss: 0.36094942688941956, Train: 1.0000, Val: 0.4000, Test: 0.3980\n",
      "Epoch: 030, Loss: 0.3451593816280365, Train: 1.0000, Val: 0.4120, Test: 0.4080\n",
      "Epoch: 031, Loss: 0.3305915296077728, Train: 1.0000, Val: 0.4180, Test: 0.4110\n",
      "Epoch: 032, Loss: 0.3172926902770996, Train: 1.0000, Val: 0.4160, Test: 0.4110\n",
      "Epoch: 033, Loss: 0.30522164702415466, Train: 1.0000, Val: 0.4180, Test: 0.4110\n",
      "Epoch: 034, Loss: 0.2943878173828125, Train: 1.0000, Val: 0.4200, Test: 0.4210\n",
      "Epoch: 035, Loss: 0.2846589684486389, Train: 1.0000, Val: 0.4220, Test: 0.4250\n",
      "Epoch: 036, Loss: 0.27614396810531616, Train: 1.0000, Val: 0.4240, Test: 0.4240\n",
      "Epoch: 037, Loss: 0.26851120591163635, Train: 1.0000, Val: 0.4280, Test: 0.4250\n",
      "Epoch: 038, Loss: 0.26167136430740356, Train: 1.0000, Val: 0.4260, Test: 0.4250\n",
      "Epoch: 039, Loss: 0.25559085607528687, Train: 1.0000, Val: 0.4280, Test: 0.4250\n",
      "Epoch: 040, Loss: 0.25016942620277405, Train: 1.0000, Val: 0.4320, Test: 0.4270\n",
      "Epoch: 041, Loss: 0.24526651203632355, Train: 1.0000, Val: 0.4380, Test: 0.4300\n",
      "Epoch: 042, Loss: 0.2408447265625, Train: 1.0000, Val: 0.4380, Test: 0.4300\n",
      "Epoch: 043, Loss: 0.23681320250034332, Train: 1.0000, Val: 0.4400, Test: 0.4250\n",
      "Epoch: 044, Loss: 0.23310723900794983, Train: 1.0000, Val: 0.4380, Test: 0.4250\n",
      "Epoch: 045, Loss: 0.22966822981834412, Train: 1.0000, Val: 0.4380, Test: 0.4250\n",
      "Epoch: 046, Loss: 0.22646722197532654, Train: 1.0000, Val: 0.4360, Test: 0.4250\n",
      "Epoch: 047, Loss: 0.22350236773490906, Train: 1.0000, Val: 0.4340, Test: 0.4250\n",
      "Epoch: 048, Loss: 0.220606729388237, Train: 1.0000, Val: 0.4360, Test: 0.4250\n",
      "Epoch: 049, Loss: 0.2180866301059723, Train: 1.0000, Val: 0.4380, Test: 0.4250\n",
      "Epoch: 050, Loss: 0.21530602872371674, Train: 1.0000, Val: 0.4400, Test: 0.4250\n",
      "Epoch: 051, Loss: 0.21277856826782227, Train: 1.0000, Val: 0.4400, Test: 0.4250\n",
      "Epoch: 052, Loss: 0.21034474670886993, Train: 1.0000, Val: 0.4400, Test: 0.4250\n",
      "Epoch: 053, Loss: 0.20800255239009857, Train: 1.0000, Val: 0.4400, Test: 0.4250\n",
      "Epoch: 054, Loss: 0.20573139190673828, Train: 1.0000, Val: 0.4400, Test: 0.4250\n",
      "Epoch: 055, Loss: 0.20342227816581726, Train: 1.0000, Val: 0.4380, Test: 0.4250\n",
      "Epoch: 056, Loss: 0.2012222707271576, Train: 1.0000, Val: 0.4380, Test: 0.4250\n",
      "Epoch: 057, Loss: 0.1990671306848526, Train: 1.0000, Val: 0.4420, Test: 0.4270\n",
      "Epoch: 058, Loss: 0.19699013233184814, Train: 1.0000, Val: 0.4400, Test: 0.4270\n",
      "Epoch: 059, Loss: 0.194883793592453, Train: 1.0000, Val: 0.4400, Test: 0.4270\n",
      "Epoch: 060, Loss: 0.19286438822746277, Train: 1.0000, Val: 0.4400, Test: 0.4270\n",
      "Epoch: 061, Loss: 0.19085566699504852, Train: 1.0000, Val: 0.4400, Test: 0.4270\n",
      "Epoch: 062, Loss: 0.18889367580413818, Train: 1.0000, Val: 0.4420, Test: 0.4270\n",
      "Epoch: 063, Loss: 0.18698763847351074, Train: 1.0000, Val: 0.4420, Test: 0.4270\n",
      "Epoch: 064, Loss: 0.18510307371616364, Train: 1.0000, Val: 0.4420, Test: 0.4270\n",
      "Epoch: 065, Loss: 0.18319907784461975, Train: 1.0000, Val: 0.4440, Test: 0.4350\n",
      "Epoch: 066, Loss: 0.18135936558246613, Train: 1.0000, Val: 0.4440, Test: 0.4350\n",
      "Epoch: 067, Loss: 0.1795467585325241, Train: 1.0000, Val: 0.4460, Test: 0.4370\n",
      "Epoch: 068, Loss: 0.17775991559028625, Train: 1.0000, Val: 0.4460, Test: 0.4370\n",
      "Epoch: 069, Loss: 0.17599712312221527, Train: 1.0000, Val: 0.4460, Test: 0.4370\n",
      "Epoch: 070, Loss: 0.17429852485656738, Train: 1.0000, Val: 0.4460, Test: 0.4370\n",
      "Epoch: 071, Loss: 0.17255790531635284, Train: 1.0000, Val: 0.4460, Test: 0.4370\n",
      "Epoch: 072, Loss: 0.17087940871715546, Train: 1.0000, Val: 0.4440, Test: 0.4370\n",
      "Epoch: 073, Loss: 0.1692246049642563, Train: 1.0000, Val: 0.4480, Test: 0.4450\n",
      "Epoch: 074, Loss: 0.16759097576141357, Train: 1.0000, Val: 0.4480, Test: 0.4450\n",
      "Epoch: 075, Loss: 0.16597501933574677, Train: 1.0000, Val: 0.4480, Test: 0.4450\n",
      "Epoch: 076, Loss: 0.1643763780593872, Train: 1.0000, Val: 0.4500, Test: 0.4500\n",
      "Epoch: 077, Loss: 0.162795752286911, Train: 1.0000, Val: 0.4480, Test: 0.4500\n",
      "Epoch: 078, Loss: 0.16122929751873016, Train: 1.0000, Val: 0.4460, Test: 0.4500\n",
      "Epoch: 079, Loss: 0.15968070924282074, Train: 1.0000, Val: 0.4460, Test: 0.4500\n",
      "Epoch: 080, Loss: 0.15821310877799988, Train: 1.0000, Val: 0.4460, Test: 0.4500\n",
      "Epoch: 081, Loss: 0.15663456916809082, Train: 1.0000, Val: 0.4500, Test: 0.4500\n",
      "Epoch: 082, Loss: 0.155136376619339, Train: 1.0000, Val: 0.4540, Test: 0.4540\n",
      "Epoch: 083, Loss: 0.1536536067724228, Train: 1.0000, Val: 0.4540, Test: 0.4540\n",
      "Epoch: 084, Loss: 0.1521858274936676, Train: 1.0000, Val: 0.4540, Test: 0.4540\n",
      "Epoch: 085, Loss: 0.15073952078819275, Train: 1.0000, Val: 0.4540, Test: 0.4540\n",
      "Epoch: 086, Loss: 0.1492941677570343, Train: 1.0000, Val: 0.4520, Test: 0.4540\n",
      "Epoch: 087, Loss: 0.14786937832832336, Train: 1.0000, Val: 0.4520, Test: 0.4540\n",
      "Epoch: 088, Loss: 0.14645829796791077, Train: 1.0000, Val: 0.4520, Test: 0.4540\n",
      "Epoch: 089, Loss: 0.14509865641593933, Train: 1.0000, Val: 0.4560, Test: 0.4540\n",
      "Epoch: 090, Loss: 0.1436782330274582, Train: 1.0000, Val: 0.4560, Test: 0.4540\n",
      "Epoch: 091, Loss: 0.14231112599372864, Train: 1.0000, Val: 0.4540, Test: 0.4540\n",
      "Epoch: 092, Loss: 0.140957310795784, Train: 1.0000, Val: 0.4540, Test: 0.4540\n",
      "Epoch: 093, Loss: 0.1396467089653015, Train: 1.0000, Val: 0.4560, Test: 0.4540\n",
      "Epoch: 094, Loss: 0.13829609751701355, Train: 1.0000, Val: 0.4560, Test: 0.4540\n",
      "Epoch: 095, Loss: 0.13698604702949524, Train: 1.0000, Val: 0.4580, Test: 0.4580\n",
      "Epoch: 096, Loss: 0.13573265075683594, Train: 1.0000, Val: 0.4580, Test: 0.4580\n",
      "Epoch: 097, Loss: 0.1344062089920044, Train: 1.0000, Val: 0.4580, Test: 0.4580\n",
      "Epoch: 098, Loss: 0.1331331878900528, Train: 1.0000, Val: 0.4600, Test: 0.4590\n",
      "Epoch: 099, Loss: 0.1318933218717575, Train: 1.0000, Val: 0.4600, Test: 0.4590\n",
      "Epoch: 100, Loss: 0.1306401491165161, Train: 1.0000, Val: 0.4600, Test: 0.4590\n",
      "Epoch: 101, Loss: 0.12939365208148956, Train: 1.0000, Val: 0.4600, Test: 0.4590\n",
      "Epoch: 102, Loss: 0.12818489968776703, Train: 1.0000, Val: 0.4580, Test: 0.4590\n",
      "Epoch: 103, Loss: 0.1269790083169937, Train: 1.0000, Val: 0.4580, Test: 0.4590\n",
      "Epoch: 104, Loss: 0.1258220374584198, Train: 1.0000, Val: 0.4580, Test: 0.4590\n",
      "Epoch: 105, Loss: 0.12459634989500046, Train: 1.0000, Val: 0.4600, Test: 0.4590\n",
      "Epoch: 106, Loss: 0.12343519181013107, Train: 1.0000, Val: 0.4600, Test: 0.4590\n",
      "Epoch: 107, Loss: 0.1222854033112526, Train: 1.0000, Val: 0.4640, Test: 0.4690\n",
      "Epoch: 108, Loss: 0.12114059180021286, Train: 1.0000, Val: 0.4640, Test: 0.4690\n",
      "Epoch: 109, Loss: 0.12001264095306396, Train: 1.0000, Val: 0.4640, Test: 0.4690\n",
      "Epoch: 110, Loss: 0.11888425052165985, Train: 1.0000, Val: 0.4640, Test: 0.4690\n",
      "Epoch: 111, Loss: 0.1177707388997078, Train: 1.0000, Val: 0.4640, Test: 0.4690\n",
      "Epoch: 112, Loss: 0.11669230461120605, Train: 1.0000, Val: 0.4660, Test: 0.4690\n",
      "Epoch: 113, Loss: 0.11557929217815399, Train: 1.0000, Val: 0.4660, Test: 0.4690\n",
      "Epoch: 114, Loss: 0.11450809240341187, Train: 1.0000, Val: 0.4680, Test: 0.4730\n",
      "Epoch: 115, Loss: 0.11343234032392502, Train: 1.0000, Val: 0.4680, Test: 0.4730\n",
      "Epoch: 116, Loss: 0.11237498372793198, Train: 1.0000, Val: 0.4680, Test: 0.4730\n",
      "Epoch: 117, Loss: 0.11132746189832687, Train: 1.0000, Val: 0.4680, Test: 0.4730\n",
      "Epoch: 118, Loss: 0.11029017716646194, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 119, Loss: 0.1092633530497551, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 120, Loss: 0.10825138539075851, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 121, Loss: 0.10724158585071564, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 122, Loss: 0.1062827929854393, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 123, Loss: 0.10526391118764877, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 124, Loss: 0.10429257899522781, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 125, Loss: 0.10333158820867538, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 126, Loss: 0.10238619893789291, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 127, Loss: 0.1014440655708313, Train: 1.0000, Val: 0.4700, Test: 0.4730\n",
      "Epoch: 128, Loss: 0.10051043331623077, Train: 1.0000, Val: 0.4720, Test: 0.4740\n",
      "Epoch: 129, Loss: 0.09958815574645996, Train: 1.0000, Val: 0.4700, Test: 0.4740\n",
      "Epoch: 130, Loss: 0.0986771509051323, Train: 1.0000, Val: 0.4700, Test: 0.4740\n",
      "Epoch: 131, Loss: 0.09778577089309692, Train: 1.0000, Val: 0.4720, Test: 0.4740\n",
      "Epoch: 132, Loss: 0.09688689559698105, Train: 1.0000, Val: 0.4720, Test: 0.4740\n",
      "Epoch: 133, Loss: 0.09600780159235, Train: 1.0000, Val: 0.4720, Test: 0.4740\n",
      "Epoch: 134, Loss: 0.09513811022043228, Train: 1.0000, Val: 0.4720, Test: 0.4740\n",
      "Epoch: 135, Loss: 0.09427522122859955, Train: 1.0000, Val: 0.4740, Test: 0.4790\n",
      "Epoch: 136, Loss: 0.09343291819095612, Train: 1.0000, Val: 0.4740, Test: 0.4790\n",
      "Epoch: 137, Loss: 0.09257911890745163, Train: 1.0000, Val: 0.4740, Test: 0.4790\n",
      "Epoch: 138, Loss: 0.09174956381320953, Train: 1.0000, Val: 0.4760, Test: 0.4780\n",
      "Epoch: 139, Loss: 0.09092260152101517, Train: 1.0000, Val: 0.4780, Test: 0.4780\n",
      "Epoch: 140, Loss: 0.09010829776525497, Train: 1.0000, Val: 0.4780, Test: 0.4780\n",
      "Epoch: 141, Loss: 0.08930151909589767, Train: 1.0000, Val: 0.4780, Test: 0.4780\n",
      "Epoch: 142, Loss: 0.0885126143693924, Train: 1.0000, Val: 0.4780, Test: 0.4780\n",
      "Epoch: 143, Loss: 0.08775555342435837, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 144, Loss: 0.08695603907108307, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 145, Loss: 0.08617769926786423, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 146, Loss: 0.08542050421237946, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 147, Loss: 0.08467308431863785, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 148, Loss: 0.0839301347732544, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 149, Loss: 0.08320235460996628, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 150, Loss: 0.08247361332178116, Train: 1.0000, Val: 0.4800, Test: 0.4820\n",
      "Epoch: 151, Loss: 0.081775963306427, Train: 1.0000, Val: 0.4820, Test: 0.4860\n",
      "Epoch: 152, Loss: 0.08105043321847916, Train: 1.0000, Val: 0.4820, Test: 0.4860\n",
      "Epoch: 153, Loss: 0.0803716704249382, Train: 1.0000, Val: 0.4820, Test: 0.4860\n",
      "Epoch: 154, Loss: 0.07966716587543488, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 155, Loss: 0.07898599654436111, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 156, Loss: 0.07831313461065292, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 157, Loss: 0.077647864818573, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 158, Loss: 0.07698995620012283, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 159, Loss: 0.07633873075246811, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 160, Loss: 0.07569409906864166, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 161, Loss: 0.07505714148283005, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 162, Loss: 0.07442896068096161, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 163, Loss: 0.07381285727024078, Train: 1.0000, Val: 0.4800, Test: 0.4860\n",
      "Epoch: 164, Loss: 0.07319726049900055, Train: 1.0000, Val: 0.4820, Test: 0.4860\n",
      "Epoch: 165, Loss: 0.0725901871919632, Train: 1.0000, Val: 0.4820, Test: 0.4860\n",
      "Epoch: 166, Loss: 0.07199020683765411, Train: 1.0000, Val: 0.4820, Test: 0.4860\n",
      "Epoch: 167, Loss: 0.07139699161052704, Train: 1.0000, Val: 0.4840, Test: 0.4910\n",
      "Epoch: 168, Loss: 0.07081907242536545, Train: 1.0000, Val: 0.4860, Test: 0.4910\n",
      "Epoch: 169, Loss: 0.07025282084941864, Train: 1.0000, Val: 0.4860, Test: 0.4910\n",
      "Epoch: 170, Loss: 0.06967173516750336, Train: 1.0000, Val: 0.4880, Test: 0.4930\n",
      "Epoch: 171, Loss: 0.06913875043392181, Train: 1.0000, Val: 0.4900, Test: 0.4930\n",
      "Epoch: 172, Loss: 0.06855900585651398, Train: 1.0000, Val: 0.4900, Test: 0.4930\n",
      "Epoch: 173, Loss: 0.06801123172044754, Train: 1.0000, Val: 0.4900, Test: 0.4930\n",
      "Epoch: 174, Loss: 0.06747104227542877, Train: 1.0000, Val: 0.4920, Test: 0.4930\n",
      "Epoch: 175, Loss: 0.06693660467863083, Train: 1.0000, Val: 0.4920, Test: 0.4930\n",
      "Epoch: 176, Loss: 0.0664055123925209, Train: 1.0000, Val: 0.4920, Test: 0.4930\n",
      "Epoch: 177, Loss: 0.06588000804185867, Train: 1.0000, Val: 0.4920, Test: 0.4930\n",
      "Epoch: 178, Loss: 0.06536085903644562, Train: 1.0000, Val: 0.4920, Test: 0.4930\n",
      "Epoch: 179, Loss: 0.06485243886709213, Train: 1.0000, Val: 0.4920, Test: 0.4930\n",
      "Epoch: 180, Loss: 0.06433820724487305, Train: 1.0000, Val: 0.4920, Test: 0.4930\n",
      "Epoch: 181, Loss: 0.06383396685123444, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 182, Loss: 0.06335369497537613, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 183, Loss: 0.06285460293292999, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 184, Loss: 0.062357060611248016, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 185, Loss: 0.061882633715867996, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 186, Loss: 0.06140105798840523, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 187, Loss: 0.06093060225248337, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 188, Loss: 0.06046583130955696, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 189, Loss: 0.06000427529215813, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 190, Loss: 0.05954708904027939, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 191, Loss: 0.059095751494169235, Train: 1.0000, Val: 0.4940, Test: 0.4940\n",
      "Epoch: 192, Loss: 0.05865946412086487, Train: 1.0000, Val: 0.4960, Test: 0.5000\n",
      "Epoch: 193, Loss: 0.058206576853990555, Train: 1.0000, Val: 0.4960, Test: 0.5000\n",
      "Epoch: 194, Loss: 0.05777030810713768, Train: 1.0000, Val: 0.4960, Test: 0.5000\n",
      "Epoch: 195, Loss: 0.05733880400657654, Train: 1.0000, Val: 0.4960, Test: 0.5000\n",
      "Epoch: 196, Loss: 0.056912861764431, Train: 1.0000, Val: 0.4960, Test: 0.5000\n",
      "Epoch: 197, Loss: 0.05650797113776207, Train: 1.0000, Val: 0.4940, Test: 0.5000\n",
      "Epoch: 198, Loss: 0.05609341710805893, Train: 1.0000, Val: 0.4980, Test: 0.5000\n",
      "Epoch: 199, Loss: 0.055668529123067856, Train: 1.0000, Val: 0.4980, Test: 0.5000\n",
      "Best Val Acc: 0.4980 Test Acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=50, min_lr=0.0001)\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 200):\n",
    "    loss = train(model, data, optimizer, scheduler=None, loss='cross_entropy', alpha=0.0005)\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Val Acc: 0.7260 Test Acc: 0.7140 Best l: 16 Best a: 0.99\n"
     ]
    }
   ],
   "source": [
    "# dataset = citeseer\n",
    "# dataset = cora\n",
    "dataset = pubmed\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "best_l = best_a = 0\n",
    "\n",
    "for l in [1, 2, 4, 8, 16, 32]:\n",
    "    for a in [0.05, 0.1, 0.3, 0.6, 0.8, 0.9, 0.95, 0.99, 1]:\n",
    "        model = LP(num_layers=l, alpha=a)\n",
    "        outs = model.train(dataset)\n",
    "        train_acc, val_acc, tmp_test_acc = model.test()\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "            best_l = l\n",
    "            best_a = a\n",
    "            \n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}', f'Best l: {best_l}', f'Best a: {best_a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8833333333333333, 0.712, 0.707]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = model.test()\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19717, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 2, 0, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 2, 0, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe8eaf950b0cf64c1b70de22759d9a144a2595c541b4003711edd1f96d908e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, edgeindex2adj\n",
    "from models import GCN, GAT, LP\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "pubmed = Planetoid(root='.', name='Pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 24, dataset.num_classes)\n",
    "\n",
    "dataset = cora\n",
    "model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "from models import ALP, GCN\n",
    "dataset = cora\n",
    "data = dataset[0]\n",
    "\n",
    "# take k random training nodes for each class\n",
    "k = 2\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "\n",
    "print(data.train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9669246673583984, Train: 0.5000, Val: 0.1140, Test: 0.1330\n",
      "Epoch: 002, Loss: 1.9150588512420654, Train: 0.5000, Val: 0.1420, Test: 0.1510\n",
      "Epoch: 003, Loss: 1.8717554807662964, Train: 0.5714, Val: 0.1640, Test: 0.1850\n",
      "Epoch: 004, Loss: 1.8289309740066528, Train: 0.6429, Val: 0.1760, Test: 0.1960\n",
      "Epoch: 005, Loss: 1.779698371887207, Train: 0.7143, Val: 0.1800, Test: 0.1920\n",
      "Epoch: 006, Loss: 1.7275491952896118, Train: 0.7143, Val: 0.1780, Test: 0.1920\n",
      "Epoch: 007, Loss: 1.6721351146697998, Train: 0.7857, Val: 0.1620, Test: 0.1920\n",
      "Epoch: 008, Loss: 1.6143798828125, Train: 0.7857, Val: 0.1560, Test: 0.1920\n",
      "Epoch: 009, Loss: 1.5547969341278076, Train: 0.7857, Val: 0.1520, Test: 0.1920\n",
      "Epoch: 010, Loss: 1.4935821294784546, Train: 0.7857, Val: 0.1460, Test: 0.1920\n",
      "Epoch: 011, Loss: 1.4316102266311646, Train: 0.7857, Val: 0.1440, Test: 0.1920\n",
      "Epoch: 012, Loss: 1.3698276281356812, Train: 0.7857, Val: 0.1560, Test: 0.1920\n",
      "Epoch: 013, Loss: 1.308699369430542, Train: 0.7857, Val: 0.1640, Test: 0.1920\n",
      "Epoch: 014, Loss: 1.248463749885559, Train: 0.8571, Val: 0.1860, Test: 0.1930\n",
      "Epoch: 015, Loss: 1.189827561378479, Train: 0.8571, Val: 0.2100, Test: 0.2190\n",
      "Epoch: 016, Loss: 1.1328465938568115, Train: 0.8571, Val: 0.2380, Test: 0.2330\n",
      "Epoch: 017, Loss: 1.0776841640472412, Train: 0.8571, Val: 0.2620, Test: 0.2490\n",
      "Epoch: 018, Loss: 1.0239769220352173, Train: 1.0000, Val: 0.2740, Test: 0.2660\n",
      "Epoch: 019, Loss: 0.9720551371574402, Train: 1.0000, Val: 0.2960, Test: 0.2800\n",
      "Epoch: 020, Loss: 0.9219681024551392, Train: 1.0000, Val: 0.3120, Test: 0.2880\n",
      "Epoch: 021, Loss: 0.873482346534729, Train: 1.0000, Val: 0.3240, Test: 0.3000\n",
      "Epoch: 022, Loss: 0.8270670175552368, Train: 1.0000, Val: 0.3340, Test: 0.3120\n",
      "Epoch: 023, Loss: 0.7827285528182983, Train: 1.0000, Val: 0.3380, Test: 0.3250\n",
      "Epoch: 024, Loss: 0.7401379346847534, Train: 1.0000, Val: 0.3520, Test: 0.3350\n",
      "Epoch: 025, Loss: 0.6995801329612732, Train: 1.0000, Val: 0.3620, Test: 0.3390\n",
      "Epoch: 026, Loss: 0.6606260538101196, Train: 1.0000, Val: 0.3680, Test: 0.3420\n",
      "Epoch: 027, Loss: 0.6236589550971985, Train: 1.0000, Val: 0.3820, Test: 0.3510\n",
      "Epoch: 028, Loss: 0.5883007049560547, Train: 1.0000, Val: 0.3880, Test: 0.3550\n",
      "Epoch: 029, Loss: 0.554683268070221, Train: 1.0000, Val: 0.3840, Test: 0.3550\n",
      "Epoch: 030, Loss: 0.5226261615753174, Train: 1.0000, Val: 0.3840, Test: 0.3550\n",
      "Epoch: 031, Loss: 0.4922415316104889, Train: 1.0000, Val: 0.3820, Test: 0.3550\n",
      "Epoch: 032, Loss: 0.4631657302379608, Train: 1.0000, Val: 0.3940, Test: 0.3740\n",
      "Epoch: 033, Loss: 0.435529887676239, Train: 1.0000, Val: 0.3940, Test: 0.3740\n",
      "Epoch: 034, Loss: 0.4092234969139099, Train: 1.0000, Val: 0.4000, Test: 0.3810\n",
      "Epoch: 035, Loss: 0.3841826915740967, Train: 1.0000, Val: 0.4040, Test: 0.3820\n",
      "Epoch: 036, Loss: 0.36038389801979065, Train: 1.0000, Val: 0.4020, Test: 0.3820\n",
      "Epoch: 037, Loss: 0.3377421200275421, Train: 1.0000, Val: 0.4020, Test: 0.3820\n",
      "Epoch: 038, Loss: 0.31609684228897095, Train: 1.0000, Val: 0.4020, Test: 0.3820\n",
      "Epoch: 039, Loss: 0.2955705225467682, Train: 1.0000, Val: 0.4060, Test: 0.3970\n",
      "Epoch: 040, Loss: 0.27608826756477356, Train: 1.0000, Val: 0.4040, Test: 0.3970\n",
      "Epoch: 041, Loss: 0.2576085329055786, Train: 1.0000, Val: 0.4060, Test: 0.3970\n",
      "Epoch: 042, Loss: 0.2401515692472458, Train: 1.0000, Val: 0.4040, Test: 0.3970\n",
      "Epoch: 043, Loss: 0.22369734942913055, Train: 1.0000, Val: 0.4020, Test: 0.3970\n",
      "Epoch: 044, Loss: 0.2081589549779892, Train: 1.0000, Val: 0.4000, Test: 0.3970\n",
      "Epoch: 045, Loss: 0.19360964000225067, Train: 1.0000, Val: 0.4040, Test: 0.3970\n",
      "Epoch: 046, Loss: 0.18005183339118958, Train: 1.0000, Val: 0.4060, Test: 0.3970\n",
      "Epoch: 047, Loss: 0.16738419234752655, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 048, Loss: 0.15569008886814117, Train: 1.0000, Val: 0.4060, Test: 0.3980\n",
      "Epoch: 049, Loss: 0.1448424756526947, Train: 1.0000, Val: 0.4060, Test: 0.3980\n",
      "Epoch: 050, Loss: 0.13482651114463806, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 051, Loss: 0.12560565769672394, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 052, Loss: 0.11713175475597382, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 053, Loss: 0.10940685123205185, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 054, Loss: 0.10231773555278778, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 055, Loss: 0.09583122283220291, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 056, Loss: 0.08991061896085739, Train: 1.0000, Val: 0.4080, Test: 0.3980\n",
      "Epoch: 057, Loss: 0.08451636135578156, Train: 1.0000, Val: 0.4060, Test: 0.3980\n",
      "Epoch: 058, Loss: 0.07960785925388336, Train: 1.0000, Val: 0.4060, Test: 0.3980\n",
      "Epoch: 059, Loss: 0.07512841373682022, Train: 1.0000, Val: 0.4060, Test: 0.3980\n",
      "Epoch: 060, Loss: 0.07102910429239273, Train: 1.0000, Val: 0.4060, Test: 0.3980\n",
      "Epoch: 061, Loss: 0.0672929510474205, Train: 1.0000, Val: 0.4100, Test: 0.3990\n",
      "Epoch: 062, Loss: 0.06388291716575623, Train: 1.0000, Val: 0.4100, Test: 0.3990\n",
      "Epoch: 063, Loss: 0.06075701117515564, Train: 1.0000, Val: 0.4120, Test: 0.4000\n",
      "Epoch: 064, Loss: 0.05788945034146309, Train: 1.0000, Val: 0.4120, Test: 0.4000\n",
      "Epoch: 065, Loss: 0.055258896201848984, Train: 1.0000, Val: 0.4120, Test: 0.4000\n",
      "Epoch: 066, Loss: 0.05285011604428291, Train: 1.0000, Val: 0.4120, Test: 0.4000\n",
      "Epoch: 067, Loss: 0.05062514916062355, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 068, Loss: 0.048569727689027786, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 069, Loss: 0.04666886478662491, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 070, Loss: 0.044912755489349365, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 071, Loss: 0.04328057914972305, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 072, Loss: 0.041766490787267685, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 073, Loss: 0.04036066308617592, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 074, Loss: 0.039043329656124115, Train: 1.0000, Val: 0.4080, Test: 0.4000\n",
      "Epoch: 075, Loss: 0.037817250937223434, Train: 1.0000, Val: 0.4080, Test: 0.4000\n",
      "Epoch: 076, Loss: 0.03666650131344795, Train: 1.0000, Val: 0.4080, Test: 0.4000\n",
      "Epoch: 077, Loss: 0.03559189662337303, Train: 1.0000, Val: 0.4060, Test: 0.4000\n",
      "Epoch: 078, Loss: 0.03458747640252113, Train: 1.0000, Val: 0.4060, Test: 0.4000\n",
      "Epoch: 079, Loss: 0.03364779055118561, Train: 1.0000, Val: 0.4060, Test: 0.4000\n",
      "Epoch: 080, Loss: 0.03276173770427704, Train: 1.0000, Val: 0.4040, Test: 0.4000\n",
      "Epoch: 081, Loss: 0.031930167227983475, Train: 1.0000, Val: 0.4040, Test: 0.4000\n",
      "Epoch: 082, Loss: 0.031147079542279243, Train: 1.0000, Val: 0.4080, Test: 0.4000\n",
      "Epoch: 083, Loss: 0.030411075800657272, Train: 1.0000, Val: 0.4100, Test: 0.4000\n",
      "Epoch: 084, Loss: 0.029711123555898666, Train: 1.0000, Val: 0.4120, Test: 0.4000\n",
      "Epoch: 085, Loss: 0.02905164659023285, Train: 1.0000, Val: 0.4140, Test: 0.4060\n",
      "Epoch: 086, Loss: 0.02842734381556511, Train: 1.0000, Val: 0.4140, Test: 0.4060\n",
      "Epoch: 087, Loss: 0.02783411741256714, Train: 1.0000, Val: 0.4140, Test: 0.4060\n",
      "Epoch: 088, Loss: 0.027272028848528862, Train: 1.0000, Val: 0.4140, Test: 0.4060\n",
      "Epoch: 089, Loss: 0.026739099994301796, Train: 1.0000, Val: 0.4140, Test: 0.4060\n",
      "Epoch: 090, Loss: 0.026232292875647545, Train: 1.0000, Val: 0.4160, Test: 0.4080\n",
      "Epoch: 091, Loss: 0.0257476307451725, Train: 1.0000, Val: 0.4160, Test: 0.4080\n",
      "Epoch: 092, Loss: 0.02528347261250019, Train: 1.0000, Val: 0.4160, Test: 0.4080\n",
      "Epoch: 093, Loss: 0.024843022227287292, Train: 1.0000, Val: 0.4180, Test: 0.4090\n",
      "Epoch: 094, Loss: 0.02442244626581669, Train: 1.0000, Val: 0.4180, Test: 0.4090\n",
      "Epoch: 095, Loss: 0.024018550291657448, Train: 1.0000, Val: 0.4200, Test: 0.4110\n",
      "Epoch: 096, Loss: 0.023631839081645012, Train: 1.0000, Val: 0.4200, Test: 0.4110\n",
      "Epoch: 097, Loss: 0.02326076664030552, Train: 1.0000, Val: 0.4240, Test: 0.4120\n",
      "Epoch: 098, Loss: 0.02290620468556881, Train: 1.0000, Val: 0.4240, Test: 0.4120\n",
      "Epoch: 099, Loss: 0.022564956918358803, Train: 1.0000, Val: 0.4240, Test: 0.4120\n",
      "Epoch: 100, Loss: 0.022237200289964676, Train: 1.0000, Val: 0.4240, Test: 0.4120\n",
      "Epoch: 101, Loss: 0.02191908285021782, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 102, Loss: 0.02161462977528572, Train: 1.0000, Val: 0.4220, Test: 0.4130\n",
      "Epoch: 103, Loss: 0.021321699023246765, Train: 1.0000, Val: 0.4220, Test: 0.4130\n",
      "Epoch: 104, Loss: 0.021038630977272987, Train: 1.0000, Val: 0.4220, Test: 0.4130\n",
      "Epoch: 105, Loss: 0.020765328779816628, Train: 1.0000, Val: 0.4220, Test: 0.4130\n",
      "Epoch: 106, Loss: 0.020500775426626205, Train: 1.0000, Val: 0.4220, Test: 0.4130\n",
      "Epoch: 107, Loss: 0.020245162770152092, Train: 1.0000, Val: 0.4220, Test: 0.4130\n",
      "Epoch: 108, Loss: 0.019996849820017815, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 109, Loss: 0.019757019355893135, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 110, Loss: 0.019525231793522835, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 111, Loss: 0.01930186152458191, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 112, Loss: 0.019080353900790215, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 113, Loss: 0.018867580220103264, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 114, Loss: 0.01865999400615692, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 115, Loss: 0.01845845952630043, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 116, Loss: 0.01826249435544014, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 117, Loss: 0.01807289384305477, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 118, Loss: 0.01788788102567196, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 119, Loss: 0.017707504332065582, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 120, Loss: 0.01753123290836811, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 121, Loss: 0.017359090968966484, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 122, Loss: 0.017190473154187202, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 123, Loss: 0.017026042565703392, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 124, Loss: 0.016865042969584465, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 125, Loss: 0.0167077723890543, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 126, Loss: 0.01655479520559311, Train: 1.0000, Val: 0.4240, Test: 0.4130\n",
      "Epoch: 127, Loss: 0.01640448160469532, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 128, Loss: 0.01625751331448555, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 129, Loss: 0.016114061698317528, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 130, Loss: 0.01597409136593342, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 131, Loss: 0.015836676582694054, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 132, Loss: 0.015703437849879265, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 133, Loss: 0.01557099912315607, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 134, Loss: 0.015440226532518864, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 135, Loss: 0.015313680283725262, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 136, Loss: 0.015190021134912968, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 137, Loss: 0.01506772730499506, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 138, Loss: 0.014948311261832714, Train: 1.0000, Val: 0.4260, Test: 0.4130\n",
      "Epoch: 139, Loss: 0.014831289649009705, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 140, Loss: 0.014716948382556438, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 141, Loss: 0.01460418663918972, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 142, Loss: 0.014493390917778015, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 143, Loss: 0.014384204521775246, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 144, Loss: 0.01427683886140585, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 145, Loss: 0.014172003604471684, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 146, Loss: 0.014069641008973122, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 147, Loss: 0.01396785955876112, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 148, Loss: 0.013867388479411602, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 149, Loss: 0.013769542798399925, Train: 1.0000, Val: 0.4300, Test: 0.4200\n",
      "Epoch: 150, Loss: 0.013673228211700916, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 151, Loss: 0.013577991165220737, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 152, Loss: 0.013484718278050423, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 153, Loss: 0.013393300585448742, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 154, Loss: 0.013302961364388466, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 155, Loss: 0.013213740661740303, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 156, Loss: 0.013126112520694733, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 157, Loss: 0.013040510937571526, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 158, Loss: 0.012956244871020317, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 159, Loss: 0.012872457504272461, Train: 1.0000, Val: 0.4320, Test: 0.4220\n",
      "Epoch: 160, Loss: 0.012790040113031864, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 161, Loss: 0.01270959060639143, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 162, Loss: 0.01263011246919632, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 163, Loss: 0.01255176030099392, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 164, Loss: 0.012473993003368378, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 165, Loss: 0.01239741500467062, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 166, Loss: 0.012322178110480309, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 167, Loss: 0.01224815659224987, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 168, Loss: 0.012175532057881355, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 169, Loss: 0.012104419060051441, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 170, Loss: 0.012034059502184391, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 171, Loss: 0.01196356676518917, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 172, Loss: 0.011894290335476398, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 173, Loss: 0.01182633824646473, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 174, Loss: 0.01175988931208849, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 175, Loss: 0.011694538407027721, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 176, Loss: 0.011629336513578892, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 177, Loss: 0.01156506035476923, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 178, Loss: 0.011501590721309185, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 179, Loss: 0.011438806541264057, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 180, Loss: 0.011377276852726936, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 181, Loss: 0.011317037045955658, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 182, Loss: 0.011256196536123753, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 183, Loss: 0.011197604238986969, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 184, Loss: 0.01113910786807537, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 185, Loss: 0.011080987751483917, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 186, Loss: 0.011024015955626965, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 187, Loss: 0.010968242771923542, Train: 1.0000, Val: 0.4340, Test: 0.4240\n",
      "Epoch: 188, Loss: 0.010912551544606686, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 189, Loss: 0.010857393965125084, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 190, Loss: 0.010803149081766605, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 191, Loss: 0.010749834589660168, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 192, Loss: 0.010696997866034508, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 193, Loss: 0.010644913651049137, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 194, Loss: 0.010593495331704617, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 195, Loss: 0.01054257620126009, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 196, Loss: 0.01049188431352377, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 197, Loss: 0.010441767983138561, Train: 1.0000, Val: 0.4360, Test: 0.4310\n",
      "Epoch: 198, Loss: 0.010393381118774414, Train: 1.0000, Val: 0.4420, Test: 0.4370\n",
      "Epoch: 199, Loss: 0.010344786569476128, Train: 1.0000, Val: 0.4420, Test: 0.4370\n",
      "Best Val Acc: 0.4420 Test Acc: 0.4370\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 200):\n",
    "    loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Val Acc: 0.7260 Test Acc: 0.7140 Best l: 16 Best a: 0.99\n"
     ]
    }
   ],
   "source": [
    "# dataset = citeseer\n",
    "# dataset = cora\n",
    "dataset = pubmed\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "best_l = best_a = 0\n",
    "\n",
    "for l in [1, 2, 4, 8, 16, 32]:\n",
    "    for a in [0.05, 0.1, 0.3, 0.6, 0.8, 0.9, 0.95, 0.99, 1]:\n",
    "        model = LP(num_layers=l, alpha=a)\n",
    "        outs = model.train(dataset)\n",
    "        train_acc, val_acc, tmp_test_acc = model.test()\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "            best_l = l\n",
    "            best_a = a\n",
    "            \n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}', f'Best l: {best_l}', f'Best a: {best_a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8833333333333333, 0.712, 0.707]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = model.test()\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19717, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 2, 0, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 2, 0, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe8eaf950b0cf64c1b70de22759d9a144a2595c541b4003711edd1f96d908e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, edgeindex2adj\n",
    "from models import GCN, GAT\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 24, dataset.num_classes)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GCN(dataset.num_features, 8, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "dataset = cora\n",
    "model = GAT(dataset.num_features, 8, dataset.num_classes, heads=4)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei = dataset[0].edge_index\n",
    "adj = edgeindex2adj(ei, dataset.x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.2753424644470215, Train: 0.2929, Val: 0.2820, Test: 0.2610\n",
      "Epoch: 002, Loss: 2.1583802700042725, Train: 0.4357, Val: 0.4080, Test: 0.3750\n",
      "Epoch: 003, Loss: 2.113858461380005, Train: 0.6214, Val: 0.5180, Test: 0.4850\n",
      "Epoch: 004, Loss: 2.0122859477996826, Train: 0.7286, Val: 0.5840, Test: 0.5750\n",
      "Epoch: 005, Loss: 1.9730573892593384, Train: 0.7714, Val: 0.6380, Test: 0.6240\n",
      "Epoch: 006, Loss: 1.8953174352645874, Train: 0.8571, Val: 0.7040, Test: 0.6740\n",
      "Epoch: 007, Loss: 1.9403340816497803, Train: 0.9286, Val: 0.7520, Test: 0.7370\n",
      "Epoch: 008, Loss: 1.8624531030654907, Train: 0.9429, Val: 0.7720, Test: 0.7740\n",
      "Epoch: 009, Loss: 1.8072384595870972, Train: 0.9571, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 010, Loss: 1.784002423286438, Train: 0.9500, Val: 0.7860, Test: 0.7900\n",
      "Epoch: 011, Loss: 1.771308422088623, Train: 0.9571, Val: 0.7900, Test: 0.7990\n",
      "Epoch: 012, Loss: 1.7809185981750488, Train: 0.9643, Val: 0.7960, Test: 0.7960\n",
      "Epoch: 013, Loss: 1.7247285842895508, Train: 0.9643, Val: 0.7960, Test: 0.7960\n",
      "Epoch: 014, Loss: 1.7469886541366577, Train: 0.9643, Val: 0.7960, Test: 0.7960\n",
      "Epoch: 015, Loss: 1.6834102869033813, Train: 0.9643, Val: 0.7960, Test: 0.7960\n",
      "Epoch: 016, Loss: 1.692063808441162, Train: 0.9714, Val: 0.7940, Test: 0.7960\n",
      "Epoch: 017, Loss: 1.700770616531372, Train: 0.9714, Val: 0.7940, Test: 0.7960\n",
      "Epoch: 018, Loss: 1.6843481063842773, Train: 0.9714, Val: 0.7880, Test: 0.7960\n",
      "Epoch: 019, Loss: 1.5781370401382446, Train: 0.9714, Val: 0.7840, Test: 0.7960\n",
      "Epoch: 020, Loss: 1.7451236248016357, Train: 0.9714, Val: 0.7840, Test: 0.7960\n",
      "Epoch: 021, Loss: 1.6835709810256958, Train: 0.9714, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 022, Loss: 1.6381028890609741, Train: 0.9786, Val: 0.7780, Test: 0.7960\n",
      "Epoch: 023, Loss: 1.6462401151657104, Train: 0.9857, Val: 0.7820, Test: 0.7960\n",
      "Epoch: 024, Loss: 1.680733561515808, Train: 0.9857, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 025, Loss: 1.695462703704834, Train: 0.9857, Val: 0.7740, Test: 0.7960\n",
      "Epoch: 026, Loss: 1.616674542427063, Train: 0.9857, Val: 0.7700, Test: 0.7960\n",
      "Epoch: 027, Loss: 1.6204034090042114, Train: 0.9857, Val: 0.7680, Test: 0.7960\n",
      "Epoch: 028, Loss: 1.5859627723693848, Train: 0.9929, Val: 0.7740, Test: 0.7960\n",
      "Epoch: 029, Loss: 1.6348187923431396, Train: 0.9929, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 030, Loss: 1.65940260887146, Train: 0.9929, Val: 0.7780, Test: 0.7960\n",
      "Epoch: 031, Loss: 1.5821069478988647, Train: 0.9929, Val: 0.7680, Test: 0.7960\n",
      "Epoch: 032, Loss: 1.634639024734497, Train: 0.9929, Val: 0.7640, Test: 0.7960\n",
      "Epoch: 033, Loss: 1.6207401752471924, Train: 0.9929, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 034, Loss: 1.57353675365448, Train: 0.9929, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 035, Loss: 1.5565859079360962, Train: 0.9929, Val: 0.7600, Test: 0.7960\n",
      "Epoch: 036, Loss: 1.6296155452728271, Train: 0.9929, Val: 0.7620, Test: 0.7960\n",
      "Epoch: 037, Loss: 1.5519267320632935, Train: 0.9857, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 038, Loss: 1.5291693210601807, Train: 0.9857, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 039, Loss: 1.570326805114746, Train: 0.9857, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 040, Loss: 1.578802227973938, Train: 0.9857, Val: 0.7520, Test: 0.7960\n",
      "Epoch: 041, Loss: 1.4761260747909546, Train: 0.9857, Val: 0.7520, Test: 0.7960\n",
      "Epoch: 042, Loss: 1.5059800148010254, Train: 0.9857, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 043, Loss: 1.5679478645324707, Train: 0.9857, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 044, Loss: 1.508223533630371, Train: 0.9857, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 045, Loss: 1.5427385568618774, Train: 0.9857, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 046, Loss: 1.4702033996582031, Train: 0.9929, Val: 0.7600, Test: 0.7960\n",
      "Epoch: 047, Loss: 1.515804409980774, Train: 0.9929, Val: 0.7600, Test: 0.7960\n",
      "Epoch: 048, Loss: 1.5427966117858887, Train: 0.9929, Val: 0.7620, Test: 0.7960\n",
      "Epoch: 049, Loss: 1.5381132364273071, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 050, Loss: 1.4987305402755737, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 051, Loss: 1.4872286319732666, Train: 0.9929, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 052, Loss: 1.5571929216384888, Train: 0.9929, Val: 0.7620, Test: 0.7960\n",
      "Epoch: 053, Loss: 1.4905664920806885, Train: 0.9929, Val: 0.7600, Test: 0.7960\n",
      "Epoch: 054, Loss: 1.4132527112960815, Train: 0.9929, Val: 0.7540, Test: 0.7960\n",
      "Epoch: 055, Loss: 1.504357099533081, Train: 0.9929, Val: 0.7540, Test: 0.7960\n",
      "Epoch: 056, Loss: 1.5320062637329102, Train: 0.9929, Val: 0.7480, Test: 0.7960\n",
      "Epoch: 057, Loss: 1.562220573425293, Train: 0.9929, Val: 0.7460, Test: 0.7960\n",
      "Epoch: 058, Loss: 1.4602468013763428, Train: 0.9929, Val: 0.7500, Test: 0.7960\n",
      "Epoch: 059, Loss: 1.5042266845703125, Train: 0.9857, Val: 0.7480, Test: 0.7960\n",
      "Epoch: 060, Loss: 1.501856803894043, Train: 0.9857, Val: 0.7500, Test: 0.7960\n",
      "Epoch: 061, Loss: 1.4939899444580078, Train: 0.9857, Val: 0.7520, Test: 0.7960\n",
      "Epoch: 062, Loss: 1.4370068311691284, Train: 0.9857, Val: 0.7540, Test: 0.7960\n",
      "Epoch: 063, Loss: 1.4922356605529785, Train: 0.9929, Val: 0.7500, Test: 0.7960\n",
      "Epoch: 064, Loss: 1.559688925743103, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 065, Loss: 1.4850506782531738, Train: 0.9929, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 066, Loss: 1.546238660812378, Train: 0.9929, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 067, Loss: 1.452063798904419, Train: 0.9929, Val: 0.7620, Test: 0.7960\n",
      "Epoch: 068, Loss: 1.531015157699585, Train: 0.9929, Val: 0.7620, Test: 0.7960\n",
      "Epoch: 069, Loss: 1.347294569015503, Train: 0.9929, Val: 0.7660, Test: 0.7960\n",
      "Epoch: 070, Loss: 1.5207405090332031, Train: 0.9929, Val: 0.7660, Test: 0.7960\n",
      "Epoch: 071, Loss: 1.4172993898391724, Train: 0.9929, Val: 0.7600, Test: 0.7960\n",
      "Epoch: 072, Loss: 1.4136862754821777, Train: 0.9929, Val: 0.7600, Test: 0.7960\n",
      "Epoch: 073, Loss: 1.52628755569458, Train: 0.9929, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 074, Loss: 1.4477145671844482, Train: 0.9929, Val: 0.7540, Test: 0.7960\n",
      "Epoch: 075, Loss: 1.3943477869033813, Train: 0.9929, Val: 0.7500, Test: 0.7960\n",
      "Epoch: 076, Loss: 1.4475677013397217, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 077, Loss: 1.5011119842529297, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 078, Loss: 1.4557693004608154, Train: 0.9929, Val: 0.7580, Test: 0.7960\n",
      "Epoch: 079, Loss: 1.4964337348937988, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 080, Loss: 1.4428589344024658, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 081, Loss: 1.56109619140625, Train: 0.9929, Val: 0.7560, Test: 0.7960\n",
      "Epoch: 082, Loss: 1.4049369096755981, Train: 0.9929, Val: 0.7460, Test: 0.7960\n",
      "Epoch: 083, Loss: 1.4761629104614258, Train: 0.9929, Val: 0.7440, Test: 0.7960\n",
      "Epoch: 084, Loss: 1.4350638389587402, Train: 0.9929, Val: 0.7440, Test: 0.7960\n",
      "Epoch: 085, Loss: 1.4828131198883057, Train: 0.9929, Val: 0.7420, Test: 0.7960\n",
      "Epoch: 086, Loss: 1.4984692335128784, Train: 0.9929, Val: 0.7480, Test: 0.7960\n",
      "Epoch: 087, Loss: 1.423663854598999, Train: 0.9929, Val: 0.7500, Test: 0.7960\n",
      "Epoch: 088, Loss: 1.4566707611083984, Train: 0.9929, Val: 0.7500, Test: 0.7960\n",
      "Epoch: 089, Loss: 1.4471689462661743, Train: 0.9929, Val: 0.7480, Test: 0.7960\n",
      "Epoch: 090, Loss: 1.4467148780822754, Train: 0.9929, Val: 0.7540, Test: 0.7960\n",
      "Epoch: 091, Loss: 1.417032241821289, Train: 0.9929, Val: 0.7500, Test: 0.7960\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, 1000):\n",
    "    loss = train(model, data, optimizer, loss='smooth_label')\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe8eaf950b0cf64c1b70de22759d9a144a2595c541b4003711edd1f96d908e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, non_smooth_label_metric\n",
    "from models import GCN, GAT, LP\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "pubmed = Planetoid(root='.', name='Pubmed')\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "seeds = [1234, 42, 2021]\n",
    "lr = 0.02\n",
    "epochs = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT/GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "dataset = cora\n",
    "model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "data = dataset[0]\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AdaptiveLP\n",
    "lp = AdaptiveLP(num_layers=8, yshape=dataset[0].y.shape[0], edge_dim=dataset.edge_index.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FOR SEED = 1234\n",
      "Epoch: 025, Loss: 4.796726703643799, Train: 1.0000, Val: 0.4660, Test: 0.4930\n",
      "Epoch: 050, Loss: 4.758389949798584, Train: 1.0000, Val: 0.5060, Test: 0.4930\n",
      "Epoch: 075, Loss: 4.340081214904785, Train: 1.0000, Val: 0.4720, Test: 0.4930\n",
      "Epoch: 100, Loss: 4.239111423492432, Train: 1.0000, Val: 0.5060, Test: 0.4930\n",
      "Epoch: 125, Loss: 4.619776725769043, Train: 1.0000, Val: 0.5100, Test: 0.5220\n",
      "Epoch: 150, Loss: 4.316086292266846, Train: 1.0000, Val: 0.5280, Test: 0.5360\n",
      "Epoch: 175, Loss: 4.033543109893799, Train: 1.0000, Val: 0.5420, Test: 0.5550\n",
      "Epoch: 025, Loss: 0.27085956931114197, Train: 1.0000, Val: 0.5620, Test: 0.5670\n",
      "Epoch: 050, Loss: 0.28032025694847107, Train: 1.0000, Val: 0.5520, Test: 0.5670\n",
      "Epoch: 075, Loss: 0.2713465392589569, Train: 1.0000, Val: 0.5580, Test: 0.5670\n",
      "Epoch: 100, Loss: 0.27072054147720337, Train: 1.0000, Val: 0.5520, Test: 0.5670\n",
      "Epoch: 125, Loss: 0.2689717411994934, Train: 1.0000, Val: 0.5500, Test: 0.5670\n",
      "Epoch: 150, Loss: 0.267770379781723, Train: 1.0000, Val: 0.5540, Test: 0.5670\n",
      "Epoch: 175, Loss: 0.2685365378856659, Train: 1.0000, Val: 0.5520, Test: 0.5670\n",
      "Best Val Acc: 0.5820 Test Acc: 0.5670\n",
      "RUNNING FOR SEED = 42\n",
      "Epoch: 025, Loss: 4.109287738800049, Train: 1.0000, Val: 0.5900, Test: 0.5890\n",
      "Epoch: 050, Loss: 4.224493503570557, Train: 1.0000, Val: 0.5540, Test: 0.5890\n",
      "Epoch: 075, Loss: 4.276562690734863, Train: 1.0000, Val: 0.5600, Test: 0.5890\n",
      "Epoch: 100, Loss: 3.735373020172119, Train: 1.0000, Val: 0.5680, Test: 0.5890\n",
      "Epoch: 125, Loss: 3.9048242568969727, Train: 1.0000, Val: 0.5660, Test: 0.5890\n",
      "Epoch: 150, Loss: 4.1099162101745605, Train: 1.0000, Val: 0.5640, Test: 0.5890\n",
      "Epoch: 175, Loss: 4.526960372924805, Train: 1.0000, Val: 0.5660, Test: 0.5890\n",
      "Epoch: 025, Loss: 0.22829222679138184, Train: 1.0000, Val: 0.5580, Test: 0.5890\n",
      "Epoch: 050, Loss: 0.27232417464256287, Train: 1.0000, Val: 0.5560, Test: 0.5890\n",
      "Epoch: 075, Loss: 0.265860915184021, Train: 1.0000, Val: 0.5500, Test: 0.5890\n",
      "Epoch: 100, Loss: 0.26582765579223633, Train: 1.0000, Val: 0.5580, Test: 0.5890\n",
      "Epoch: 125, Loss: 0.2654077112674713, Train: 1.0000, Val: 0.5520, Test: 0.5890\n",
      "Epoch: 150, Loss: 0.2648397982120514, Train: 1.0000, Val: 0.5480, Test: 0.5890\n",
      "Epoch: 175, Loss: 0.2642812728881836, Train: 1.0000, Val: 0.5500, Test: 0.5890\n",
      "Best Val Acc: 0.5940 Test Acc: 0.5890\n",
      "RUNNING FOR SEED = 2021\n",
      "Epoch: 025, Loss: 4.236114501953125, Train: 1.0000, Val: 0.5600, Test: 0.5510\n",
      "Epoch: 050, Loss: 3.990511417388916, Train: 1.0000, Val: 0.5500, Test: 0.5510\n",
      "Epoch: 075, Loss: 4.069301128387451, Train: 1.0000, Val: 0.5640, Test: 0.5670\n",
      "Epoch: 100, Loss: 3.9111411571502686, Train: 1.0000, Val: 0.5640, Test: 0.5670\n",
      "Epoch: 125, Loss: 3.84769606590271, Train: 1.0000, Val: 0.5620, Test: 0.5670\n",
      "Epoch: 150, Loss: 3.773128032684326, Train: 1.0000, Val: 0.5600, Test: 0.5670\n",
      "Epoch: 175, Loss: 3.8090360164642334, Train: 1.0000, Val: 0.5620, Test: 0.5670\n",
      "Epoch: 025, Loss: 0.21704770624637604, Train: 1.0000, Val: 0.5480, Test: 0.5670\n",
      "Epoch: 050, Loss: 0.26209622621536255, Train: 1.0000, Val: 0.5500, Test: 0.5670\n",
      "Epoch: 075, Loss: 0.2640378177165985, Train: 1.0000, Val: 0.5500, Test: 0.5670\n",
      "Epoch: 100, Loss: 0.2633231282234192, Train: 1.0000, Val: 0.5520, Test: 0.5670\n",
      "Epoch: 125, Loss: 0.2631146013736725, Train: 1.0000, Val: 0.5540, Test: 0.5670\n",
      "Epoch: 150, Loss: 0.26247286796569824, Train: 1.0000, Val: 0.5500, Test: 0.5670\n",
      "Epoch: 175, Loss: 0.2620963454246521, Train: 1.0000, Val: 0.5520, Test: 0.5670\n",
      "Best Val Acc: 0.5660 Test Acc: 0.5670\n",
      "Average Val Acc / Average Test Acc: 0.5807 / 0.5743\n"
     ]
    }
   ],
   "source": [
    "av_val_acc = av_test_acc = 0\n",
    "state_dict_model = model.state_dict().copy()\n",
    "state_dict_lp = lp.state_dict().copy()\n",
    "\n",
    "for seed in seeds:\n",
    "    print(\"RUNNING FOR SEED =\", seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model.load_state_dict(state_dict_model)\n",
    "    lp.load_state_dict(state_dict_lp)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(lp.parameters()), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "    best_val_acc = final_test_acc = 0\n",
    "    for epoch in range(1, 200):\n",
    "        model.train()\n",
    "        lp.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_model = model(data.x, data.edge_index)\n",
    "        out_lp = lp(data)\n",
    "        \n",
    "        loss_model = F.cross_entropy(out_model[data.train_mask], data.y[data.train_mask])\n",
    "        # loss_lp = (out_lp[data.train_mask] - data.y[data.train_mask]).pow(2).mean()\n",
    "        loss_lp = F.cross_entropy(out_lp[data.train_mask], data.y[data.train_mask])\n",
    "        ##########################################\n",
    "        # sample some nodes from the unlabelled set\n",
    "        unlab_mask = ~data.train_mask & ~data.val_mask & ~data.test_mask\n",
    "        unlab_idx = unlab_mask.nonzero(as_tuple=False).view(-1)\n",
    "        sample_unlab_idx = unlab_idx[torch.rand(unlab_idx.shape[0]) < 0.005]\n",
    "        sample_unlab_mask = torch.zeros(unlab_mask.shape[0], dtype=torch.bool)\n",
    "        sample_unlab_mask[sample_unlab_idx] = True\n",
    "        \n",
    "        # loss_unsup = F.cross_entropy(out_model[sample_unlab_mask], out_lp[sample_unlab_mask].argmax(dim=1))\n",
    "        loss_unsup = F.cross_entropy(out_lp[sample_unlab_mask], out_model[sample_unlab_mask].argmax(dim=1))\n",
    "        ##########################################\n",
    "        # print(loss_model, loss_lp, loss_unsup)\n",
    "        loss = loss_model + 2 * loss_lp + loss_unsup\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-2)\n",
    "    for epoch in range(1, 200):\n",
    "        loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    \n",
    "    print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n",
    "    av_val_acc += best_val_acc\n",
    "    av_test_acc += test_acc\n",
    "    \n",
    "print(f'Average Val Acc / Average Test Acc: {av_val_acc / len(seeds):.4f} / {av_test_acc / len(seeds):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.154"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "preds = model(data.x, data.edge_index).argmax(dim=1)\n",
    "non_smooth_label_metric(dataset, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 0.5390069484710693, Train: 0.9667, Val: 0.7080, Test: 0.7150\n",
      "Epoch: 050, Loss: 0.47215163707733154, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 075, Loss: 0.47753751277923584, Train: 0.9667, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 100, Loss: 0.4743448793888092, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 125, Loss: 0.47406837344169617, Train: 0.9667, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 150, Loss: 0.47376105189323425, Train: 0.9750, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 175, Loss: 0.47430703043937683, Train: 0.9667, Val: 0.7180, Test: 0.7150\n",
      "Epoch: 200, Loss: 0.4737687408924103, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 225, Loss: 0.4732465445995331, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 250, Loss: 0.47335386276245117, Train: 0.9667, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 275, Loss: 0.47310057282447815, Train: 0.9667, Val: 0.7240, Test: 0.7150\n",
      "Best Val Acc: 0.7360 Test Acc: 0.7150\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-2)\n",
    "for epoch in range(1, 300):\n",
    "    loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    if epoch % 25 == 0:\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3327]), tensor(10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlab_mask = ~data.train_mask & ~data.val_mask & ~data.test_mask\n",
    "unlab_idx = unlab_mask.nonzero(as_tuple=False).view(-1)\n",
    "sample_unlab_idx = unlab_idx[torch.rand(unlab_idx.shape[0]) < 0.005]\n",
    "sample_unlab_mask = torch.zeros(unlab_mask.shape[0], dtype=torch.bool)\n",
    "sample_unlab_mask[sample_unlab_idx] = True\n",
    "sample_unlab_mask.shape, sample_unlab_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.154"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

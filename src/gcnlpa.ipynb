{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test, non_smooth_label_metric\n",
    "from models import GCN, GAT, LP\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "pubmed = Planetoid(root='.', name='Pubmed')\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "seeds = [1234, 42, 2021]\n",
    "lr = 0.02\n",
    "epochs = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT/GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "dataset = cora\n",
    "model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "data = dataset[0]\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AdaptiveLP\n",
    "lp = AdaptiveLP(num_layers=8, yshape=dataset[0].y.shape[0], edge_dim=dataset.edge_index.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FOR SEED = 1234\n",
      "Epoch: 025, Loss: 5.009136199951172, Train: 1.0000, Val: 0.6320, Test: 0.6640\n",
      "Epoch: 050, Loss: 4.9541850090026855, Train: 1.0000, Val: 0.6560, Test: 0.6810\n",
      "Epoch: 075, Loss: 4.621029853820801, Train: 1.0000, Val: 0.6700, Test: 0.6920\n",
      "Epoch: 100, Loss: 4.780718803405762, Train: 1.0000, Val: 0.6960, Test: 0.7080\n",
      "Epoch: 125, Loss: 4.4924821853637695, Train: 1.0000, Val: 0.7000, Test: 0.7160\n",
      "Epoch: 150, Loss: 4.3006110191345215, Train: 1.0000, Val: 0.6960, Test: 0.7160\n",
      "Epoch: 175, Loss: 4.320293426513672, Train: 1.0000, Val: 0.6940, Test: 0.7160\n",
      "Epoch: 025, Loss: 0.34270328283309937, Train: 1.0000, Val: 0.6860, Test: 0.7160\n",
      "Epoch: 050, Loss: 0.4366002380847931, Train: 1.0000, Val: 0.6900, Test: 0.7160\n",
      "Epoch: 075, Loss: 0.44261693954467773, Train: 1.0000, Val: 0.6840, Test: 0.7160\n",
      "Epoch: 100, Loss: 0.4389713704586029, Train: 1.0000, Val: 0.6840, Test: 0.7160\n",
      "Epoch: 125, Loss: 0.4399879574775696, Train: 1.0000, Val: 0.6880, Test: 0.7160\n",
      "Epoch: 150, Loss: 0.4395817220211029, Train: 1.0000, Val: 0.6840, Test: 0.7160\n",
      "Epoch: 175, Loss: 0.4386403262615204, Train: 1.0000, Val: 0.6880, Test: 0.7160\n",
      "Best Val Acc: 0.7000 Test Acc: 0.7160\n",
      "RUNNING FOR SEED = 42\n",
      "Epoch: 025, Loss: 4.398225784301758, Train: 1.0000, Val: 0.6660, Test: 0.6850\n",
      "Epoch: 050, Loss: 4.404765605926514, Train: 1.0000, Val: 0.6900, Test: 0.7080\n",
      "Epoch: 075, Loss: 3.961815357208252, Train: 1.0000, Val: 0.6920, Test: 0.7080\n",
      "Epoch: 100, Loss: 4.023499965667725, Train: 1.0000, Val: 0.6900, Test: 0.7080\n",
      "Epoch: 125, Loss: 3.901463508605957, Train: 1.0000, Val: 0.6900, Test: 0.7080\n",
      "Epoch: 150, Loss: 4.415735721588135, Train: 1.0000, Val: 0.6900, Test: 0.7080\n",
      "Epoch: 175, Loss: 4.037553787231445, Train: 1.0000, Val: 0.6900, Test: 0.7080\n",
      "Epoch: 025, Loss: 0.3112844228744507, Train: 1.0000, Val: 0.6840, Test: 0.7080\n",
      "Epoch: 050, Loss: 0.42976444959640503, Train: 1.0000, Val: 0.6900, Test: 0.7070\n",
      "Epoch: 075, Loss: 0.4412156343460083, Train: 1.0000, Val: 0.6840, Test: 0.7070\n",
      "Epoch: 100, Loss: 0.4363933205604553, Train: 1.0000, Val: 0.6820, Test: 0.7070\n",
      "Epoch: 125, Loss: 0.4372873306274414, Train: 1.0000, Val: 0.6880, Test: 0.7070\n",
      "Epoch: 150, Loss: 0.4374658167362213, Train: 1.0000, Val: 0.6840, Test: 0.7070\n",
      "Epoch: 175, Loss: 0.4374444782733917, Train: 1.0000, Val: 0.6820, Test: 0.7070\n",
      "Best Val Acc: 0.6980 Test Acc: 0.7070\n",
      "RUNNING FOR SEED = 2021\n",
      "Epoch: 025, Loss: 4.074943542480469, Train: 1.0000, Val: 0.6680, Test: 0.6740\n",
      "Epoch: 050, Loss: 4.205110549926758, Train: 1.0000, Val: 0.6920, Test: 0.7030\n",
      "Epoch: 075, Loss: 4.954164505004883, Train: 1.0000, Val: 0.6880, Test: 0.7030\n",
      "Epoch: 100, Loss: 4.662702560424805, Train: 1.0000, Val: 0.6880, Test: 0.7030\n",
      "Epoch: 125, Loss: 3.9802088737487793, Train: 1.0000, Val: 0.6920, Test: 0.7030\n",
      "Epoch: 150, Loss: 4.066102027893066, Train: 1.0000, Val: 0.6920, Test: 0.7030\n",
      "Epoch: 175, Loss: 3.9510536193847656, Train: 1.0000, Val: 0.6920, Test: 0.7030\n",
      "Epoch: 025, Loss: 0.3066664934158325, Train: 1.0000, Val: 0.6860, Test: 0.7030\n",
      "Epoch: 050, Loss: 0.4251439869403839, Train: 1.0000, Val: 0.6900, Test: 0.7030\n",
      "Epoch: 075, Loss: 0.43842875957489014, Train: 1.0000, Val: 0.6820, Test: 0.7030\n",
      "Epoch: 100, Loss: 0.4355055093765259, Train: 1.0000, Val: 0.6800, Test: 0.7030\n",
      "Epoch: 125, Loss: 0.43634817004203796, Train: 1.0000, Val: 0.6820, Test: 0.7030\n",
      "Epoch: 150, Loss: 0.4361754357814789, Train: 1.0000, Val: 0.6800, Test: 0.7030\n",
      "Epoch: 175, Loss: 0.4361399710178375, Train: 1.0000, Val: 0.6820, Test: 0.7030\n",
      "Best Val Acc: 0.6940 Test Acc: 0.7030\n",
      "Average Val Acc / Average Test Acc: 0.6973 / 0.7087\n"
     ]
    }
   ],
   "source": [
    "av_val_acc = av_test_acc = 0\n",
    "state_dict_model = model.state_dict().copy()\n",
    "state_dict_lp = lp.state_dict().copy()\n",
    "\n",
    "for seed in seeds:\n",
    "    print(\"RUNNING FOR SEED =\", seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model.load_state_dict(state_dict_model)\n",
    "    lp.load_state_dict(state_dict_lp)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(lp.parameters()), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "    best_val_acc = final_test_acc = 0\n",
    "    for epoch in range(1, 200):\n",
    "        model.train()\n",
    "        lp.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_model = model(data.x, data.edge_index)\n",
    "        out_lp = lp(data)\n",
    "        \n",
    "        loss_model = F.cross_entropy(out_model[data.train_mask], data.y[data.train_mask])\n",
    "        # loss_lp = (out_lp[data.train_mask] - data.y[data.train_mask]).pow(2).mean()\n",
    "        loss_lp = F.cross_entropy(out_lp[data.train_mask], data.y[data.train_mask])\n",
    "        ##########################################\n",
    "        # sample some nodes from the unlabelled set\n",
    "        unlab_mask = ~data.train_mask & ~data.val_mask & ~data.test_mask\n",
    "        unlab_idx = unlab_mask.nonzero(as_tuple=False).view(-1)\n",
    "        sample_unlab_idx = unlab_idx[torch.rand(unlab_idx.shape[0]) < 0.005]\n",
    "        sample_unlab_mask = torch.zeros(unlab_mask.shape[0], dtype=torch.bool)\n",
    "        sample_unlab_mask[sample_unlab_idx] = True\n",
    "        \n",
    "        # loss_unsup = F.cross_entropy(out_model[sample_unlab_mask], out_lp[sample_unlab_mask].argmax(dim=1))\n",
    "        loss_unsup = F.cross_entropy(out_lp[sample_unlab_mask], out_model[sample_unlab_mask].argmax(dim=1))\n",
    "        ##########################################\n",
    "        # print(loss_model, loss_lp, loss_unsup)\n",
    "        loss = loss_model + 2 * loss_lp + loss_unsup\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-2)\n",
    "    for epoch in range(1, 200):\n",
    "        loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    \n",
    "    print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n",
    "    av_val_acc += best_val_acc\n",
    "    av_test_acc += test_acc\n",
    "    \n",
    "print(f'Average Val Acc / Average Test Acc: {av_val_acc / len(seeds):.4f} / {av_test_acc / len(seeds):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.074"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "preds = model(data.x, data.edge_index).argmax(dim=1)\n",
    "non_smooth_label_metric(dataset, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 0.5390069484710693, Train: 0.9667, Val: 0.7080, Test: 0.7150\n",
      "Epoch: 050, Loss: 0.47215163707733154, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 075, Loss: 0.47753751277923584, Train: 0.9667, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 100, Loss: 0.4743448793888092, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 125, Loss: 0.47406837344169617, Train: 0.9667, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 150, Loss: 0.47376105189323425, Train: 0.9750, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 175, Loss: 0.47430703043937683, Train: 0.9667, Val: 0.7180, Test: 0.7150\n",
      "Epoch: 200, Loss: 0.4737687408924103, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 225, Loss: 0.4732465445995331, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 250, Loss: 0.47335386276245117, Train: 0.9667, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 275, Loss: 0.47310057282447815, Train: 0.9667, Val: 0.7240, Test: 0.7150\n",
      "Best Val Acc: 0.7360 Test Acc: 0.7150\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-2)\n",
    "for epoch in range(1, 300):\n",
    "    loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    if epoch % 25 == 0:\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kousi\\Documents\\cmu\\courses\\10708\\project\\code\\10708-project\\src\\gcnlpa.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kousi/Documents/cmu/courses/10708/project/code/10708-project/src/gcnlpa.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset\u001b[39m.\u001b[39mname\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3327]), tensor(10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlab_mask = ~data.train_mask & ~data.val_mask & ~data.test_mask\n",
    "unlab_idx = unlab_mask.nonzero(as_tuple=False).view(-1)\n",
    "sample_unlab_idx = unlab_idx[torch.rand(unlab_idx.shape[0]) < 0.005]\n",
    "sample_unlab_mask = torch.zeros(unlab_mask.shape[0], dtype=torch.bool)\n",
    "sample_unlab_mask[sample_unlab_idx] = True\n",
    "sample_unlab_mask.shape, sample_unlab_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.154"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

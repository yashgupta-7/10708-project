{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils import train, test\n",
    "from models import GCN, GAT, LP\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "pubmed = Planetoid(root='.', name='Pubmed')\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "seeds = [1234, 42, 2021]\n",
    "lr = 0.02\n",
    "epochs = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT/GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "dataset = cora\n",
    "model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "data = dataset[0]\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AdaptiveLP\n",
    "lp = AdaptiveLP(num_layers=8, yshape=dataset[0].y.shape[0], edge_dim=dataset.edge_index.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FOR SEED = 1234\n",
      "Epoch: 025, Loss: 5.895421028137207, Train: 1.0000, Val: 0.7620, Test: 0.8040\n",
      "Epoch: 050, Loss: 5.607670307159424, Train: 0.9929, Val: 0.7620, Test: 0.8080\n",
      "Epoch: 075, Loss: 5.727466583251953, Train: 1.0000, Val: 0.7760, Test: 0.8080\n",
      "Epoch: 100, Loss: 5.707205772399902, Train: 1.0000, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 125, Loss: 5.623482704162598, Train: 1.0000, Val: 0.7680, Test: 0.8080\n",
      "Epoch: 150, Loss: 5.030736923217773, Train: 1.0000, Val: 0.7540, Test: 0.8080\n",
      "Epoch: 175, Loss: 5.133493423461914, Train: 1.0000, Val: 0.7720, Test: 0.8080\n",
      "Epoch: 025, Loss: 1.0260175466537476, Train: 0.9786, Val: 0.7620, Test: 0.8080\n",
      "Epoch: 050, Loss: 1.1990495920181274, Train: 0.9714, Val: 0.7780, Test: 0.8080\n",
      "Epoch: 075, Loss: 1.189237117767334, Train: 0.9643, Val: 0.8020, Test: 0.8220\n",
      "Epoch: 100, Loss: 1.1560168266296387, Train: 0.9500, Val: 0.7620, Test: 0.8220\n",
      "Epoch: 125, Loss: 1.1544034481048584, Train: 0.9714, Val: 0.7760, Test: 0.8220\n",
      "Epoch: 150, Loss: 1.1317775249481201, Train: 0.9643, Val: 0.7780, Test: 0.8220\n",
      "Epoch: 175, Loss: 1.2229185104370117, Train: 0.9571, Val: 0.7720, Test: 0.8220\n",
      "Best Val Acc: 0.8100 Test Acc: 0.8220\n",
      "RUNNING FOR SEED = 42\n",
      "Epoch: 025, Loss: 5.024656295776367, Train: 1.0000, Val: 0.7700, Test: 0.8160\n",
      "Epoch: 050, Loss: 5.278354644775391, Train: 1.0000, Val: 0.7740, Test: 0.8160\n",
      "Epoch: 075, Loss: 5.5786590576171875, Train: 1.0000, Val: 0.7780, Test: 0.8160\n",
      "Epoch: 100, Loss: 4.857641220092773, Train: 1.0000, Val: 0.7660, Test: 0.8160\n",
      "Epoch: 125, Loss: 5.0697245597839355, Train: 1.0000, Val: 0.7720, Test: 0.8160\n",
      "Epoch: 150, Loss: 5.057578086853027, Train: 1.0000, Val: 0.7580, Test: 0.8160\n",
      "Epoch: 175, Loss: 5.2619309425354, Train: 1.0000, Val: 0.7700, Test: 0.8160\n"
     ]
    }
   ],
   "source": [
    "av_val_acc = av_test_acc = 0\n",
    "state_dict_model = model.state_dict().copy()\n",
    "state_dict_lp = lp.state_dict().copy()\n",
    "\n",
    "for seed in seeds:\n",
    "    print(\"RUNNING FOR SEED =\", seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model.load_state_dict(state_dict_model)\n",
    "    lp.load_state_dict(state_dict_lp)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(lp.parameters()), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "    best_val_acc = final_test_acc = 0\n",
    "    for epoch in range(1, 200):\n",
    "        model.train()\n",
    "        lp.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_model = model(data.x, data.edge_index)\n",
    "        out_lp = lp(data)\n",
    "        \n",
    "        loss_model = F.cross_entropy(out_model[data.train_mask], data.y[data.train_mask])\n",
    "        # loss_lp = (out_lp[data.train_mask] - data.y[data.train_mask]).pow(2).mean()\n",
    "        loss_lp = F.cross_entropy(out_lp[data.train_mask], data.y[data.train_mask])\n",
    "        ##########################################\n",
    "        # sample some nodes from the unlabelled set\n",
    "        unlab_mask = ~data.train_mask & ~data.val_mask & ~data.test_mask\n",
    "        unlab_idx = unlab_mask.nonzero(as_tuple=False).view(-1)\n",
    "        sample_unlab_idx = unlab_idx[torch.rand(unlab_idx.shape[0]) < 0.005]\n",
    "        sample_unlab_mask = torch.zeros(unlab_mask.shape[0], dtype=torch.bool)\n",
    "        sample_unlab_mask[sample_unlab_idx] = True\n",
    "        \n",
    "        # loss_unsup = F.cross_entropy(out_model[sample_unlab_mask], out_lp[sample_unlab_mask].argmax(dim=1))\n",
    "        loss_unsup = F.cross_entropy(out_lp[sample_unlab_mask], out_model[sample_unlab_mask].argmax(dim=1))\n",
    "        ##########################################\n",
    "        # print(loss_model, loss_lp, loss_unsup)\n",
    "        loss = loss_model + 2 * loss_lp + loss_unsup\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-2)\n",
    "    for epoch in range(1, 200):\n",
    "        loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        if epoch % 25 == 0:\n",
    "            log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    \n",
    "    print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n",
    "    av_val_acc += best_val_acc\n",
    "    av_test_acc += test_acc\n",
    "    \n",
    "print(f'Average Val Acc / Average Test Acc: {av_val_acc / len(seeds):.4f} / {av_test_acc / len(seeds):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 0.5390069484710693, Train: 0.9667, Val: 0.7080, Test: 0.7150\n",
      "Epoch: 050, Loss: 0.47215163707733154, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 075, Loss: 0.47753751277923584, Train: 0.9667, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 100, Loss: 0.4743448793888092, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 125, Loss: 0.47406837344169617, Train: 0.9667, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 150, Loss: 0.47376105189323425, Train: 0.9750, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 175, Loss: 0.47430703043937683, Train: 0.9667, Val: 0.7180, Test: 0.7150\n",
      "Epoch: 200, Loss: 0.4737687408924103, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 225, Loss: 0.4732465445995331, Train: 0.9667, Val: 0.7200, Test: 0.7150\n",
      "Epoch: 250, Loss: 0.47335386276245117, Train: 0.9667, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 275, Loss: 0.47310057282447815, Train: 0.9667, Val: 0.7240, Test: 0.7150\n",
      "Best Val Acc: 0.7360 Test Acc: 0.7150\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-2)\n",
    "for epoch in range(1, 300):\n",
    "    loss = train(model, data, optimizer, loss='cross_entropy')\n",
    "    train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    if epoch % 25 == 0:\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "\n",
    "print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3327]), tensor(10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlab_mask = ~data.train_mask & ~data.val_mask & ~data.test_mask\n",
    "unlab_idx = unlab_mask.nonzero(as_tuple=False).view(-1)\n",
    "sample_unlab_idx = unlab_idx[torch.rand(unlab_idx.shape[0]) < 0.005]\n",
    "sample_unlab_mask = torch.zeros(unlab_mask.shape[0], dtype=torch.bool)\n",
    "sample_unlab_mask[sample_unlab_idx] = True\n",
    "sample_unlab_mask.shape, sample_unlab_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from utils_LS import train, test, edgeindex2adj, adj2edgeindex, laplacian, non_smooth_label_metric\n",
    "from models import GCN, GAT, LP\n",
    "citeseer = Planetoid(root='.', name='Citeseer')\n",
    "cora = Planetoid(root='.', name='Cora')\n",
    "pubmed = Planetoid(root='.', name='Pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = citeseer\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "dataset = cora\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "# \n",
    "# dataset = citeseer\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "# dataset = cora\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "# dataset = pubmed\n",
    "# model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(140), tensor(500), tensor(1000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_mask.sum(), data.val_mask.sum(), data.test_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35)\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "for c in data.y.unique():\n",
    "    idx = ((data.y == c) & data.train_mask).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "    idx = idx[k:]\n",
    "    data.train_mask[idx] = False\n",
    "print(data.train_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = edgeindex2adj(data.edge_index, data.x.shape[0])\n",
    "lap = laplacian(adj, typ='normalized')\n",
    "lap.isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9686776399612427, Train: 0.5143, Val: 0.2860, Test: 0.3030\n",
      "Epoch: 002, Loss: 1.8849784135818481, Train: 0.7143, Val: 0.3680, Test: 0.3740\n",
      "Epoch: 003, Loss: 1.7385286092758179, Train: 0.7429, Val: 0.4080, Test: 0.4030\n",
      "Epoch: 004, Loss: 1.5868669748306274, Train: 0.8857, Val: 0.4420, Test: 0.4270\n",
      "Epoch: 005, Loss: 1.4758074283599854, Train: 0.9714, Val: 0.4780, Test: 0.4720\n",
      "Epoch: 006, Loss: 1.2254014015197754, Train: 1.0000, Val: 0.5180, Test: 0.5020\n",
      "Epoch: 007, Loss: 1.129117727279663, Train: 1.0000, Val: 0.5640, Test: 0.5490\n",
      "Epoch: 008, Loss: 1.0419087409973145, Train: 1.0000, Val: 0.6140, Test: 0.6020\n",
      "Epoch: 009, Loss: 1.0858254432678223, Train: 1.0000, Val: 0.6420, Test: 0.6390\n",
      "Epoch: 010, Loss: 0.8004163503646851, Train: 1.0000, Val: 0.6640, Test: 0.6630\n",
      "Epoch: 011, Loss: 0.6679726243019104, Train: 1.0000, Val: 0.6680, Test: 0.6770\n",
      "Epoch: 012, Loss: 0.6991634964942932, Train: 1.0000, Val: 0.6760, Test: 0.6900\n",
      "Epoch: 013, Loss: 0.9292731881141663, Train: 1.0000, Val: 0.6820, Test: 0.7000\n",
      "Epoch: 014, Loss: 0.9430466890335083, Train: 1.0000, Val: 0.6860, Test: 0.7070\n",
      "Epoch: 015, Loss: 0.6969120502471924, Train: 1.0000, Val: 0.6840, Test: 0.7070\n",
      "Epoch: 016, Loss: 0.7276644110679626, Train: 1.0000, Val: 0.6820, Test: 0.7070\n",
      "Epoch: 017, Loss: 0.5914122462272644, Train: 1.0000, Val: 0.6860, Test: 0.7070\n",
      "Epoch: 018, Loss: 0.6461918354034424, Train: 1.0000, Val: 0.6840, Test: 0.7070\n",
      "Epoch: 019, Loss: 0.611427366733551, Train: 1.0000, Val: 0.6900, Test: 0.7170\n",
      "Epoch: 020, Loss: 0.6248959302902222, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 021, Loss: 0.4621886610984802, Train: 1.0000, Val: 0.6940, Test: 0.7110\n",
      "Epoch: 022, Loss: 0.7763398885726929, Train: 1.0000, Val: 0.6900, Test: 0.7110\n",
      "Epoch: 023, Loss: 0.6736725568771362, Train: 1.0000, Val: 0.7000, Test: 0.7100\n",
      "Epoch: 024, Loss: 0.3529073894023895, Train: 1.0000, Val: 0.7040, Test: 0.7100\n",
      "Epoch: 025, Loss: 0.5296041965484619, Train: 1.0000, Val: 0.7000, Test: 0.7100\n",
      "Epoch: 026, Loss: 0.5114790201187134, Train: 1.0000, Val: 0.7020, Test: 0.7100\n",
      "Epoch: 027, Loss: 0.5590871572494507, Train: 1.0000, Val: 0.6980, Test: 0.7100\n",
      "Epoch: 028, Loss: 0.3865196704864502, Train: 1.0000, Val: 0.6980, Test: 0.7100\n",
      "Epoch: 029, Loss: 0.37510034441947937, Train: 1.0000, Val: 0.7080, Test: 0.7160\n",
      "Epoch: 030, Loss: 0.48605525493621826, Train: 1.0000, Val: 0.7060, Test: 0.7160\n",
      "Epoch: 031, Loss: 0.3596436381340027, Train: 1.0000, Val: 0.7080, Test: 0.7160\n",
      "Epoch: 032, Loss: 0.4510670006275177, Train: 1.0000, Val: 0.7060, Test: 0.7160\n",
      "Epoch: 033, Loss: 0.6296089887619019, Train: 1.0000, Val: 0.7060, Test: 0.7160\n",
      "Epoch: 034, Loss: 0.40685296058654785, Train: 1.0000, Val: 0.7080, Test: 0.7160\n",
      "Epoch: 035, Loss: 0.32473158836364746, Train: 1.0000, Val: 0.7080, Test: 0.7160\n",
      "Epoch: 036, Loss: 0.5192236304283142, Train: 1.0000, Val: 0.7060, Test: 0.7160\n",
      "Epoch: 037, Loss: 0.32202383875846863, Train: 1.0000, Val: 0.7060, Test: 0.7160\n",
      "Epoch: 038, Loss: 0.363998144865036, Train: 1.0000, Val: 0.7000, Test: 0.7160\n",
      "Epoch: 039, Loss: 0.3639572858810425, Train: 1.0000, Val: 0.7020, Test: 0.7160\n",
      "Epoch: 040, Loss: 0.38851433992385864, Train: 1.0000, Val: 0.7060, Test: 0.7160\n",
      "Epoch: 041, Loss: 0.4174748659133911, Train: 1.0000, Val: 0.7080, Test: 0.7160\n",
      "Epoch: 042, Loss: 0.4268898069858551, Train: 1.0000, Val: 0.7120, Test: 0.7140\n",
      "Epoch: 043, Loss: 0.4603281021118164, Train: 1.0000, Val: 0.7120, Test: 0.7140\n",
      "Epoch: 044, Loss: 0.4081154465675354, Train: 1.0000, Val: 0.7140, Test: 0.7180\n",
      "Epoch: 045, Loss: 0.36843234300613403, Train: 1.0000, Val: 0.7160, Test: 0.7150\n",
      "Epoch: 046, Loss: 0.2675098776817322, Train: 1.0000, Val: 0.7140, Test: 0.7150\n",
      "Epoch: 047, Loss: 0.29513582587242126, Train: 1.0000, Val: 0.7120, Test: 0.7150\n",
      "Epoch: 048, Loss: 0.33379778265953064, Train: 1.0000, Val: 0.7120, Test: 0.7150\n",
      "Epoch: 049, Loss: 0.3274229168891907, Train: 1.0000, Val: 0.7100, Test: 0.7150\n",
      "Epoch: 050, Loss: 0.3506312668323517, Train: 1.0000, Val: 0.7120, Test: 0.7150\n",
      "Epoch: 051, Loss: 0.3544186055660248, Train: 1.0000, Val: 0.7100, Test: 0.7150\n",
      "Epoch: 052, Loss: 0.5286559462547302, Train: 1.0000, Val: 0.7060, Test: 0.7150\n",
      "Epoch: 053, Loss: 0.2894425392150879, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 054, Loss: 0.3323834538459778, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 055, Loss: 0.2740546762943268, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 056, Loss: 0.3934842646121979, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 057, Loss: 0.4378948509693146, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 058, Loss: 0.25789737701416016, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 059, Loss: 0.43950188159942627, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 060, Loss: 0.28509047627449036, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 061, Loss: 0.25697171688079834, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 062, Loss: 0.5058924555778503, Train: 1.0000, Val: 0.7000, Test: 0.7150\n",
      "Epoch: 063, Loss: 0.38223445415496826, Train: 1.0000, Val: 0.6940, Test: 0.7150\n",
      "Epoch: 064, Loss: 0.4029541313648224, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 065, Loss: 0.27648940682411194, Train: 1.0000, Val: 0.6860, Test: 0.7150\n",
      "Epoch: 066, Loss: 0.12954112887382507, Train: 1.0000, Val: 0.6840, Test: 0.7150\n",
      "Epoch: 067, Loss: 0.4756922423839569, Train: 1.0000, Val: 0.6840, Test: 0.7150\n",
      "Epoch: 068, Loss: 0.3993592858314514, Train: 1.0000, Val: 0.6860, Test: 0.7150\n",
      "Epoch: 069, Loss: 0.4441507160663605, Train: 1.0000, Val: 0.6860, Test: 0.7150\n",
      "Epoch: 070, Loss: 0.41467681527137756, Train: 1.0000, Val: 0.6860, Test: 0.7150\n",
      "Epoch: 071, Loss: 0.36277511715888977, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 072, Loss: 0.32946088910102844, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 073, Loss: 0.4995894134044647, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 074, Loss: 0.23187188804149628, Train: 1.0000, Val: 0.6760, Test: 0.7150\n",
      "Epoch: 075, Loss: 0.4189402163028717, Train: 1.0000, Val: 0.6760, Test: 0.7150\n",
      "Epoch: 076, Loss: 0.2772490680217743, Train: 1.0000, Val: 0.6760, Test: 0.7150\n",
      "Epoch: 077, Loss: 0.38659900426864624, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 078, Loss: 0.33984801173210144, Train: 1.0000, Val: 0.6740, Test: 0.7150\n",
      "Epoch: 079, Loss: 0.31438541412353516, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 080, Loss: 0.22711212933063507, Train: 1.0000, Val: 0.6800, Test: 0.7150\n",
      "Epoch: 081, Loss: 0.4432435929775238, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 082, Loss: 0.2912349998950958, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 083, Loss: 0.12910807132720947, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 084, Loss: 0.1466657519340515, Train: 1.0000, Val: 0.6760, Test: 0.7150\n",
      "Epoch: 085, Loss: 0.30835428833961487, Train: 1.0000, Val: 0.6740, Test: 0.7150\n",
      "Epoch: 086, Loss: 0.19106340408325195, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 087, Loss: 0.353555291891098, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 088, Loss: 0.3516773581504822, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 089, Loss: 0.3652265965938568, Train: 1.0000, Val: 0.6680, Test: 0.7150\n",
      "Epoch: 090, Loss: 0.29531049728393555, Train: 1.0000, Val: 0.6680, Test: 0.7150\n",
      "Epoch: 091, Loss: 0.2891390919685364, Train: 1.0000, Val: 0.6680, Test: 0.7150\n",
      "Epoch: 092, Loss: 0.4583744704723358, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 093, Loss: 0.3502086102962494, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 094, Loss: 0.44808346033096313, Train: 1.0000, Val: 0.6820, Test: 0.7150\n",
      "Epoch: 095, Loss: 0.3813094198703766, Train: 1.0000, Val: 0.6880, Test: 0.7150\n",
      "Epoch: 096, Loss: 0.4030981957912445, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 097, Loss: 0.21103426814079285, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 098, Loss: 0.4025333523750305, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 099, Loss: 0.38450637459754944, Train: 1.0000, Val: 0.7020, Test: 0.7150\n",
      "Epoch: 100, Loss: 0.31756463646888733, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 101, Loss: 0.28084132075309753, Train: 1.0000, Val: 0.6980, Test: 0.7150\n",
      "Epoch: 102, Loss: 0.27154815196990967, Train: 1.0000, Val: 0.6980, Test: 0.7150\n",
      "Epoch: 103, Loss: 0.4183043837547302, Train: 1.0000, Val: 0.6980, Test: 0.7150\n",
      "Epoch: 104, Loss: 0.26312294602394104, Train: 1.0000, Val: 0.7000, Test: 0.7150\n",
      "Epoch: 105, Loss: 0.2469557672739029, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 106, Loss: 0.4002872705459595, Train: 1.0000, Val: 0.6980, Test: 0.7150\n",
      "Epoch: 107, Loss: 0.26946699619293213, Train: 1.0000, Val: 0.7040, Test: 0.7150\n",
      "Epoch: 108, Loss: 0.38744792342185974, Train: 1.0000, Val: 0.7020, Test: 0.7150\n",
      "Epoch: 109, Loss: 0.3564596176147461, Train: 1.0000, Val: 0.7000, Test: 0.7150\n",
      "Epoch: 110, Loss: 0.31591010093688965, Train: 1.0000, Val: 0.6960, Test: 0.7150\n",
      "Epoch: 111, Loss: 0.35974976420402527, Train: 1.0000, Val: 0.6980, Test: 0.7150\n",
      "Epoch: 112, Loss: 0.3732662796974182, Train: 1.0000, Val: 0.6960, Test: 0.7150\n",
      "Epoch: 113, Loss: 0.39724451303482056, Train: 1.0000, Val: 0.7000, Test: 0.7150\n",
      "Epoch: 114, Loss: 0.3460451364517212, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 115, Loss: 0.23049527406692505, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 116, Loss: 0.2651885151863098, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 117, Loss: 0.3264825940132141, Train: 1.0000, Val: 0.6860, Test: 0.7150\n",
      "Epoch: 118, Loss: 0.3276408314704895, Train: 1.0000, Val: 0.6860, Test: 0.7150\n",
      "Epoch: 119, Loss: 0.37063634395599365, Train: 1.0000, Val: 0.6820, Test: 0.7150\n",
      "Epoch: 120, Loss: 0.2535727322101593, Train: 1.0000, Val: 0.6820, Test: 0.7150\n",
      "Epoch: 121, Loss: 0.21296639740467072, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 122, Loss: 0.35252270102500916, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 123, Loss: 0.14989818632602692, Train: 1.0000, Val: 0.6800, Test: 0.7150\n",
      "Epoch: 124, Loss: 0.15980364382266998, Train: 1.0000, Val: 0.6740, Test: 0.7150\n",
      "Epoch: 125, Loss: 0.5523864030838013, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 126, Loss: 0.31815314292907715, Train: 1.0000, Val: 0.6680, Test: 0.7150\n",
      "Epoch: 127, Loss: 0.5100494623184204, Train: 1.0000, Val: 0.6640, Test: 0.7150\n",
      "Epoch: 128, Loss: 0.12067721039056778, Train: 1.0000, Val: 0.6640, Test: 0.7150\n",
      "Epoch: 129, Loss: 0.1406698375940323, Train: 1.0000, Val: 0.6640, Test: 0.7150\n",
      "Epoch: 130, Loss: 0.3013622760772705, Train: 1.0000, Val: 0.6640, Test: 0.7150\n",
      "Epoch: 131, Loss: 0.3744736313819885, Train: 1.0000, Val: 0.6600, Test: 0.7150\n",
      "Epoch: 132, Loss: 0.40617817640304565, Train: 1.0000, Val: 0.6560, Test: 0.7150\n",
      "Epoch: 133, Loss: 0.28348541259765625, Train: 1.0000, Val: 0.6520, Test: 0.7150\n",
      "Epoch: 134, Loss: 0.20715904235839844, Train: 1.0000, Val: 0.6480, Test: 0.7150\n",
      "Epoch: 135, Loss: 0.37107858061790466, Train: 1.0000, Val: 0.6440, Test: 0.7150\n",
      "Epoch: 136, Loss: 0.3061485290527344, Train: 1.0000, Val: 0.6400, Test: 0.7150\n",
      "Epoch: 137, Loss: 0.4428984820842743, Train: 1.0000, Val: 0.6420, Test: 0.7150\n",
      "Epoch: 138, Loss: 0.3634008467197418, Train: 1.0000, Val: 0.6420, Test: 0.7150\n",
      "Epoch: 139, Loss: 0.4234964847564697, Train: 1.0000, Val: 0.6440, Test: 0.7150\n",
      "Epoch: 140, Loss: 0.36659255623817444, Train: 1.0000, Val: 0.6440, Test: 0.7150\n",
      "Epoch: 141, Loss: 0.2952989339828491, Train: 1.0000, Val: 0.6440, Test: 0.7150\n",
      "Epoch: 142, Loss: 0.442096084356308, Train: 1.0000, Val: 0.6500, Test: 0.7150\n",
      "Epoch: 143, Loss: 0.3248368501663208, Train: 1.0000, Val: 0.6460, Test: 0.7150\n",
      "Epoch: 144, Loss: 0.29086926579475403, Train: 1.0000, Val: 0.6500, Test: 0.7150\n",
      "Epoch: 145, Loss: 0.29527074098587036, Train: 1.0000, Val: 0.6540, Test: 0.7150\n",
      "Epoch: 146, Loss: 0.33161863684654236, Train: 1.0000, Val: 0.6560, Test: 0.7150\n",
      "Epoch: 147, Loss: 0.23449385166168213, Train: 1.0000, Val: 0.6560, Test: 0.7150\n",
      "Epoch: 148, Loss: 0.42564135789871216, Train: 1.0000, Val: 0.6580, Test: 0.7150\n",
      "Epoch: 149, Loss: 0.2582353949546814, Train: 1.0000, Val: 0.6600, Test: 0.7150\n",
      "Epoch: 150, Loss: 0.19456985592842102, Train: 1.0000, Val: 0.6600, Test: 0.7150\n",
      "Epoch: 151, Loss: 0.34589821100234985, Train: 1.0000, Val: 0.6580, Test: 0.7150\n",
      "Epoch: 152, Loss: 0.35419872403144836, Train: 1.0000, Val: 0.6580, Test: 0.7150\n",
      "Epoch: 153, Loss: 0.2892948389053345, Train: 1.0000, Val: 0.6560, Test: 0.7150\n",
      "Epoch: 154, Loss: 0.13739162683486938, Train: 1.0000, Val: 0.6560, Test: 0.7150\n",
      "Epoch: 155, Loss: 0.26203444600105286, Train: 1.0000, Val: 0.6480, Test: 0.7150\n",
      "Epoch: 156, Loss: 0.17083826661109924, Train: 1.0000, Val: 0.6440, Test: 0.7150\n",
      "Epoch: 157, Loss: 0.42432957887649536, Train: 1.0000, Val: 0.6420, Test: 0.7150\n",
      "Epoch: 158, Loss: 0.4583767354488373, Train: 1.0000, Val: 0.6440, Test: 0.7150\n",
      "Epoch: 159, Loss: 0.2690669000148773, Train: 1.0000, Val: 0.6420, Test: 0.7150\n",
      "Epoch: 160, Loss: 0.31983643770217896, Train: 1.0000, Val: 0.6400, Test: 0.7150\n",
      "Epoch: 161, Loss: 0.515963613986969, Train: 1.0000, Val: 0.6400, Test: 0.7150\n",
      "Epoch: 162, Loss: 0.33971983194351196, Train: 1.0000, Val: 0.6460, Test: 0.7150\n",
      "Epoch: 163, Loss: 0.316623330116272, Train: 1.0000, Val: 0.6500, Test: 0.7150\n",
      "Epoch: 164, Loss: 0.29874905943870544, Train: 1.0000, Val: 0.6520, Test: 0.7150\n",
      "Epoch: 165, Loss: 0.15124937891960144, Train: 1.0000, Val: 0.6540, Test: 0.7150\n",
      "Epoch: 166, Loss: 0.29016047716140747, Train: 1.0000, Val: 0.6580, Test: 0.7150\n",
      "Epoch: 167, Loss: 0.22659577429294586, Train: 1.0000, Val: 0.6660, Test: 0.7150\n",
      "Epoch: 168, Loss: 0.1668592095375061, Train: 1.0000, Val: 0.6660, Test: 0.7150\n",
      "Epoch: 169, Loss: 0.18658678233623505, Train: 1.0000, Val: 0.6700, Test: 0.7150\n",
      "Epoch: 170, Loss: 0.147035151720047, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 171, Loss: 0.4271164834499359, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 172, Loss: 0.42989620566368103, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 173, Loss: 0.3682679235935211, Train: 1.0000, Val: 0.6820, Test: 0.7150\n",
      "Epoch: 174, Loss: 0.11495161056518555, Train: 1.0000, Val: 0.6880, Test: 0.7150\n",
      "Epoch: 175, Loss: 0.2533929646015167, Train: 1.0000, Val: 0.6880, Test: 0.7150\n",
      "Epoch: 176, Loss: 0.19876649975776672, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 177, Loss: 0.3422243595123291, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 178, Loss: 0.32415297627449036, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 179, Loss: 0.13963986933231354, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 180, Loss: 0.2327132225036621, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 181, Loss: 0.42213964462280273, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 182, Loss: 0.24500353634357452, Train: 1.0000, Val: 0.6880, Test: 0.7150\n",
      "Epoch: 183, Loss: 0.31575170159339905, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 184, Loss: 0.31271764636039734, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 185, Loss: 0.29560980200767517, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 186, Loss: 0.32349735498428345, Train: 1.0000, Val: 0.6840, Test: 0.7150\n",
      "Epoch: 187, Loss: 0.15051594376564026, Train: 1.0000, Val: 0.6780, Test: 0.7150\n",
      "Epoch: 188, Loss: 0.23603864014148712, Train: 1.0000, Val: 0.6800, Test: 0.7150\n",
      "Epoch: 189, Loss: 0.3014410138130188, Train: 1.0000, Val: 0.6740, Test: 0.7150\n",
      "Epoch: 190, Loss: 0.25272202491760254, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 191, Loss: 0.3731584846973419, Train: 1.0000, Val: 0.6740, Test: 0.7150\n",
      "Epoch: 192, Loss: 0.3001086711883545, Train: 1.0000, Val: 0.6720, Test: 0.7150\n",
      "Epoch: 193, Loss: 0.23428647220134735, Train: 1.0000, Val: 0.6660, Test: 0.7150\n",
      "Epoch: 194, Loss: 0.27379995584487915, Train: 1.0000, Val: 0.6640, Test: 0.7150\n",
      "Epoch: 195, Loss: 0.19041983783245087, Train: 1.0000, Val: 0.6620, Test: 0.7150\n",
      "Epoch: 196, Loss: 0.21422190964221954, Train: 1.0000, Val: 0.6640, Test: 0.7150\n",
      "Epoch: 197, Loss: 0.14234872162342072, Train: 1.0000, Val: 0.6620, Test: 0.7150\n",
      "Epoch: 198, Loss: 0.12423046678304672, Train: 1.0000, Val: 0.6580, Test: 0.7150\n",
      "Epoch: 199, Loss: 0.1299445629119873, Train: 1.0000, Val: 0.6580, Test: 0.7150\n",
      "Best Val Acc: 0.7160 Test Acc: 0.7150\n",
      "Epoch: 001, Loss: 2.0066332817077637, Train: 0.5143, Val: 0.2280, Test: 0.2370\n",
      "Epoch: 002, Loss: 1.7727241516113281, Train: 0.5429, Val: 0.2820, Test: 0.2980\n",
      "Epoch: 003, Loss: 1.5555297136306763, Train: 0.7143, Val: 0.3440, Test: 0.3660\n",
      "Epoch: 004, Loss: 1.4803344011306763, Train: 0.8571, Val: 0.3980, Test: 0.4250\n",
      "Epoch: 005, Loss: 1.4057133197784424, Train: 0.9143, Val: 0.4480, Test: 0.4700\n",
      "Epoch: 006, Loss: 1.3528212308883667, Train: 0.9143, Val: 0.5120, Test: 0.5280\n",
      "Epoch: 007, Loss: 1.236229658126831, Train: 1.0000, Val: 0.5480, Test: 0.5780\n",
      "Epoch: 008, Loss: 1.0327856540679932, Train: 1.0000, Val: 0.5740, Test: 0.6240\n",
      "Epoch: 009, Loss: 0.9705113172531128, Train: 1.0000, Val: 0.6180, Test: 0.6580\n",
      "Epoch: 010, Loss: 0.9958052039146423, Train: 1.0000, Val: 0.6500, Test: 0.6750\n",
      "Epoch: 011, Loss: 0.8196480870246887, Train: 1.0000, Val: 0.6540, Test: 0.6850\n",
      "Epoch: 012, Loss: 0.8715206384658813, Train: 1.0000, Val: 0.6560, Test: 0.6840\n",
      "Epoch: 013, Loss: 1.0829565525054932, Train: 1.0000, Val: 0.6600, Test: 0.6880\n",
      "Epoch: 014, Loss: 0.8258746266365051, Train: 1.0000, Val: 0.6660, Test: 0.6910\n",
      "Epoch: 015, Loss: 0.6612477898597717, Train: 1.0000, Val: 0.6620, Test: 0.6910\n",
      "Epoch: 016, Loss: 0.7029231786727905, Train: 1.0000, Val: 0.6620, Test: 0.6910\n",
      "Epoch: 017, Loss: 0.6409317851066589, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 018, Loss: 0.5100422501564026, Train: 1.0000, Val: 0.6620, Test: 0.6910\n",
      "Epoch: 019, Loss: 0.4390996992588043, Train: 1.0000, Val: 0.6600, Test: 0.6910\n",
      "Epoch: 020, Loss: 0.6242305636405945, Train: 1.0000, Val: 0.6500, Test: 0.6910\n",
      "Epoch: 021, Loss: 0.6082596182823181, Train: 1.0000, Val: 0.6560, Test: 0.6910\n",
      "Epoch: 022, Loss: 0.6542578339576721, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 023, Loss: 0.6293569803237915, Train: 1.0000, Val: 0.6480, Test: 0.6910\n",
      "Epoch: 024, Loss: 0.47048306465148926, Train: 1.0000, Val: 0.6460, Test: 0.6910\n",
      "Epoch: 025, Loss: 0.4152180254459381, Train: 1.0000, Val: 0.6380, Test: 0.6910\n",
      "Epoch: 026, Loss: 0.536605715751648, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 027, Loss: 0.588403582572937, Train: 1.0000, Val: 0.6260, Test: 0.6910\n",
      "Epoch: 028, Loss: 0.3961033225059509, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 029, Loss: 0.4133760333061218, Train: 1.0000, Val: 0.6200, Test: 0.6910\n",
      "Epoch: 030, Loss: 0.35871270298957825, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 031, Loss: 0.4588277339935303, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 032, Loss: 0.4455208480358124, Train: 1.0000, Val: 0.6360, Test: 0.6910\n",
      "Epoch: 033, Loss: 0.5568280816078186, Train: 1.0000, Val: 0.6420, Test: 0.6910\n",
      "Epoch: 034, Loss: 0.5391263961791992, Train: 1.0000, Val: 0.6480, Test: 0.6910\n",
      "Epoch: 035, Loss: 0.28292760252952576, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 036, Loss: 0.2973966598510742, Train: 1.0000, Val: 0.6460, Test: 0.6910\n",
      "Epoch: 037, Loss: 0.4561615288257599, Train: 1.0000, Val: 0.6500, Test: 0.6910\n",
      "Epoch: 038, Loss: 0.2650272846221924, Train: 1.0000, Val: 0.6500, Test: 0.6910\n",
      "Epoch: 039, Loss: 0.29465994238853455, Train: 1.0000, Val: 0.6480, Test: 0.6910\n",
      "Epoch: 040, Loss: 0.42216846346855164, Train: 1.0000, Val: 0.6440, Test: 0.6910\n",
      "Epoch: 041, Loss: 0.6874553561210632, Train: 1.0000, Val: 0.6440, Test: 0.6910\n",
      "Epoch: 042, Loss: 0.5195398330688477, Train: 1.0000, Val: 0.6440, Test: 0.6910\n",
      "Epoch: 043, Loss: 0.5208501219749451, Train: 1.0000, Val: 0.6440, Test: 0.6910\n",
      "Epoch: 044, Loss: 0.2584005296230316, Train: 1.0000, Val: 0.6480, Test: 0.6910\n",
      "Epoch: 045, Loss: 0.29029548168182373, Train: 1.0000, Val: 0.6480, Test: 0.6910\n",
      "Epoch: 046, Loss: 0.36705413460731506, Train: 1.0000, Val: 0.6500, Test: 0.6910\n",
      "Epoch: 047, Loss: 0.21069708466529846, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 048, Loss: 0.4340601861476898, Train: 1.0000, Val: 0.6600, Test: 0.6910\n",
      "Epoch: 049, Loss: 0.24725879728794098, Train: 1.0000, Val: 0.6640, Test: 0.6910\n",
      "Epoch: 050, Loss: 0.3622411787509918, Train: 1.0000, Val: 0.6640, Test: 0.6910\n",
      "Epoch: 051, Loss: 0.39609259366989136, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 052, Loss: 0.42747434973716736, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 053, Loss: 0.5913391709327698, Train: 1.0000, Val: 0.6560, Test: 0.6910\n",
      "Epoch: 054, Loss: 0.36028143763542175, Train: 1.0000, Val: 0.6500, Test: 0.6910\n",
      "Epoch: 055, Loss: 0.30196109414100647, Train: 1.0000, Val: 0.6460, Test: 0.6910\n",
      "Epoch: 056, Loss: 0.2793656289577484, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 057, Loss: 0.4190021753311157, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 058, Loss: 0.2237764298915863, Train: 1.0000, Val: 0.6540, Test: 0.6910\n",
      "Epoch: 059, Loss: 0.27129867672920227, Train: 1.0000, Val: 0.6560, Test: 0.6910\n",
      "Epoch: 060, Loss: 0.5237097144126892, Train: 1.0000, Val: 0.6540, Test: 0.6910\n",
      "Epoch: 061, Loss: 0.17522110044956207, Train: 1.0000, Val: 0.6540, Test: 0.6910\n",
      "Epoch: 062, Loss: 0.3788791000843048, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 063, Loss: 0.2731122076511383, Train: 1.0000, Val: 0.6540, Test: 0.6910\n",
      "Epoch: 064, Loss: 0.4866354465484619, Train: 1.0000, Val: 0.6540, Test: 0.6910\n",
      "Epoch: 065, Loss: 0.17442727088928223, Train: 1.0000, Val: 0.6560, Test: 0.6910\n",
      "Epoch: 066, Loss: 0.35479751229286194, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 067, Loss: 0.3181592524051666, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 068, Loss: 0.47332465648651123, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 069, Loss: 0.32410287857055664, Train: 1.0000, Val: 0.6540, Test: 0.6910\n",
      "Epoch: 070, Loss: 0.3596498370170593, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 071, Loss: 0.39793121814727783, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 072, Loss: 0.22280141711235046, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 073, Loss: 0.29335200786590576, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 074, Loss: 0.4301210641860962, Train: 1.0000, Val: 0.6500, Test: 0.6910\n",
      "Epoch: 075, Loss: 0.38079366087913513, Train: 1.0000, Val: 0.6480, Test: 0.6910\n",
      "Epoch: 076, Loss: 0.24898160994052887, Train: 1.0000, Val: 0.6440, Test: 0.6910\n",
      "Epoch: 077, Loss: 0.41290807723999023, Train: 1.0000, Val: 0.6420, Test: 0.6910\n",
      "Epoch: 078, Loss: 0.38141587376594543, Train: 1.0000, Val: 0.6440, Test: 0.6910\n",
      "Epoch: 079, Loss: 0.18687205016613007, Train: 1.0000, Val: 0.6380, Test: 0.6910\n",
      "Epoch: 080, Loss: 0.4096289277076721, Train: 1.0000, Val: 0.6380, Test: 0.6910\n",
      "Epoch: 081, Loss: 0.3153790831565857, Train: 1.0000, Val: 0.6340, Test: 0.6910\n",
      "Epoch: 082, Loss: 0.2820630371570587, Train: 1.0000, Val: 0.6340, Test: 0.6910\n",
      "Epoch: 083, Loss: 0.13438566029071808, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 084, Loss: 0.4106387197971344, Train: 1.0000, Val: 0.6300, Test: 0.6910\n",
      "Epoch: 085, Loss: 0.4450092315673828, Train: 1.0000, Val: 0.6280, Test: 0.6910\n",
      "Epoch: 086, Loss: 0.5938805937767029, Train: 1.0000, Val: 0.6280, Test: 0.6910\n",
      "Epoch: 087, Loss: 0.09570353478193283, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 088, Loss: 0.27453190088272095, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 089, Loss: 0.22529417276382446, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 090, Loss: 0.4148513674736023, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 091, Loss: 0.4192396104335785, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 092, Loss: 0.2443435788154602, Train: 1.0000, Val: 0.6260, Test: 0.6910\n",
      "Epoch: 093, Loss: 0.5016445517539978, Train: 1.0000, Val: 0.6300, Test: 0.6910\n",
      "Epoch: 094, Loss: 0.24146662652492523, Train: 1.0000, Val: 0.6300, Test: 0.6910\n",
      "Epoch: 095, Loss: 0.22082625329494476, Train: 1.0000, Val: 0.6300, Test: 0.6910\n",
      "Epoch: 096, Loss: 0.42109203338623047, Train: 1.0000, Val: 0.6260, Test: 0.6910\n",
      "Epoch: 097, Loss: 0.2689123749732971, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 098, Loss: 0.38832128047943115, Train: 1.0000, Val: 0.6160, Test: 0.6910\n",
      "Epoch: 099, Loss: 0.2519824206829071, Train: 1.0000, Val: 0.6220, Test: 0.6910\n",
      "Epoch: 100, Loss: 0.34896472096443176, Train: 1.0000, Val: 0.6260, Test: 0.6910\n",
      "Epoch: 101, Loss: 0.3052380084991455, Train: 1.0000, Val: 0.6180, Test: 0.6910\n",
      "Epoch: 102, Loss: 0.24834418296813965, Train: 1.0000, Val: 0.6200, Test: 0.6910\n",
      "Epoch: 103, Loss: 0.4028199315071106, Train: 1.0000, Val: 0.6220, Test: 0.6910\n",
      "Epoch: 104, Loss: 0.3427937626838684, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 105, Loss: 0.5436013340950012, Train: 1.0000, Val: 0.6220, Test: 0.6910\n",
      "Epoch: 106, Loss: 0.4150673747062683, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 107, Loss: 0.5180673003196716, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 108, Loss: 0.3908505141735077, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 109, Loss: 0.5340575575828552, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 110, Loss: 0.24298851191997528, Train: 1.0000, Val: 0.6240, Test: 0.6910\n",
      "Epoch: 111, Loss: 0.2577683925628662, Train: 1.0000, Val: 0.6280, Test: 0.6910\n",
      "Epoch: 112, Loss: 0.27371034026145935, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 113, Loss: 0.42022207379341125, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 114, Loss: 0.6979748606681824, Train: 1.0000, Val: 0.6340, Test: 0.6910\n",
      "Epoch: 115, Loss: 0.3740181624889374, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 116, Loss: 0.46481966972351074, Train: 1.0000, Val: 0.6320, Test: 0.6910\n",
      "Epoch: 117, Loss: 0.41617727279663086, Train: 1.0000, Val: 0.6340, Test: 0.6910\n",
      "Epoch: 118, Loss: 0.3478512763977051, Train: 1.0000, Val: 0.6420, Test: 0.6910\n",
      "Epoch: 119, Loss: 0.39411601424217224, Train: 1.0000, Val: 0.6440, Test: 0.6910\n",
      "Epoch: 120, Loss: 0.30266302824020386, Train: 1.0000, Val: 0.6500, Test: 0.6910\n",
      "Epoch: 121, Loss: 0.2704302966594696, Train: 1.0000, Val: 0.6520, Test: 0.6910\n",
      "Epoch: 122, Loss: 0.34634336829185486, Train: 1.0000, Val: 0.6560, Test: 0.6910\n",
      "Epoch: 123, Loss: 0.5505294799804688, Train: 1.0000, Val: 0.6580, Test: 0.6910\n",
      "Epoch: 124, Loss: 0.41887930035591125, Train: 1.0000, Val: 0.6600, Test: 0.6910\n",
      "Epoch: 125, Loss: 0.4124404191970825, Train: 1.0000, Val: 0.6600, Test: 0.6910\n",
      "Epoch: 126, Loss: 0.36165013909339905, Train: 1.0000, Val: 0.6600, Test: 0.6910\n",
      "Epoch: 127, Loss: 0.31648656725883484, Train: 1.0000, Val: 0.6640, Test: 0.6910\n",
      "Epoch: 128, Loss: 0.5048959851264954, Train: 1.0000, Val: 0.6640, Test: 0.6910\n",
      "Epoch: 129, Loss: 0.32188984751701355, Train: 1.0000, Val: 0.6680, Test: 0.6810\n",
      "Epoch: 130, Loss: 0.2731112837791443, Train: 1.0000, Val: 0.6720, Test: 0.6840\n",
      "Epoch: 131, Loss: 0.3443453311920166, Train: 1.0000, Val: 0.6720, Test: 0.6840\n",
      "Epoch: 132, Loss: 0.26685091853141785, Train: 1.0000, Val: 0.6740, Test: 0.6840\n",
      "Epoch: 133, Loss: 0.434672474861145, Train: 1.0000, Val: 0.6720, Test: 0.6840\n",
      "Epoch: 134, Loss: 0.3562560975551605, Train: 1.0000, Val: 0.6700, Test: 0.6840\n",
      "Epoch: 135, Loss: 0.38892653584480286, Train: 1.0000, Val: 0.6700, Test: 0.6840\n",
      "Epoch: 136, Loss: 0.31583863496780396, Train: 1.0000, Val: 0.6700, Test: 0.6840\n",
      "Epoch: 137, Loss: 0.4345241189002991, Train: 1.0000, Val: 0.6720, Test: 0.6840\n",
      "Epoch: 138, Loss: 0.2831723988056183, Train: 1.0000, Val: 0.6720, Test: 0.6840\n",
      "Epoch: 139, Loss: 0.37455978989601135, Train: 1.0000, Val: 0.6700, Test: 0.6840\n",
      "Epoch: 140, Loss: 0.395040899515152, Train: 1.0000, Val: 0.6680, Test: 0.6840\n",
      "Epoch: 141, Loss: 0.3247299790382385, Train: 1.0000, Val: 0.6640, Test: 0.6840\n",
      "Epoch: 142, Loss: 0.3562638759613037, Train: 1.0000, Val: 0.6620, Test: 0.6840\n",
      "Epoch: 143, Loss: 0.35660579800605774, Train: 1.0000, Val: 0.6620, Test: 0.6840\n",
      "Epoch: 144, Loss: 0.43765929341316223, Train: 1.0000, Val: 0.6600, Test: 0.6840\n",
      "Epoch: 145, Loss: 0.3255038559436798, Train: 1.0000, Val: 0.6560, Test: 0.6840\n",
      "Epoch: 146, Loss: 0.2792975902557373, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Epoch: 147, Loss: 0.2089395821094513, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Epoch: 148, Loss: 0.1552581489086151, Train: 1.0000, Val: 0.6560, Test: 0.6840\n",
      "Epoch: 149, Loss: 0.3358811140060425, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Epoch: 150, Loss: 0.207249715924263, Train: 1.0000, Val: 0.6520, Test: 0.6840\n",
      "Epoch: 151, Loss: 0.1744418442249298, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Epoch: 152, Loss: 0.15737339854240417, Train: 1.0000, Val: 0.6560, Test: 0.6840\n",
      "Epoch: 153, Loss: 0.16876469552516937, Train: 1.0000, Val: 0.6480, Test: 0.6840\n",
      "Epoch: 154, Loss: 0.270579069852829, Train: 1.0000, Val: 0.6440, Test: 0.6840\n",
      "Epoch: 155, Loss: 0.36824822425842285, Train: 1.0000, Val: 0.6440, Test: 0.6840\n",
      "Epoch: 156, Loss: 0.22862914204597473, Train: 1.0000, Val: 0.6400, Test: 0.6840\n",
      "Epoch: 157, Loss: 0.31521397829055786, Train: 1.0000, Val: 0.6380, Test: 0.6840\n",
      "Epoch: 158, Loss: 0.3721159100532532, Train: 1.0000, Val: 0.6380, Test: 0.6840\n",
      "Epoch: 159, Loss: 0.24090398848056793, Train: 1.0000, Val: 0.6400, Test: 0.6840\n",
      "Epoch: 160, Loss: 0.28486353158950806, Train: 1.0000, Val: 0.6400, Test: 0.6840\n",
      "Epoch: 161, Loss: 0.23138470947742462, Train: 1.0000, Val: 0.6400, Test: 0.6840\n",
      "Epoch: 162, Loss: 0.2760283648967743, Train: 1.0000, Val: 0.6380, Test: 0.6840\n",
      "Epoch: 163, Loss: 0.21722014248371124, Train: 1.0000, Val: 0.6360, Test: 0.6840\n",
      "Epoch: 164, Loss: 0.2500084936618805, Train: 1.0000, Val: 0.6360, Test: 0.6840\n",
      "Epoch: 165, Loss: 0.4719808101654053, Train: 1.0000, Val: 0.6340, Test: 0.6840\n",
      "Epoch: 166, Loss: 0.4536146819591522, Train: 1.0000, Val: 0.6340, Test: 0.6840\n",
      "Epoch: 167, Loss: 0.29049351811408997, Train: 1.0000, Val: 0.6340, Test: 0.6840\n",
      "Epoch: 168, Loss: 0.2353261113166809, Train: 1.0000, Val: 0.6360, Test: 0.6840\n",
      "Epoch: 169, Loss: 0.26340922713279724, Train: 1.0000, Val: 0.6360, Test: 0.6840\n",
      "Epoch: 170, Loss: 0.2710862457752228, Train: 1.0000, Val: 0.6360, Test: 0.6840\n",
      "Epoch: 171, Loss: 0.43007662892341614, Train: 1.0000, Val: 0.6360, Test: 0.6840\n",
      "Epoch: 172, Loss: 0.16649162769317627, Train: 1.0000, Val: 0.6380, Test: 0.6840\n",
      "Epoch: 173, Loss: 0.41587111353874207, Train: 1.0000, Val: 0.6380, Test: 0.6840\n",
      "Epoch: 174, Loss: 0.18007420003414154, Train: 1.0000, Val: 0.6380, Test: 0.6840\n",
      "Epoch: 175, Loss: 0.11275216937065125, Train: 1.0000, Val: 0.6400, Test: 0.6840\n",
      "Epoch: 176, Loss: 0.18810510635375977, Train: 1.0000, Val: 0.6420, Test: 0.6840\n",
      "Epoch: 177, Loss: 0.3540889322757721, Train: 1.0000, Val: 0.6440, Test: 0.6840\n",
      "Epoch: 178, Loss: 0.2132868468761444, Train: 1.0000, Val: 0.6440, Test: 0.6840\n",
      "Epoch: 179, Loss: 0.3867585361003876, Train: 1.0000, Val: 0.6440, Test: 0.6840\n",
      "Epoch: 180, Loss: 0.2550639808177948, Train: 1.0000, Val: 0.6440, Test: 0.6840\n",
      "Epoch: 181, Loss: 0.2561201751232147, Train: 1.0000, Val: 0.6440, Test: 0.6840\n",
      "Epoch: 182, Loss: 0.20267990231513977, Train: 1.0000, Val: 0.6460, Test: 0.6840\n",
      "Epoch: 183, Loss: 0.3433968722820282, Train: 1.0000, Val: 0.6480, Test: 0.6840\n",
      "Epoch: 184, Loss: 0.2632084786891937, Train: 1.0000, Val: 0.6520, Test: 0.6840\n",
      "Epoch: 185, Loss: 0.37253278493881226, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Epoch: 186, Loss: 0.3186193108558655, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Epoch: 187, Loss: 0.3847302794456482, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Epoch: 188, Loss: 0.3320677876472473, Train: 1.0000, Val: 0.6560, Test: 0.6840\n",
      "Epoch: 189, Loss: 0.2300405353307724, Train: 1.0000, Val: 0.6600, Test: 0.6840\n",
      "Epoch: 190, Loss: 0.11236728727817535, Train: 1.0000, Val: 0.6620, Test: 0.6840\n",
      "Epoch: 191, Loss: 0.28244704008102417, Train: 1.0000, Val: 0.6620, Test: 0.6840\n",
      "Epoch: 192, Loss: 0.25780829787254333, Train: 1.0000, Val: 0.6640, Test: 0.6840\n",
      "Epoch: 193, Loss: 0.24269863963127136, Train: 1.0000, Val: 0.6640, Test: 0.6840\n",
      "Epoch: 194, Loss: 0.42588499188423157, Train: 1.0000, Val: 0.6640, Test: 0.6840\n",
      "Epoch: 195, Loss: 0.21900120377540588, Train: 1.0000, Val: 0.6620, Test: 0.6840\n",
      "Epoch: 196, Loss: 0.3844909071922302, Train: 1.0000, Val: 0.6600, Test: 0.6840\n",
      "Epoch: 197, Loss: 0.26143935322761536, Train: 1.0000, Val: 0.6560, Test: 0.6840\n",
      "Epoch: 198, Loss: 0.45330435037612915, Train: 1.0000, Val: 0.6560, Test: 0.6840\n",
      "Epoch: 199, Loss: 0.20760996639728546, Train: 1.0000, Val: 0.6540, Test: 0.6840\n",
      "Best Val Acc: 0.6740 Test Acc: 0.6840\n",
      "Epoch: 001, Loss: 1.941463828086853, Train: 0.6286, Val: 0.3620, Test: 0.3330\n",
      "Epoch: 002, Loss: 1.838064432144165, Train: 0.6571, Val: 0.3940, Test: 0.3950\n",
      "Epoch: 003, Loss: 1.736746907234192, Train: 0.8571, Val: 0.4620, Test: 0.4380\n",
      "Epoch: 004, Loss: 1.5259430408477783, Train: 0.9714, Val: 0.5200, Test: 0.5000\n",
      "Epoch: 005, Loss: 1.3676165342330933, Train: 0.9714, Val: 0.5760, Test: 0.5590\n",
      "Epoch: 006, Loss: 1.3280937671661377, Train: 0.9714, Val: 0.6200, Test: 0.6010\n",
      "Epoch: 007, Loss: 1.1261100769042969, Train: 1.0000, Val: 0.6420, Test: 0.6290\n",
      "Epoch: 008, Loss: 0.9858443140983582, Train: 1.0000, Val: 0.6600, Test: 0.6520\n",
      "Epoch: 009, Loss: 1.0010548830032349, Train: 1.0000, Val: 0.6680, Test: 0.6590\n",
      "Epoch: 010, Loss: 0.90635085105896, Train: 1.0000, Val: 0.6660, Test: 0.6590\n",
      "Epoch: 011, Loss: 0.8027613162994385, Train: 1.0000, Val: 0.6600, Test: 0.6590\n",
      "Epoch: 012, Loss: 0.8920335173606873, Train: 1.0000, Val: 0.6540, Test: 0.6590\n",
      "Epoch: 013, Loss: 0.9090524315834045, Train: 1.0000, Val: 0.6540, Test: 0.6590\n",
      "Epoch: 014, Loss: 0.7698498964309692, Train: 1.0000, Val: 0.6580, Test: 0.6590\n",
      "Epoch: 015, Loss: 0.8338692784309387, Train: 1.0000, Val: 0.6560, Test: 0.6590\n",
      "Epoch: 016, Loss: 0.8820852041244507, Train: 1.0000, Val: 0.6580, Test: 0.6590\n",
      "Epoch: 017, Loss: 0.6226542592048645, Train: 1.0000, Val: 0.6520, Test: 0.6590\n",
      "Epoch: 018, Loss: 0.4589698910713196, Train: 1.0000, Val: 0.6420, Test: 0.6590\n",
      "Epoch: 019, Loss: 0.6421200633049011, Train: 1.0000, Val: 0.6400, Test: 0.6590\n",
      "Epoch: 020, Loss: 0.5827215313911438, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 021, Loss: 0.5987038612365723, Train: 1.0000, Val: 0.6360, Test: 0.6590\n",
      "Epoch: 022, Loss: 0.8201669454574585, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 023, Loss: 0.4457339942455292, Train: 1.0000, Val: 0.6280, Test: 0.6590\n",
      "Epoch: 024, Loss: 0.3268286883831024, Train: 1.0000, Val: 0.6200, Test: 0.6590\n",
      "Epoch: 025, Loss: 0.5905346274375916, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 026, Loss: 0.5307895541191101, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 027, Loss: 0.2337523251771927, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 028, Loss: 0.45190224051475525, Train: 1.0000, Val: 0.6160, Test: 0.6590\n",
      "Epoch: 029, Loss: 0.7139160633087158, Train: 1.0000, Val: 0.6160, Test: 0.6590\n",
      "Epoch: 030, Loss: 0.38364464044570923, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 031, Loss: 0.4863189458847046, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 032, Loss: 0.4480198919773102, Train: 1.0000, Val: 0.6200, Test: 0.6590\n",
      "Epoch: 033, Loss: 0.6745274066925049, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 034, Loss: 0.5564200282096863, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 035, Loss: 0.5662684440612793, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 036, Loss: 0.30481308698654175, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 037, Loss: 0.3756696581840515, Train: 1.0000, Val: 0.6180, Test: 0.6590\n",
      "Epoch: 038, Loss: 0.6012738943099976, Train: 1.0000, Val: 0.6240, Test: 0.6590\n",
      "Epoch: 039, Loss: 0.7654566764831543, Train: 1.0000, Val: 0.6280, Test: 0.6590\n",
      "Epoch: 040, Loss: 0.4397686719894409, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 041, Loss: 0.4573887884616852, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 042, Loss: 0.4976644217967987, Train: 1.0000, Val: 0.6400, Test: 0.6590\n",
      "Epoch: 043, Loss: 0.47042348980903625, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 044, Loss: 0.4760994017124176, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 045, Loss: 0.3718537390232086, Train: 1.0000, Val: 0.6400, Test: 0.6590\n",
      "Epoch: 046, Loss: 0.39549675583839417, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 047, Loss: 0.40964940190315247, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 048, Loss: 0.3527992367744446, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 049, Loss: 0.30615076422691345, Train: 1.0000, Val: 0.6540, Test: 0.6590\n",
      "Epoch: 050, Loss: 0.1966622769832611, Train: 1.0000, Val: 0.6620, Test: 0.6590\n",
      "Epoch: 051, Loss: 0.3288326561450958, Train: 1.0000, Val: 0.6620, Test: 0.6590\n",
      "Epoch: 052, Loss: 0.3593061864376068, Train: 1.0000, Val: 0.6660, Test: 0.6590\n",
      "Epoch: 053, Loss: 0.34756261110305786, Train: 1.0000, Val: 0.6680, Test: 0.6590\n",
      "Epoch: 054, Loss: 0.3110247552394867, Train: 1.0000, Val: 0.6680, Test: 0.6590\n",
      "Epoch: 055, Loss: 0.46893325448036194, Train: 1.0000, Val: 0.6660, Test: 0.6590\n",
      "Epoch: 056, Loss: 0.2739165723323822, Train: 1.0000, Val: 0.6660, Test: 0.6590\n",
      "Epoch: 057, Loss: 0.537010908126831, Train: 1.0000, Val: 0.6660, Test: 0.6590\n",
      "Epoch: 058, Loss: 0.3367183208465576, Train: 1.0000, Val: 0.6640, Test: 0.6590\n",
      "Epoch: 059, Loss: 0.4232422709465027, Train: 1.0000, Val: 0.6620, Test: 0.6590\n",
      "Epoch: 060, Loss: 0.548944890499115, Train: 1.0000, Val: 0.6580, Test: 0.6590\n",
      "Epoch: 061, Loss: 0.5570226311683655, Train: 1.0000, Val: 0.6560, Test: 0.6590\n",
      "Epoch: 062, Loss: 0.22185282409191132, Train: 1.0000, Val: 0.6560, Test: 0.6590\n",
      "Epoch: 063, Loss: 0.20675164461135864, Train: 1.0000, Val: 0.6560, Test: 0.6590\n",
      "Epoch: 064, Loss: 0.3816035985946655, Train: 1.0000, Val: 0.6520, Test: 0.6590\n",
      "Epoch: 065, Loss: 0.3107531666755676, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 066, Loss: 0.37267664074897766, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 067, Loss: 0.49833032488822937, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 068, Loss: 0.5656725168228149, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 069, Loss: 0.2825361490249634, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 070, Loss: 0.25044912099838257, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 071, Loss: 0.3467291295528412, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 072, Loss: 0.3330703675746918, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 073, Loss: 0.5775407552719116, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 074, Loss: 0.25242751836776733, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 075, Loss: 0.4303632378578186, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 076, Loss: 0.2560499608516693, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 077, Loss: 0.2894488275051117, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 078, Loss: 0.48139262199401855, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 079, Loss: 0.16897261142730713, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 080, Loss: 0.21567648649215698, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 081, Loss: 0.2675389349460602, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 082, Loss: 0.26944592595100403, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 083, Loss: 0.32900574803352356, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 084, Loss: 0.3947061598300934, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 085, Loss: 0.5232533812522888, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 086, Loss: 0.21801753342151642, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 087, Loss: 0.24711920320987701, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 088, Loss: 0.31170058250427246, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 089, Loss: 0.37853577733039856, Train: 1.0000, Val: 0.6520, Test: 0.6590\n",
      "Epoch: 090, Loss: 0.3087072968482971, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 091, Loss: 0.3371163010597229, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 092, Loss: 0.27061134576797485, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 093, Loss: 0.2960207760334015, Train: 1.0000, Val: 0.6520, Test: 0.6590\n",
      "Epoch: 094, Loss: 0.403075635433197, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 095, Loss: 0.3898455798625946, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 096, Loss: 0.5326396822929382, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 097, Loss: 0.5196444988250732, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 098, Loss: 0.23112763464450836, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 099, Loss: 0.31463339924812317, Train: 1.0000, Val: 0.6420, Test: 0.6590\n",
      "Epoch: 100, Loss: 0.08922523260116577, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 101, Loss: 0.3708834946155548, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 102, Loss: 0.31251707673072815, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 103, Loss: 0.4270417094230652, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 104, Loss: 0.41227489709854126, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 105, Loss: 0.5827931761741638, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 106, Loss: 0.16151955723762512, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 107, Loss: 0.3082980811595917, Train: 1.0000, Val: 0.6480, Test: 0.6590\n",
      "Epoch: 108, Loss: 0.2777942717075348, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 109, Loss: 0.3633846640586853, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 110, Loss: 0.3174895644187927, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 111, Loss: 0.3793164789676666, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 112, Loss: 0.17417731881141663, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 113, Loss: 0.400405615568161, Train: 1.0000, Val: 0.6400, Test: 0.6590\n",
      "Epoch: 114, Loss: 0.32241934537887573, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 115, Loss: 0.3732207715511322, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 116, Loss: 0.23194734752178192, Train: 1.0000, Val: 0.6440, Test: 0.6590\n",
      "Epoch: 117, Loss: 0.31331637501716614, Train: 1.0000, Val: 0.6420, Test: 0.6590\n",
      "Epoch: 118, Loss: 0.2744283080101013, Train: 1.0000, Val: 0.6360, Test: 0.6590\n",
      "Epoch: 119, Loss: 0.32729822397232056, Train: 1.0000, Val: 0.6360, Test: 0.6590\n",
      "Epoch: 120, Loss: 0.42733144760131836, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 121, Loss: 0.4250643253326416, Train: 1.0000, Val: 0.6360, Test: 0.6590\n",
      "Epoch: 122, Loss: 0.20508462190628052, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 123, Loss: 0.381407767534256, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 124, Loss: 0.30770865082740784, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 125, Loss: 0.27461665868759155, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 126, Loss: 0.386637419462204, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 127, Loss: 0.25349926948547363, Train: 1.0000, Val: 0.6360, Test: 0.6590\n",
      "Epoch: 128, Loss: 0.3080180287361145, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 129, Loss: 0.2521534562110901, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 130, Loss: 0.3150118887424469, Train: 1.0000, Val: 0.6520, Test: 0.6590\n",
      "Epoch: 131, Loss: 0.41228753328323364, Train: 1.0000, Val: 0.6400, Test: 0.6590\n",
      "Epoch: 132, Loss: 0.38045406341552734, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 133, Loss: 0.3269328773021698, Train: 1.0000, Val: 0.6360, Test: 0.6590\n",
      "Epoch: 134, Loss: 0.10198131203651428, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 135, Loss: 0.24967192113399506, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 136, Loss: 0.27391308546066284, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 137, Loss: 0.41720640659332275, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 138, Loss: 0.22593055665493011, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 139, Loss: 0.3315125107765198, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 140, Loss: 0.306957483291626, Train: 1.0000, Val: 0.6300, Test: 0.6590\n",
      "Epoch: 141, Loss: 0.20733124017715454, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 142, Loss: 0.5731072425842285, Train: 1.0000, Val: 0.6360, Test: 0.6590\n",
      "Epoch: 143, Loss: 0.5875601172447205, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 144, Loss: 0.5367291569709778, Train: 1.0000, Val: 0.6300, Test: 0.6590\n",
      "Epoch: 145, Loss: 0.24136056005954742, Train: 1.0000, Val: 0.6200, Test: 0.6590\n",
      "Epoch: 146, Loss: 0.17951957881450653, Train: 1.0000, Val: 0.6220, Test: 0.6590\n",
      "Epoch: 147, Loss: 0.26567450165748596, Train: 1.0000, Val: 0.6260, Test: 0.6590\n",
      "Epoch: 148, Loss: 0.2643616795539856, Train: 1.0000, Val: 0.6260, Test: 0.6590\n",
      "Epoch: 149, Loss: 0.2804069221019745, Train: 1.0000, Val: 0.6240, Test: 0.6590\n",
      "Epoch: 150, Loss: 0.1289191097021103, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 151, Loss: 0.2243853509426117, Train: 1.0000, Val: 0.6280, Test: 0.6590\n",
      "Epoch: 152, Loss: 0.24929098784923553, Train: 1.0000, Val: 0.6260, Test: 0.6590\n",
      "Epoch: 153, Loss: 0.3769758641719818, Train: 1.0000, Val: 0.6260, Test: 0.6590\n",
      "Epoch: 154, Loss: 0.27624356746673584, Train: 1.0000, Val: 0.6300, Test: 0.6590\n",
      "Epoch: 155, Loss: 0.3636203110218048, Train: 1.0000, Val: 0.6300, Test: 0.6590\n",
      "Epoch: 156, Loss: 0.24815672636032104, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 157, Loss: 0.3789083659648895, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 158, Loss: 0.18213902413845062, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 159, Loss: 0.3653770387172699, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 160, Loss: 0.25784844160079956, Train: 1.0000, Val: 0.6320, Test: 0.6590\n",
      "Epoch: 161, Loss: 0.35216665267944336, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 162, Loss: 0.4486195743083954, Train: 1.0000, Val: 0.6400, Test: 0.6590\n",
      "Epoch: 163, Loss: 0.19385720789432526, Train: 1.0000, Val: 0.6400, Test: 0.6590\n",
      "Epoch: 164, Loss: 0.33252403140068054, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 165, Loss: 0.3437366485595703, Train: 1.0000, Val: 0.6460, Test: 0.6590\n",
      "Epoch: 166, Loss: 0.16517893970012665, Train: 1.0000, Val: 0.6500, Test: 0.6590\n",
      "Epoch: 167, Loss: 0.3102997839450836, Train: 1.0000, Val: 0.6520, Test: 0.6590\n",
      "Epoch: 168, Loss: 0.15171119570732117, Train: 1.0000, Val: 0.6620, Test: 0.6590\n",
      "Epoch: 169, Loss: 0.2581644654273987, Train: 1.0000, Val: 0.6640, Test: 0.6590\n",
      "Epoch: 170, Loss: 0.4494279623031616, Train: 1.0000, Val: 0.6660, Test: 0.6590\n",
      "Epoch: 171, Loss: 0.43178224563598633, Train: 1.0000, Val: 0.6660, Test: 0.6590\n",
      "Epoch: 172, Loss: 0.45754218101501465, Train: 1.0000, Val: 0.6700, Test: 0.6820\n",
      "Epoch: 173, Loss: 0.2753662168979645, Train: 1.0000, Val: 0.6760, Test: 0.6850\n",
      "Epoch: 174, Loss: 0.47163915634155273, Train: 1.0000, Val: 0.6760, Test: 0.6850\n",
      "Epoch: 175, Loss: 0.28922155499458313, Train: 1.0000, Val: 0.6800, Test: 0.6920\n",
      "Epoch: 176, Loss: 0.3194673955440521, Train: 1.0000, Val: 0.6820, Test: 0.6930\n",
      "Epoch: 177, Loss: 0.17296135425567627, Train: 1.0000, Val: 0.6780, Test: 0.6930\n",
      "Epoch: 178, Loss: 0.24605362117290497, Train: 1.0000, Val: 0.6800, Test: 0.6930\n",
      "Epoch: 179, Loss: 0.33637985587120056, Train: 1.0000, Val: 0.6760, Test: 0.6930\n",
      "Epoch: 180, Loss: 0.3290547728538513, Train: 1.0000, Val: 0.6740, Test: 0.6930\n",
      "Epoch: 181, Loss: 0.24828682839870453, Train: 1.0000, Val: 0.6700, Test: 0.6930\n",
      "Epoch: 182, Loss: 0.47126710414886475, Train: 1.0000, Val: 0.6700, Test: 0.6930\n",
      "Epoch: 183, Loss: 0.46568575501441956, Train: 1.0000, Val: 0.6700, Test: 0.6930\n",
      "Epoch: 184, Loss: 0.5291840434074402, Train: 1.0000, Val: 0.6660, Test: 0.6930\n",
      "Epoch: 185, Loss: 0.4009459316730499, Train: 1.0000, Val: 0.6660, Test: 0.6930\n",
      "Epoch: 186, Loss: 0.43337708711624146, Train: 1.0000, Val: 0.6640, Test: 0.6930\n",
      "Epoch: 187, Loss: 0.3792679011821747, Train: 1.0000, Val: 0.6640, Test: 0.6930\n",
      "Epoch: 188, Loss: 0.22940196096897125, Train: 1.0000, Val: 0.6620, Test: 0.6930\n",
      "Epoch: 189, Loss: 0.49831587076187134, Train: 1.0000, Val: 0.6620, Test: 0.6930\n",
      "Epoch: 190, Loss: 0.498085081577301, Train: 1.0000, Val: 0.6640, Test: 0.6930\n",
      "Epoch: 191, Loss: 0.4736451506614685, Train: 1.0000, Val: 0.6580, Test: 0.6930\n",
      "Epoch: 192, Loss: 0.5097127556800842, Train: 1.0000, Val: 0.6560, Test: 0.6930\n",
      "Epoch: 193, Loss: 0.3312048017978668, Train: 1.0000, Val: 0.6600, Test: 0.6930\n",
      "Epoch: 194, Loss: 0.45086055994033813, Train: 1.0000, Val: 0.6580, Test: 0.6930\n",
      "Epoch: 195, Loss: 0.5009932518005371, Train: 1.0000, Val: 0.6520, Test: 0.6930\n",
      "Epoch: 196, Loss: 0.4711511433124542, Train: 1.0000, Val: 0.6560, Test: 0.6930\n",
      "Epoch: 197, Loss: 0.21381843090057373, Train: 1.0000, Val: 0.6560, Test: 0.6930\n",
      "Epoch: 198, Loss: 0.13834667205810547, Train: 1.0000, Val: 0.6560, Test: 0.6930\n",
      "Epoch: 199, Loss: 0.3967379629611969, Train: 1.0000, Val: 0.6540, Test: 0.6930\n",
      "Best Val Acc: 0.6820 Test Acc: 0.6930\n",
      "Average Val Acc: 0.6907\n",
      "Average Test Acc: 0.6973 ± 0.0130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_acc = []\n",
    "avg_val_acc = 0\n",
    "for seed in [42, 1234, 2021]:\n",
    "    torch.manual_seed(seed)\n",
    "    model = GAT(dataset.num_features, 8, dataset.num_classes, heads=8)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=50, min_lr=0.01)\n",
    "    best_val_acc = final_test_acc = 0\n",
    "    for epoch in range(1, 200):\n",
    "        loss = train(model, data, optimizer, lap, scheduler=None, loss='smooth_label', alpha=0.00001)\n",
    "        train_acc, val_acc, tmp_test_acc = test(model, data)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = tmp_test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    print(f'Best Val Acc: {best_val_acc:.4f}', f'Test Acc: {test_acc:.4f}')\n",
    "    total_test_acc.append(test_acc)\n",
    "    avg_val_acc += best_val_acc\n",
    "print(f'Average Val Acc: {avg_val_acc/3:.4f}')\n",
    "print(f'Average Test Acc: {np.mean(total_test_acc):.4f} ± {np.std(total_test_acc):.4f}')\n",
    "torch.save(model.state_dict(), f'./models_LS/{dataset.name}_GAT_{k}.pt')\n",
    "preds = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "nsm = non_smooth_label_metric(dataset, preds)\n",
    "nsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe8eaf950b0cf64c1b70de22759d9a144a2595c541b4003711edd1f96d908e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
